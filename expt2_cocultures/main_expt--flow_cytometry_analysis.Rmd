---
title: "Resistance & recovery 4-sp communities: Analyze flow cytometry data"
author: "Hermina"
Date: "2025-02-19"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_float:
      smooth_scroll: false
  pdf_document:
    toc: true
editor_options: 
  markdown: 
    wrap: 90
---

# Introduction

This document is updated from `analyze_temp_serial_transfer_expt--28Oct24.Rmd`.

There are 5 treatments: no heat (5 days serial transfer), 6h heat, 12h heat, 24h heat, and
48h heat. Each of these is setup with 5 technical replicates that was initially inoculated
to about equal ratios (on Day 0).

Summary of choices: Based on the number of cells observed in true blank wells, I only
include data from wells with \>50 cells. Based on the flow cytometry data from Day 0 (which is estimated from the blanks in the OD data to have never experienced any contamination events), a
misclassification rate of 1% is assumed. Contaminated replicates are
defined as having a substantially higher % of a species that was not inoculated in that well (i.e., than expected from this misclassification rate). When a contaminated well was detected, all time points associated with that well were removed from the data.

# Load & Annotate Data

After loading the environment, I will load all of the flow cytometry cell count data and information about extinct wells from the OD data. Data is also annotated.

```{r, loadEnv}
library(tidyverse)
library(readxl) # for importing data directly from Excel sheet
library(RColorBrewer) # for changing the colours of plots
library(ggbeeswarm) # for beeswarm plots
library(vegan) # to estimate diversity and for ordination (NMDS)
library(ggordiplots) # for ggplotting ellipses around treatment group centroids during ordination
library(chemodiv) # for estimating species richness
#library(lme4)  <-- not sure this is needed?
library(glmmTMB) # for fitting and trouble-shooting GLM's
library(DHARMa) # for plotting the residuals when using glmmTMB
library(rcompanion) # for r-squared estimates of GLM's
library(MuMIn) # for calculating AICc
library(performance) # for checking multicolinearity
library(effsize) # for post-hoc estimate of effect sizes
library(emmeans) # (ditto as above)
library(BSDA) # for pairwise t-tests to compare effect sizes between data subsets
#library(partitionBEFsp) # for paritioning the biodiversity effects
#library(ape) # for ordination ??
library(ggforce) # for plotting ellipses in ggplot

# print the complete info about packages and versions currently loaded in the environment:
sessionInfo()

# set theme for all plots
fave_theme <- theme_light() + # see other options at https://ggplot2.tidyverse.org/reference/ggtheme.html
              theme(text = element_text(size=15), # larger text size for titles & axes
                    panel.grid.major = element_blank(), # remove major gridlines
                    panel.grid.minor = element_blank()) # remove minor gridlines
theme_set(fave_theme)

# define a palette for plotting the 4 species
species_4pal_alphabetical = palette.colors(8, palette = "R4")[c(3, 5, 7, 2)] #in alphabetical order
species_4pal_speed = palette.colors(8, palette = "R4")[c(7, 5, 3, 2)] #from fast to slow

# define a palette for plotting the 3 treatment days
trtmt_pal = brewer.pal(4, "Set2")[c(4, 3, 1)]

# define a palette for plotting the heat duration
control_to_48h_pal <- scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9)

# define a palette for plotting the inoculated community richness
CommRich_pal <- scale_colour_viridis_d(option = "viridis", begin=0.2, end=0.95)

# define a function to find the mode of a vector. Credit to https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

Load the cell counts data from the summary Excel files created by FCS Express. Then load
the well volume data from the summary .csv files created by the Attune software.

```{r, loadData}
# a function to load the fluorescent counts data (from .xlsx files created by )
import_flow_counts <- function(file)
  return(as.data.frame(
          read_excel(path=file, sheet="FCS Express Report",
                    # replace the column names as follows:
                    col_names = c("Filename",
                                  "Gate1", "Count_grimontii",
                                  "Gate2", "Count_putida",
                                  "Gate3", "Count_protegens",
                                  "Gate4","Count_veronii")))
  )
# a function to load and parse the volume data
import_flow_volume <- function(file) {
  raw.csv <- read.csv(file)
  # keep the volume info and just enough data to identify the sample. Then remove resultant redundant rows
  vol_data <- raw.csv %>% select(Plate, Sample, Volume) %>% unique()
}

# a function to loop through the folders containing the data files, open the .xlsx and .csv files and combine their data
import_from_files <- function(dir_vector){
  # initiatize variables
  raw_counts <- raw_vols <- data.frame()
  # loop through each directory
  for(dir in dir_vector){
    # get all the file names
    files_v <- list.files(dir)
    
    # identify the excel files
    files_excel <- files_v[endsWith(files_v, ".xlsx")]
    # and loop through all of them to extract their data
    TMPraw_counts <- data.frame()
    for(val in files_excel){
      TMPraw_counts <- rbind(TMPraw_counts, import_flow_counts(paste0(dir, "/", val)))
    }
    
    # identify the csv files
    files_csv <- files_v[endsWith(files_v, ".csv")]
        # and loop through all of them to extract their data
    TMPraw_vols <- data.frame()
    for(val in files_csv){
      TMPraw_vols <- rbind(TMPraw_vols, import_flow_volume(paste0(dir, "/", val)))
    }
    
    # concatenate the data from counts and from vols
    raw_counts <- rbind(raw_counts, TMPraw_counts)
    raw_vols <- rbind(raw_vols, TMPraw_vols)
    rm(TMPraw_counts, TMPraw_vols)
  }
  return(list(raw_counts, raw_vols))
}

# get all of the raw data:
list_rawdata <- import_from_files(c("./raw_data/serial_transf--2July24", "./raw_data/serial_transf--8July24", "./raw_data/serial_transf--5Aug24", "./raw_data/serial_transf--19Aug24"))
```

Now we can process the data to create unique ID's for each sample. This info needs
to be parsed from the Filename column for the flow counts data (i.e., excel files) and
from the Plate column for the flow volumes data (i.e., csv files).

```{r, processData}
# start with flow counts data:
  # I got confused and now there are rows containing the column names. Get rid of those...
list_rawdata[[1]] <- list_rawdata[[1]][-grep("Filename", list_rawdata[[1]]$Filename),]
  # Day0 has a different pattern in the Filename column so let's process those rows first
Day0 <- list_rawdata[[1]][grep("Day0", list_rawdata[[1]]$Filename),] %>% separate_wider_regex(Filename,
                                                                                              c(Date="24-0\\d-\\d{2}", " Day", Day="\\d",
                                                                                                ".*dilution_", Well="\\w\\d+", "\\.acs compensated"))
  # Now process the Filename column for the other days
NOTday0 <- list_rawdata[[1]][-grep("Day0", list_rawdata[[1]]$Filename),] %>% separate_wider_regex(Filename,
                                                                                              c(Date="24-0\\d-\\d{2}", " Day", Day="\\d", " -- ",
                                                                                                Incubator="\\w+", "\\.plate", Plate="\\d", 
                                                                                                ".*dilution_", Well="\\w\\d+", "\\.acs compensated"))
# Put the flow counts data back together into a single data.frame:
raw_Counts <- rbind(Day0 %>% mutate(Incubator=NA, Plate=0), # add in the 2 extra empty columns that are missing from Day0
                    NOTday0) %>% select(-Gate1, -Gate2, -Gate3, -Gate4)
rm(Day0, NOTday0)


# then do a similar thing for the volume data:
  # Day0 has a different pattern in the Plate column so let's process those rows first
Day0 <- list_rawdata[[2]][grep("Day0", list_rawdata[[2]]$Plate),] %>% separate_wider_regex(Plate,
                                                                                           c(Date="24-0\\d-\\d{2}", " Day", Day="\\d", ".*"))
  # Now process the Plate column for the other days
NOTday0 <- list_rawdata[[2]][-grep("Day0", list_rawdata[[2]]$Plate),] %>% separate_wider_regex(Plate,
                                                                                              c(Date="24-0\\d-\\d{2}", " Day", Day="\\d", " -- ",
                                                                                                Incubator="\\w+", "\\.plate", Plate="\\d"))
# Put the flow volumes data back together into a single data.frame:
raw_Vol <- rbind(Day0 %>% mutate(Incubator=NA, Plate=0), # add in the 2 extra empty columns that are missing from Day0
                 NOTday0) %>% rename(Well = Sample) # rename this column for consistency with the Counts data
rm(Day0, NOTday0)

# We can now combine the counts and volume data
  # here I need to use left join because we don't have volume data for Day 0 on 24-07-02 !!!!!!
raw_data <- left_join(raw_Counts, raw_Vol,
                      by=c("Date", "Day", "Well", "Incubator", "Plate"))
rm(raw_Counts, raw_Vol)


# add annotation specifying the Heat treatment and the Incubator
  # For 2July24: all samples were subjected to 6h of heat
  # For 8July24: samples in the Epoch plate reader are control (no heat)
  #              Samples in the H1 plate reader are 48h of heat
  # For 5Aug24: all samples were subjected to 12h of heat
  # For 19Aug24: all samples were subjected to 24h of heat
raw_data$Heat <- 0
raw_data$Heat[which(raw_data$Date == "24-07-02")] <- 6
raw_data$Heat[which(raw_data$Date == "24-07-08" & raw_data$Incubator == "H1")] <- 48
raw_data$Heat[which(raw_data$Date == "24-08-05")] <- 12
raw_data$Heat[which(raw_data$Date == "24-08-19")] <- 24

# change the variable classes for data analysis
raw_data$Count_grimontii <- as.numeric(raw_data$Count_grimontii)
raw_data$Count_putida <- as.numeric(raw_data$Count_putida)
raw_data$Count_protegens <- as.numeric(raw_data$Count_protegens)
raw_data$Count_veronii <- as.numeric(raw_data$Count_veronii)
```

Finally, we can annotate the data with the sample information for each well. Note that
there are different plate layouts for Day0 (same for all dates) And the experiment from
24-07-02 uses a different layout as compared to the rest of the data (see the layout png file in the corresponding data subfolder).
...But, also, I made
other mistakes too so there's modified layouts for that too! XP

```{r, annotateData}
# the "Plate1" layout is used for all days >0 (except for 24-07-02)
layout.plate1 <- data.frame(Well = paste0(LETTERS[1:8], rep((2*1:6)-1, each=8)),
                            putida = c(0, 1, 1, 1, 0, 0, 0, 0,
                                       1, 0, 0, 0, 1, 1, 1, 0,
                                       0, 0, 1, 1, 1, 0, 1, 0,
                                       0, 0, 1, 1, 1, 0, 1, 0,
                                       1, 1, 1, 0, 1, 0, 0, 0,
                                           0, rep(0,6), 1),
                            protegens = c(0, 0, 0, 1, 0, 0, 1, 0,
                                          0, 1, 0, 0, 1, 0, 0, 1,
                                          1, 0, 1, 1, 0, 1, 1, 0,
                                          0, 0, 1, 0, 0, 1, 0, 1,
                                          1, 1, 0, 1, 1, 0, 1, 0,
                                              1, rep(0,6), 0),
                            grimontii =  c(0, 0, 1, 0, 0, 1, 0, 0,
                                           0, 0, 1, 0, 0, 1, 0, 1,
                                           0, 1, 1, 0, 1, 1, 1, 0,
                                           1, 0, 0, 1, 0, 1, 0, 0,
                                           1, 0, 1, 1, 1, 0, 0, 1,
                                               1, rep(0,6), 0),
                            veronii =    c(0, 1, 0, 0, 1, 0, 0, 0,
                                           0, 0, 0, 1, 0, 0, 1, 0,
                                           1, 1, 0, 1, 1, 1, 1, 0,
                                           0, 1, 0, 0, 1, 0, 0, 0,
                                           0, 1, 1, 1, 1, 0, 1, 1,
                                               0, rep(0,6), 0))

### CommRich = 0 corresponds to blanks, mistakes made on Day0 are removed altogether,
###     and CommRich = NA is used to indicate contamination.
# modified layout of plate1 specific for 24-07-02
layout.plate1_2Jul <- layout.plate1
layout.plate1_2Jul$putida[c(1,8, 41:48)]    <- c(0, 1, 1, 1, 1, 0, 1, 0, 0, 0) 
layout.plate1_2Jul$protegens[c(1,8, 41:48)] <- c(1, 0, 1, 0, 0, 1, 0, 1, 0, 0)
layout.plate1_2Jul$grimontii[c(1,8, 41:48)] <- c(1, 0, 0, 1, 0, 1, 0, 0, 1, 0)
layout.plate1_2Jul$veronii[c(1,8, 41:48)]   <- c(0, 0, 0, 0, 1, 0, 0, 0, 0, 1)

# modified layout of plate1 specific for mistakes made on 24-07-08
  # column 4 of OD plate is swapped orientation
layout.plate1_8Jul <- layout.plate1
layout.plate1_8Jul$putida[25:32]    <- layout.plate1$putida[9:16]
layout.plate1_8Jul$protegens[25:32] <- layout.plate1$protegens[9:16]
layout.plate1_8Jul$grimontii[25:32] <- layout.plate1$grimontii[9:16]
layout.plate1_8Jul$veronii[25:32]   <- layout.plate1$veronii[9:16]

# add a column for community richness in all of the above df's
layout.plate1 <- layout.plate1 %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")
layout.plate1_2Jul <- layout.plate1_2Jul %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")
layout.plate1_8Jul <- layout.plate1_8Jul %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")


# the "Plate2" layout
layout.plate2 <- data.frame(Well = paste0(LETTERS[1:8], rep((2*1:6)-1, each=8)),
                            putida = c(1, 1, 1, 0, 1, 0, 0, 0,
                                       1, 0, 1, 0, 0, 0, 1, 1,
                                       1, 0, 1, 0, 0, 0, 1, 1,
                                       1, 0, 0, 0, 1, 1, 1, 0,
                                           rep(0,7), 0,
                                       0, 1, 0, 1, 1, 1, 0, 0),
                            protegens = c(1, 0, 0, 1, 0, 1, 0, 0,
                                          0, 1, 1, 0, 1, 0, 1, 1,
                                          0, 1, 0, 1, 0, 0, 1, 0,
                                          1, 0, 1, 0, 1, 1, 0, 1,
                                              rep(0,7), 1,
                                          0, 1, 1, 0, 1, 1, 0, 0),
                            grimontii = c(0, 1, 0, 1, 0, 0, 1, 0,
                                          1, 1, 1, 0, 0, 1, 1, 0,
                                          0, 1, 0, 0, 1, 0, 0, 1,
                                          1, 0, 0, 1, 1, 0, 1, 1,
                                             rep(0,7), 0,
                                          0, 1, 1, 1, 0, 1, 1, 0),
                            veronii = c(0, 0, 1, 0, 0, 0, 0, 1,
                                        1, 1, 1, 0, 1, 1, 0, 1,
                                        1, 0, 0, 0, 0, 1, 0, 0, 
                                        1, 0, 1, 1, 0, 1, 1, 1,
                                           rep(0,7), 1,
                                        0, 1, 1, 1, 1, 0, 1, 0))

# modified layout of plate2 specific for 24-07-02
layout.plate2_2Jul <- layout.plate2
layout.plate2_2Jul$putida[1:32]    <- layout.plate2$putida[c(9:32,41:47,40)]
layout.plate2_2Jul$protegens[1:32] <- layout.plate2$protegens[c(9:32,41:47,40)]
layout.plate2_2Jul$grimontii[1:32] <- layout.plate2$grimontii[c(9:32,41:47,40)]
layout.plate2_2Jul$veronii[1:32]   <- layout.plate2$veronii[c(9:32,41:47,40)]
layout.plate2_2Jul <- layout.plate2_2Jul[1:32,] # rest of flow plate 2 is empty

# modified layout of plate2 specific for mistakes made on 24-07-08 and 24-08-19
layout.plate2_8Jul19Aug <- layout.plate2[-(9:16),] # I screwed up column 8 of OD plate

# add a column for community richness in all of the above df's
layout.plate2 <- layout.plate2 %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")
layout.plate2_2Jul <- layout.plate2_2Jul %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")
layout.plate2_8Jul19Aug <- layout.plate2_8Jul19Aug %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")

# the "Inocula" layout
layout.inocula <- data.frame(Well = paste0(LETTERS[1:8], rep((2*1:6)-1, each=8)),
                               putida = rep(c(1, 0, 0, 0, 1, 1, 1, 0,
                                              0, 0, 1, 1, 1, 0, 1, NA), times=3),
                            protegens = rep(c(0, 1, 0, 0, 1, 0, 0, 1,
                                              1, 0, 1, 1, 0, 1, 1, NA), times=3),
                            grimontii = rep(c(0, 0, 1, 0, 0, 1, 0, 1,
                                              0, 1, 1, 0, 1, 1, 1, NA), times=3),
                              veronii = rep(c(0, 0, 0, 1, 0, 0, 1, 0,
                                              1, 1, 0, 1, 1, 1, 1, NA), times=3)) %>%
                mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all") %>%
                    filter(!is.na(CommRich))


# a function to annotate each data set with the indicated layout
  # this will KEEP well blanks!
annotate_samples <- function(layout, select_date, select_plate) {
  relevant_data <- raw_data %>% filter(Date==select_date, Plate==select_plate)
  
  # for Innoc, use inner_join to combine the flow data with its annotation
  if(select_plate == 0){
    output_df <- inner_join(layout, relevant_data, by="Well")
  }
  if(select_plate != 0) {
    output_df <- left_join(merge(layout, relevant_data %>% select(Day, Incubator, Heat) %>% distinct()),
                         relevant_data, by=c("Well", "Day", "Incubator", "Heat"))
    output_df$Date <- select_date
    output_df$Plate <- select_plate
  }

  return(output_df)
  rm(relevant_data, output_df)#, blank_annot, blank_data)
}

# now we can add the sample names for each one.
annotated.rawdata <- rbind(annotate_samples(layout = layout.inocula, select_date = "24-07-02", select_plate=0),
                  annotate_samples(layout = layout.plate1_2Jul, select_date = "24-07-02", select_plate=1),
                  annotate_samples(layout = layout.plate2_2Jul, select_date = "24-07-02", select_plate=2),
                  annotate_samples(layout = layout.inocula, select_date = "24-07-08", select_plate=0),
                  annotate_samples(layout = layout.plate1_8Jul, select_date = "24-07-08", select_plate=1),
                  annotate_samples(layout = layout.plate2_8Jul19Aug, select_date = "24-07-08", select_plate=2), ##
                  annotate_samples(layout = layout.inocula, select_date = "24-08-05", select_plate=0),
                  annotate_samples(layout = layout.plate1, select_date = "24-08-05", select_plate=1),
                  annotate_samples(layout = layout.plate2, select_date = "24-08-05", select_plate=2),
                  annotate_samples(layout = layout.inocula, select_date = "24-08-19", select_plate=0),
                  annotate_samples(layout = layout.plate1, select_date = "24-08-19", select_plate=1),
                  annotate_samples(layout = layout.plate2_8Jul19Aug, select_date = "24-08-19", select_plate=2))

# fixing other small mistakes in annotation:
  # Day1 of 24-07-02: sample A1 from plate 2 was loaded into sample A1 plate 1.
annotated.rawdata$CommRich[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                   annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 3

annotated.rawdata$putida[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                 annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 1

annotated.rawdata$protegens[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                    annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 0

annotated.rawdata$grimontii[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                    annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 1

annotated.rawdata$veronii[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                  annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 1


# Annotate the treatments
annotated.rawdata$Heat_Day <- as.numeric(NA)
annotated.rawdata$Heat_Day[which(annotated.rawdata$Heat!=0 & annotated.rawdata$Day==1)] <- 1
annotated.rawdata$Heat_Day[which(annotated.rawdata$Heat>6 & annotated.rawdata$Day==2)] <- 2
annotated.rawdata$Heat_Day[which(annotated.rawdata$Heat==48 & annotated.rawdata$Day==3)] <- 3

annotated.rawdata$Recov_Day <- as.numeric(NA)
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat==6 & annotated.rawdata$Day==2)] <- 1
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat==6 & annotated.rawdata$Day==3)] <- 2
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat %in% c(12,24) & annotated.rawdata$Day==3)] <- 1
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat %in% c(12,24) & annotated.rawdata$Day==4)] <- 2
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat==48 & annotated.rawdata$Day==4)] <- 1
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat==48 & annotated.rawdata$Day==5)] <- 2

# sanity check to make sure there are no redundant rows
stopifnot(!any(duplicated(annotated.rawdata %>% select(Date, Day, Incubator, Plate, Well))))

# change some of the values to more appropriate types
annotated.rawdata$Plate <- as.numeric(annotated.rawdata$Plate)
annotated.rawdata$Day <- as.numeric(annotated.rawdata$Day)

# clean up
rm(layout.inocula, layout.plate1, layout.plate1_2Jul, layout.plate1_8Jul, layout.plate2, layout.plate2_2Jul, layout.plate2_8Jul19Aug, list_rawdata, raw_data)
```

The annotated data contains information on the complete dataset, including blank wells and
excluded wells. Any mistakes there were made during inoculation on Day 0 have been removed
altogether. `CommRich == 0` indicates well blanks that should be empty (in this case, all
4 species columns will also be `0`). Finally, `CommRich == NA` indicates data rows that
were excluded; e.g., due to low total counts or contamination (in this case, the 4 species
columns will be kept to indicate what should have been in that excluded well).

For reproducibility and checking that the metadata is correctly associated with the data,
print the metadata out to file. Note that the location on the incubated plates
(corresponding to OD data) is different from the location on the flow cytometery plate. In
the code below I create a column for the OD_Well and assign unique identifiers for each
time series. The metadata file `annotation_for_alldata.csv` summarizes all of this info.

```{r, addODwell}
# annotation for Day 0 lists the plate as plate 0 but let's change that to Innoc
annotated.rawdata$Plate[which(annotated.rawdata$Plate==0)] <- "Innoc"

# copy the metadata to another variable and remove the data columns
metadata <- annotated.rawdata %>% select(-Volume,
                                         -Count_grimontii, -Count_protegens, -Count_putida, -Count_veronii)

# the columns currently labeled as "Well" and "plate" is actually only true for the location of the sample on the flow cytometer data
metadata$filler <- "plate"
metadata <- metadata %>% unite(col="plateNum", c(filler, Plate), sep="", remove = FALSE) %>% 
              unite(col="FLOWplateWell", c(plateNum, Well), sep="-", remove = FALSE) %>% select(-filler, -plateNum)

#####
# add true well sample location to metadata (i.e., as corresponding to OD data)
#####
# split up the Well into separate columns for the row and column location
metadata <- metadata %>% separate_wider_regex(Well, c(row="\\w", col="\\d+"))
metadata$REALcol <- 0

# for non-Innoc days after 2 July, the pattern is actually very simple and systematic
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==1)] <- 1
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==3)] <- 2
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==5)] <- 3
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==7)] <- 4
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==9)] <- 5
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==11)] <- 6
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==1)] <- 7
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==3)] <- 8
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==5)] <- 9
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==7)] <- 10
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==9)] <- 11
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==11)] <- 12

# for non-Innoc days on 2 July, the pattern is similar for plate 1 columns 1 to 9:
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==1)] <- 1
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==3)] <- 2
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==5)] <- 3
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==7)] <- 4
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==9)] <- 5
# the pattern changes from here:
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==11)] <- 7
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==1)] <- 8
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==3)] <- 9
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==5)] <- 10
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==7)] <- 12
# Note that plate 2 Well H7 on flow actually comes from H11
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==7 & metadata$row=="H")] <- 11
# and for plate 1 column 1, Well H1 on flow actually comes from H6
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==1 & metadata$row=="H")] <- 6
# finally, plate 1 column1: Well A1 on flow actually comes from A6. But note the mistake on Day1
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$Day!=1 & metadata$col==1 & metadata$row=="A")] <- 6
# on 2 July Day 1, plate 1 well A1 on flow actually comes from A8
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$Day==1 & metadata$col==1 & metadata$row=="A")] <- 8

# now we are finished with the NON-INNOC annotations
# we can put together the row and REALcol columns to get the location on the OD plate
data_meta <- metadata %>% filter(Plate != "Innoc") %>% unite("OD_well", c(row, REALcol), sep="") %>% select(-col) %>%
                    unite("uniqID", c(Date, Incubator, OD_well), sep=" ", remove = FALSE)

# last (and perhaps least), annotate the additional blank wells from 2 July,
july2_blanks <- data_meta %>% filter(Date=="24-07-02", CommRich==0) %>%
                    select(-FLOWplateWell, -Plate, -uniqID, -OD_well) %>% distinct()
missing_blanks <- data.frame(OD_well=c("A1", "H1", "H12", paste0(LETTERS[1:7],11), paste0(LETTERS[2:7],6)),
                             FLOWplateWell=NA, Plate=NA) %>%
                    mutate(uniqID=paste("24-07-02 Epoch", OD_well), .keep="all")
july2_missing <- merge(july2_blanks,missing_blanks)
data_meta <- rbind(data_meta, july2_missing)
rm(july2_blanks, missing_blanks, july2_missing)

#####
# Innoc data: add OD_well and uniqID columns
#####
# In order to annotate the most raw version of the data, I decided to create redundant rows for the Innoc data. This way each row from Innoc appears 5x with its associated OD_well and uniqID.
innoc_meta <- metadata %>% filter(Plate == "Innoc") %>% select(-row, -col, -REALcol, -Incubator, -Heat)
innoc_meta <- suppressWarnings( # we expect left_join to be upset about many-to-many relationship, no need to issue warning.
                    left_join(innoc_meta,
                              data_meta %>%
                                  select(-FLOWplateWell, -Day, -Plate, -Heat_Day, -Recov_Day) %>%
                                      distinct(), # remove the redundant rows from each day
                              by = c("CommRich", "putida", "protegens", "grimontii", "veronii", "Date"))
              )
# trash the now old df to avoid confusion
rm(metadata)

# save the complete metadata to file
write.csv(rbind(data_meta, innoc_meta), file="./intermediate_data/annotation_for_alldata.csv", quote=FALSE, row.names=FALSE)

#####
# Save the fully annotated raw flow cytometry counts data
#####
# associate the metadata back with the raw counts data
# for Days > 0:
temp_metadata <- data_meta %>% separate_wider_regex(FLOWplateWell, c(FLOW="plate\\w+-", Well="\\w+"))
annot.days <- inner_join(temp_metadata, annotated.rawdata,
                         by=c("Well", "putida", "protegens", "grimontii", "veronii", "CommRich", "Date",
                              "Day", "Incubator", "Plate", "Heat", "Heat_Day", "Recov_Day")) %>%
                  unite("FLOWplateWell", c(FLOW, Well), sep="")
rm(temp_metadata, data_meta)

# for Innoc Days:
temp_metainnoc <- innoc_meta %>% separate_wider_regex(FLOWplateWell, c(FLOW="plate\\w+-", Well="\\w+"))
annot.innoc <- left_join(temp_metainnoc,
                         annotated.rawdata %>% select(-Incubator, -Heat),
                         by=c("Well", "putida", "protegens", "grimontii", "veronii", "CommRich", "Date",
                              "Day", "Plate", "Heat_Day", "Recov_Day")) %>%
                  unite("FLOWplateWell", c(FLOW, Well), sep="")

# save this to file as well
write.csv(rbind(annot.days, annot.innoc), file="./intermediate_data/flow_rawdata.csv", quote=FALSE, row.names=FALSE)

# remove annotated.rawdata as it has been superseded by annot.days and annot.innoc
rm(annotated.rawdata, temp_metainnoc, innoc_meta)

########
# fix annotation mistake for flow cytometry acquisition of uniqID "24-07-02 Epoch A6"
# this well is missing on Day 1 bc A8 was pipetted there instead. But now we have 2 wells for "24-07-02 Epoch A8"...
########
wrong_row <- which(annot.days$uniqID == "24-07-02 Epoch A8" & annot.days$Day == 1 & is.na(annot.days$Volume))
annot.days$uniqID[wrong_row] <- "24-07-02 Epoch A6"
annot.days$OD_well[wrong_row] <- "A6"
annot.days$putida[wrong_row] <- 0
annot.days$protegens[wrong_row] <- 1
annot.days$grimontii[wrong_row] <- 1
annot.days$veronii[wrong_row] <- 0
annot.days$CommRich[wrong_row] <- 2
rm(wrong_row)
```

The data from Day 0 (`annot.innoc`) is 3x measurements of the innoculum that is used to
inoculate the 5 replicates. For the summary annotation file (aka
metadata above), each `FLOWplateWell` appears redundantly with the up to 5 associated
uniqID and OD_well replicates. I chose to do this so that the raw values from the flow
cytometry data are preserved with their relevant annotation.

Below, this Day 0 data is averaged across the 3 different `FLOWplateWell` values. This mean that below Day 0 is now joined with the rest of the data in the variable `annotated.rawdata`.

```{r, Day0}
# average the Day0 data actually across its redundant flow cytometery measurements...
mean.innoc <- annot.innoc %>% group_by(uniqID, OD_well, Incubator, Plate, Heat, Date,
                                 Day, Heat_Day, Recov_Day,
                                 CommRich, putida, protegens, grimontii, veronii) %>%
          summarise(Mean_putida = mean(Count_putida),
                    Mean_protegens = mean(Count_protegens),
                    Mean_grimontii = mean(Count_grimontii),
                    Mean_veronii = mean(Count_veronii),
                    SD_putida = sd(Count_putida),
                    SD_protegens = sd(Count_protegens),
                    SD_grimontii = sd(Count_grimontii),
                    SD_veronii = sd(Count_veronii),
                    Vol_mean = mean(Volume),
                    vol_sd = sd(Volume))

# here's some plots to summarize how much variation there is between measurements of the same inocula
plotting_mean.innoc <- mean.innoc %>% pivot_longer(cols = Mean_putida:SD_veronii,
                                                   names_to = c(".value", "species"),
                                                   names_sep = "_") %>%
                          filter(Incubator != "H1") # the same innoculum was used for 2 treatments on 24-07-08. Remove this redundancy for plotting
ggplot(plotting_mean.innoc,
       aes(x=Mean, y=SD, colour=species)) +
  geom_point(alpha=0.7) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  labs(title="3 measures of innoculum")

ggplot(plotting_mean.innoc,
       aes(x=Mean, y=SD, colour=Date)) +
  geom_point(alpha=0.7) +
  labs(title="3 measures of innoculum")

ggplot(plotting_mean.innoc %>% mutate(CV = SD/Mean),
       aes(x=Mean, y=CV, colour=species)) +
  geom_point(alpha=0.7) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  labs(title="3 measures of innoculum")

# due to false positive counts,
# the CV blows up when I am counting species that are not actually in that sample

ggplot(plotting_mean.innoc %>% filter((putida == 1 & species == "putida") |
                                      (protegens == 1 & species == "protegens") |
                                      (grimontii == 1 & species == "grimontii") |
                                      (veronii == 1 & species == "veronii")) %>%
            mutate(CV = SD/Mean),
       aes(x=Mean, y=CV, colour=species)) +
  geom_point(alpha=0.7) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  labs(title="3 measures of innoculum (remove absent sp)")

ggplot(plotting_mean.innoc %>% unite("community", putida:veronii),
       aes(x=community, y=Mean, colour=species)) +
  geom_point(alpha=0.7) +
  geom_errorbar(aes(ymin=Mean-SD, ymax=Mean+SD), width=.2) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(title="3 measures of innoculum")

ggplot(plotting_mean.innoc %>% filter((putida == 1 & species == "putida") |
                                      (protegens == 1 & species == "protegens") |
                                      (grimontii == 1 & species == "grimontii") |
                                      (veronii == 1 & species == "veronii")) %>%
            unite("community", putida:veronii),
       aes(x=community, y=Mean, colour=species)) +
  geom_point(alpha=0.7) +
  geom_errorbar(aes(ymin=Mean-SD, ymax=Mean+SD), width=.2) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(title="3 measures of innoculum (remove absent sp)")

ggplot(plotting_mean.innoc %>% ungroup() %>%
              select(-uniqID, -OD_well) %>% distinct() %>%
                              filter((putida == 1 & species == "putida") |
                                      (protegens == 1 & species == "protegens") |
                                      (grimontii == 1 & species == "grimontii") |
                                      (veronii == 1 & species == "veronii")) %>%
            unite("community", putida:veronii) %>% mutate(CV = SD/Mean),
       aes(x=community, y=CV, colour=species)) +
  geom_jitter(width=0.2, alpha=0.7) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(title="3 measures of innoculum (remove absent sp)")

# I have no idea what I would do with this info about sample volume, but here it is:
  # There's no values for 24-07-02 because I accidentally forgot to save the .apx file (this is before I realized that volume is not saved in the .acs files)
ggplot(plotting_mean.innoc,
       aes(x=Vol_mean, y=vol_sd, colour=Date)) +
  geom_point(alpha=0.7) +
  labs(title="3 measures of innoculum")

# finally, we can add the mean counts for the Innoc to the whole data
annotated.rawdata <- mean.innoc %>% select(-SD_putida, -SD_protegens, -SD_grimontii, -SD_veronii, -vol_sd) %>%
                      rename(Count_putida=Mean_putida, Count_protegens=Mean_protegens, Count_grimontii=Mean_grimontii, Count_veronii=Mean_veronii, Volume=Vol_mean)
annotated.rawdata <- rbind(annotated.rawdata, annot.days)

# cleanup
rm(annot.days, annot.innoc, plotting_mean.innoc)
```

## Misclassification rate: estimated from innocula (Day 0)

I define the misclassification rate as
$\frac{\text{false positive events}}{\text{total events across all fluorescences}}$. In
other words, I am counting the number of events in the gate(s) where I know there should
be zero then dividing by the total number of fluorescent events in that well.
 To estimate the misclassification rate, I use the data from Day 0.

```{r, misclassification_est}
# use Day0 innoculum measurements for a first pass at estimating the misclassification rate
# i.e., the rate of falsely classifying as species A when I know for certain that species A is not present in my sample
misclass.innoc <- mean.innoc %>% mutate(Total_counts = Mean_putida + Mean_protegens + Mean_grimontii + Mean_veronii) %>% # get total for each sample
                    ungroup() %>% select(-uniqID, -OD_well) %>% distinct() %>%   # remove any redundant data
                        # put each species count in its own row in the column called mean (instead of having a column for each species)
                          pivot_longer(cols = Mean_putida:SD_veronii,
                                                   names_to = c(".value", "species"),
                                                   names_sep = "_") %>%
                            filter(Incubator != "H1") %>% # remove the redundant data
                              # keep just the instances where we know for sure that this species was NOT present
                                filter((putida == 0 & species == "putida") |
                                      (protegens == 0 & species == "protegens") |
                                      (grimontii == 0 & species == "grimontii") |
                                      (veronii == 0 & species == "veronii")) %>%
                                  # misclassification rate is the number of events / total counts
                                   mutate(mean_rate = Mean/Total_counts,
                                          sd_rate = SD/Total_counts)

# re-order the species from fast to slow for better plotting
misclass.innoc$species <- factor(misclass.innoc$species,
                                 levels = c("putida", "protegens", "grimontii", "veronii"))

ggplot(misclass.innoc, aes(x=species, y=mean_rate, colour=species)) +
  geom_beeswarm(alpha=0.5) +
  geom_errorbar(aes(ymin=mean_rate-sd_rate, ymax=mean_rate+sd_rate), width=.05, alpha=0.2) +
  scale_colour_manual(values=species_4pal_speed) +
  labs(title="misclassification rate in innoculum", y="mean +/- SD")

max(misclass.innoc$mean_rate)

ggplot(misclass.innoc %>% unite("community", putida:veronii),
       aes(x=species, y=mean_rate, colour=species)) +
  facet_wrap(vars(community)) +
  geom_point(alpha=0.5) +
  scale_y_continuous(breaks = c(0, 0.005, 0.01)) +
  scale_colour_manual(values=species_4pal_speed) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title="misclassification rate in innoculum")

# summarize the mean and max misclassification rates observed for each species
misclass.innoc %>% group_by(species) %>% summarise(mean_misclass = mean(mean_rate),
                                                   max_misclass = max(mean_rate))
# clean-up
rm(misclass.innoc, mean.innoc)
```


From here we can clearly see that the misclassification rate can be as bad as 1% and that
it depends on the species. Protegens is the most likely to be misclassified and, from the
plot of all possible community combinations, we see that the problem seems to be that
putida cells are being misclassified as belonging to protegens.

But I know that this rate of misclassification also depends on environmental conditions.
So I don't think it makes sense to correct the data using the exact values given above.
The more cautious approach would be to treat with caution any counts that are less than
1%.

# Data Processing

Here we make decisions about which data to keep and which to toss.

## Minimum Number of Events

I need to set a threshold for the minimum number of fluorescent events observed in a well
in order for me to decide that the well is not trustworthy.

At some point I did sample some wells that are true negatives. From this we learn that a
true negative can have as many as 20 total events.

Remember that I set the stopping conditions for 10 000 events in the cell gate OR until it
reaches the end of the sample (which seems to be 146uL). Let's rather arbitrarily set the
minimum total events in the well at 51 and see what happens with that.

```{r, identifyNAwells}
annotated.rawdata <- annotated.rawdata %>% mutate(Total_counts = Count_putida + Count_protegens + Count_grimontii + Count_veronii) %>%
                                           mutate(Total_density = Total_counts/Volume)

# plot the counts and volume for true negative wells
ggplot(annotated.rawdata %>% filter(CommRich==0, !is.na(Total_counts)),
       aes(x=Total_counts, y=Volume)) +
  geom_point() +
  labs(title="True negatives")

# plot the total counts as a histogram just to see what the dispersal is like
ggplot(annotated.rawdata, aes(x=Total_counts)) +
  geom_histogram(colour="black", fill="white") +
  labs(title="everything")

ggplot(annotated.rawdata, aes(x=Total_counts)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_counts in LOG SCALE!", title="everything")

ggplot(annotated.rawdata, aes(x=Total_counts)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_continuous(limits = c(-10,1010)) +
  labs(title="everything")


# then plot the total counts against the volume because we expect these very low counts should be associated with the highest volumes
ggplot(annotated.rawdata, aes(x=Total_counts, y=Volume, colour=as.factor(Heat_Day))) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  labs(colour="Day of heat") +
  labs(title="everything")


# okay, let's just see a histogram of the total cell density
ggplot(annotated.rawdata, aes(x=Total_density)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!") +
  labs(title="everything")


ggplot(annotated.rawdata %>% filter(!is.na(Heat_Day)),
       aes(x=Total_density)) +
  facet_grid(rows = vars(Heat_Day)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!", title="Day of Heat (everything)")

ggplot(annotated.rawdata %>% filter(!is.na(Recov_Day)),
       aes(x=Total_density)) +
  facet_grid(rows = vars(Recov_Day)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!", title="Day of Recovery (everything)")


### check what these graphs look like when I exclude wells where Total_counts < 51
ggplot(annotated.rawdata %>% filter(Total_counts > 50),
       aes(x=Total_counts, y=Volume, colour=as.factor(Heat_Day))) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  labs(colour="Day of heat") +
  labs(title="Total_counts > 50")

ggplot(annotated.rawdata %>% filter(Total_counts > 50),
       aes(x=Total_density)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!") +
  labs(title="Total_counts > 50")

ggplot(annotated.rawdata %>% filter(Total_counts > 50, !is.na(Heat_Day)),
       aes(x=Total_density)) +
  facet_grid(rows = vars(Heat_Day)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!", title="Day of Heat (Total_counts > 50)")

ggplot(annotated.rawdata %>% filter(Total_counts > 50, !is.na(Recov_Day)),
       aes(x=Total_density)) +
  facet_grid(rows = vars(Recov_Day)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!", title="Day of Recovery (Total_counts > 50)")

#######
# set threshold of > 50 events in total
#######
# copy everything EXCEPT BLANK WELLS to new variable
the.data <- annotated.rawdata %>% filter(CommRich != 0)

# summarize some information about the data points that I'm about to exclude
the.data %>% filter(Total_counts < 51) %>% ungroup() %>% select(uniqID, Heat, Day, Heat_Day, Recov_Day, CommRich, Volume, Total_counts) %>% summary()

# exclude from analysis all non-blanks rows where Total_counts < 51
the.data$CommRich[which(the.data$Total_counts < 51)] <- NA

# put the blank data back with the whole dataset
the.data <- rbind(the.data,
                  annotated.rawdata %>% filter(CommRich == 0))

# replace the data with NA values for all rows where Total_counts < 51
    # this includes both the excluded unreliable data as well as the true blanks flow data
the.data$Count_putida[which(the.data$Total_counts < 51)] <- NA
the.data$Count_protegens[which(the.data$Total_counts < 51)] <- NA
the.data$Count_grimontii[which(the.data$Total_counts < 51)] <- NA
the.data$Count_veronii[which(the.data$Total_counts < 51)] <- NA
the.data$Total_density[which(the.data$Total_counts < 51)] <- NA
the.data$Total_counts[which(the.data$Total_counts < 51)] <- NA

# clean-up
rm(annotated.rawdata)
```

I have re-assigned all wells that had less than 51 total fluorescent events as NA values.
This was a total of 87 wells.

Note that I've also removed any flow cytometry data from the true negative wells. This was
17 wells.

```{r, relAbundances}
# calculate densities and relative abundances for each species
the.data <- the.data %>% mutate(Conc_putida = Count_putida/Volume,
                                Conc_protegens = Count_protegens/Volume,
                                Conc_grimontii = Count_grimontii/Volume,
                                Conc_veronii = Count_veronii/Volume,
                                relDen_putida = Count_putida/Total_counts,
                                relDen_protegens = Count_protegens/Total_counts,
                                relDen_grimontii = Count_grimontii/Total_counts,
                                relDen_veronii = Count_veronii/Total_counts) #%>%
              #select(-Total_counts)

# sanity check that the relative densities are always adding up to 1
check <- the.data %>% mutate(sum_relDen = relDen_putida + relDen_protegens + relDen_grimontii + relDen_veronii) %>%
            # for convenience, remove the 87 NA values
            drop_na(Total_counts)
all.equal(check$sum_relDen, rep(1, nrow(check))) %>% # use all.equal() as there seem values very close to 1 but not exactly equal to 1
  stopifnot()

rm(check)
```

I have calculated the relative densities and made sure that all relative densities add up
to 1.

## Plot preliminary time-series

Before diving deeper into the data, let's just see quickly what the time series look like:

```{r, prelimTimeSeries}
# check: is each replicated time series annotated appropriately so that it can be pieced together?
the.data <- the.data %>% unite("community", putida:veronii, remove=FALSE) %>% ungroup()
for(com in unique(the.data$community)) {
  plot( ggplot(the.data %>% filter(community==com) %>%
           select(uniqID, Heat, Day, relDen_putida, relDen_protegens, relDen_grimontii, relDen_veronii) %>%
              pivot_longer(cols=starts_with("relDen"), names_to="species", names_prefix="relDen_", values_to="relDensity"),
         aes(x=Day, y=relDensity, colour=species, group=paste(uniqID,Heat,species))) +
    facet_grid(~Heat) +
    geom_point(alpha=0.2) +
    geom_line(alpha=0.5) +
    scale_colour_manual(values=species_4pal_alphabetical) +
    labs(title=com))
}

# clean up
rm(com)
```

After staring at the above time series for long enough, two things become clear

1.  Protegens has contaminated several wells. This is unambiguous contamination when it is
    present in communities where it was not innoculated. For these contaminated
    replicates, the entire time-series will be excluded from the analysis.

2.  The misclassification rate varies over time: e.g., putida is misclassified in a
    protegens monoculture (0_1_0_0) on day 1 for 3 different heat treatments. It appears
    in that well with a density of \> 10% ! As well, protegens and veronii are
    misclassified in putida monoculture (1_0_0_0) on days 2 and 3 of 24 hrs heat.

## Identify wells driven to extinction

Of the 87 NA values identified above,

-   Some occurred as a result of flow cytometry issues. E.G., there was probably a bubble
    that I didn't notice. (When I noticed the bubble, I would re-run that well. But this
    is only after I began to understand that this was happening. So some wells were
    unfortunately lost because of this error.)\
    \
    IN THIS CASE: this is a true NA value. It only happens at one time point (which may or
    may not be a heat day). And there is data for this well during the recovery period.

-   Some occurred as a result of prolonged heat exposure that dropped the total density in
    that well below the threshold of detection. This only happened on day 3 of heat for
    the longest heat treatment. There is data for this well during the recovery period.\
    \
    IN THIS CASE: this is a true NA value.

-   Others occurred as a result of prolonged heat exposure that drove the well to complete
    extinction. There is no flow cytometery data for this well during the recovery period
    because it went extinct. Extinction needs to be confirmed against the OD data.\
    \
    IN THIS CASE: this is a true NA value during the heat treatment **but it should become a 0 value during the recovery period.**

The OD data is analyzed in the file called `main_expt--OD_analysis.Rmd`.
This script outputs a csv file indicating the extinct wells, which I will use below.

```{r, extinctWells}
# import extinct well data from file
extinct <- read.csv("./intermediate_data/extinctOD_wells.csv")

# we know that there was no detectable growth on Recovery days. So replace the current values with true 0's here.
the.data[which(the.data$uniqID %in% extinct$uniqID & the.data$Recov_Day>0),] <- the.data[which(the.data$uniqID %in% extinct$uniqID & the.data$Recov_Day>0),] %>%
          mutate(Total_density=0, Conc_putida=0, Conc_protegens=0, Conc_grimontii=0, Conc_veronii=0,
                 relDen_putida=0, relDen_protegens=0, relDen_grimontii=0, relDen_veronii=0,
                 CommRich=putida+protegens+grimontii+veronii)
# during the heat days, we know that there was no OD-detectable growth for (extinct$Day + 1).
# This means any flow cytometry data we have is unreliable and should be replaced with NA.
  # wells where Day 2 is unreliable
tmp <- extinct %>% filter(Day == 1)
the.data[which(the.data$uniqID %in% tmp$uniqID & the.data$Day==2),] <- the.data[which(the.data$uniqID %in% tmp$uniqID & the.data$Day==2),] %>%
          mutate(Total_density=NA, Conc_putida=NA, Conc_protegens=NA, Conc_grimontii=NA, Conc_veronii=NA,
                 relDen_putida=NA, relDen_protegens=NA, relDen_grimontii=NA, relDen_veronii=NA,
                 CommRich=NA)
rm(tmp)
  # wells where Day 3 is unreliable (and Day 3 is a heat day!)
extinct <- extinct[-which(extinct$uniqID %in% c("24-08-19 Epoch B4", "24-08-19 Epoch D2")),]
the.data[which(the.data$uniqID %in% extinct$uniqID & the.data$Day==3),] <- the.data[which(the.data$uniqID %in% extinct$uniqID & the.data$Day==3),] %>%
          mutate(Total_density=NA, Conc_putida=NA, Conc_protegens=NA, Conc_grimontii=NA, Conc_veronii=NA,
                 relDen_putida=NA, relDen_protegens=NA, relDen_grimontii=NA, relDen_veronii=NA,
                 CommRich=NA)
rm(extinct)
```

## Distinguish between contamination & misclassification

To address both the problem of contamination & the problem of the misclassification rate
varying over time, I had to closely re-examine the flow cytometry raw data (which I did by eye, *insert crying emoji*).

From the Day 0 data, I hypothesized that the misclassification rate is ~1%. So let's pull up the identity of all the flow cytometry data where >1% of the relative density is attributed to a species that was not inoculated in that well (i.e., and therefore it should not be there). I then manually examined the flow cytometry raw data files in FCS Express for all the wells listed below:

```{r, contaminationANDmisclassification}
# identify contamination at 1%
contamin.df <- the.data %>%  filter((putida == 0 & relDen_putida > 0.01) |
                                    (protegens == 0 & relDen_protegens > 0.01) |
                                    (grimontii == 0 & relDen_grimontii > 0.01) |
                                    (veronii == 0 & relDen_veronii > 0.01))

contamin.df %>% filter(Date %in% c("24-08-05", "24-08-19")) %>% select(Date, FLOWplateWell, Day, community,
                                                                       relDen_putida, relDen_protegens, relDen_grimontii, relDen_veronii)
```

The gating for all the wells listed above (and more) was double-checked by eye in FCS Express and new cell counts were outputted. It seemed to me that there is a correlation between the heat environment, or at least the day of the serial transfer, and how clean or messy the gating looked. In particular it seemed to me that it was more difficult to classify species during heat days.

We know from the OD data (see `main_expt--OD_analysis`) that the 24h of heat treatment had no instance of contamination detected (i.e., at least for the blank wells). Since I suspect that the misclassification rate changes with the heat day, let's assume that the 24h heat treatment does not contain any contamination events then look at the occurence of species that were never inoculated in those wells as an estimate of the misclassification rate on different days of the serial transfer.

(In the future: it should also be possible to get a covariance matrix to estimate which species are being misclassified as which.)

```{r, misclass_24hHeat}
misclass24 <- the.data %>% filter(Day > 0, Heat == 24) %>%
                filter((putida == 0 & relDen_putida > 0) |
                       (protegens == 0 & relDen_protegens > 0) |
                       (grimontii == 0 & relDen_grimontii > 0) |
                       (veronii == 0 & relDen_veronii > 0))
# separate the correctly called species from the species that are absent
misclass24_REAL <- misclass24 %>% mutate(relDen_putida = putida * relDen_putida,
                                         relDen_protegens = protegens * relDen_protegens,
                                         relDen_grimontii = grimontii * relDen_grimontii,
                                         relDen_veronii = veronii * relDen_veronii)
misclass24 <- misclass24 %>% mutate(relDen_putida = abs(putida-1) * relDen_putida,
                                    relDen_protegens = abs(protegens-1) * relDen_protegens,
                                    relDen_grimontii = abs(grimontii-1) * relDen_grimontii,
                                    relDen_veronii = abs(veronii-1) * relDen_veronii)
# pivot longer so there's a column for species
misclass24_REAL <- misclass24_REAL %>% pivot_longer(cols = relDen_putida:relDen_veronii,
                                                    values_to = "relDen",
                                                    names_to = "species",
                                                    names_prefix = "relDen_") %>%
                    select(uniqID, Day, community, putida, protegens, grimontii, veronii,
                           Total_density, relDen, species)
misclass24 <- misclass24 %>% pivot_longer(cols = relDen_putida:relDen_veronii,
                                                    values_to = "relDen",
                                                    names_to = "species",
                                                    names_prefix = "relDen_") %>%
                select(uniqID, Day, community, putida, protegens, grimontii, veronii,
                       Total_density, relDen, species)
# remove the true species from the misclass data because these are now fake 0's
misclass24 <- misclass24[-which(misclass24$putida == 1 & misclass24$species == "putida"),]
misclass24 <- misclass24[-which(misclass24$protegens == 1 & misclass24$species == "protegens"),]
misclass24 <- misclass24[-which(misclass24$grimontii == 1 & misclass24$species == "grimontii"),]
misclass24 <- misclass24[-which(misclass24$veronii == 1 & misclass24$species == "veronii"),]
# remove the single contaminated sample
misclass24 <- misclass24[-which(misclass24$protegens == 0 & misclass24$species == "protegens" & misclass24$relDen > 0.75),]

ggplot(misclass24,
       aes(x=species, y=relDen, colour=species)) +
  facet_wrap(vars(Day)) +
  geom_beeswarm(alpha=0.5) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(y="relative density of misclassified",
       title="misclassification in 24h heat for different days")

# clean up
rm(misclass24, misclass24_REAL)
```
Recall that for 24h duration, Day 1 of serial transfer had 6h of extreme heat at the end, Day 2 was all extreme heat then returned to the "ambient" warm temperature only in the last few hours, and Days 3 & 4 were the recovery days with constant "ambient" warm temperature.

What we see from the plot above is that the misclassification rate can get as high as 20% (and that it does depend on the day but it seems that the first day of recovery is actually worse than the heat days themselves), although most replicates & days seem to be well behaved.

Therefore let's set 25% as the threshold for contamination. This means that any replicates that show >25% relative density for a species that was not inoculated there are defined as contaminated. All time-points from these contaminated replicates are completely removed from the downstream analysis.

```{r, plot_finalized_timeseries}
rm(contamin.df) # remove anything we may have had above.

# for now let's define contamination as >25% for something that should not be there.
contamin.df <- the.data %>%  filter((putida == 0 & relDen_putida > 0.25) |
                                    (protegens == 0 & relDen_protegens > 0.25) |
                                    (grimontii == 0 & relDen_grimontii > 0.25) |
                                    (veronii == 0 & relDen_veronii > 0.25))
tmp <- the.data[-which(the.data$uniqID %in% unique(contamin.df$uniqID)),]

###############
# output absolute density data for analysis
###############

# Day 0 would need to be the pre-dilution absolute densities
ggplot(tmp %>% filter(Day==0) %>% select(-uniqID, -OD_well, -Heat, -Incubator) %>% distinct(),
       aes(x=Date, y=Total_density)) +
  geom_beeswarm() +
  labs(y="")
ggplot(tmp %>% filter(Day==0) %>% select(-uniqID, -OD_well, -Heat, -Incubator) %>% distinct(),
       aes(x=Date, y=Volume)) +
  geom_beeswarm() +
  labs(y="")
# I lost the data on flow volume for Date 24-07-02.
# But we see from the plot that there's not *that* much variation between batches.

## IMPORTANT NOTE: the Day 0 data is not featured in any of the downstream analyses for this manuscript. Therefore this interpolation doesn't actually matter.

# let's interpolate the well volumes on Day 0 of 24-07-02 by using the median well volumes for all other dates on Day 0
tmp.Day0 <- tmp[which(tmp$Day==0 & tmp$Incubator=="Epoch"),] %>% select(-uniqID, -OD_well) %>% distinct()
  # get the median volume for Day 0
medianVol <- median(tmp.Day0$Volume, na.rm=TRUE)
  # apply the median volume to Day 0 values from 24-07-02
tmp.Day0 <- tmp.Day0 %>% filter(Date=="24-07-02")
tmp.Day0$Volume <- medianVol
  # recalculate the absolute densities for Day0
tmp.Day0 <- tmp.Day0 %>% mutate(Total_density = Total_counts/Volume,
                                Conc_putida = Count_putida/Volume,
                                Conc_protegens = Count_protegens/Volume,
                                Conc_grimontii = Count_grimontii/Volume,
                                Conc_veronii = Count_veronii/Volume)
# finally, join the estimated absolute densities for Day0 back in with the whole data
tmp.Day0.0702 <- left_join(tmp %>% filter(Day==0, Date=="24-07-02") %>% select(-Volume, -Total_density, -Conc_putida, -Conc_protegens, -Conc_grimontii, -Conc_veronii),
                           tmp.Day0)
tmp <- rbind(tmp %>% filter(Date != "24-07-02"),
             tmp %>% filter(Date == "24-07-02") %>% filter(Day > 0),
             tmp.Day0.0702)

rm(medianVol, tmp.Day0, tmp.Day0.0702)

# finally, remove known miscalled estimates from the data
tmp <- tmp %>% mutate(Conc_putida = putida * Conc_putida,
                      Conc_protegens = protegens * Conc_protegens,
                      Conc_grimontii = grimontii * Conc_grimontii,
                      Conc_veronii = veronii * Conc_veronii) %>%
        mutate(Total_density = Conc_putida + Conc_protegens + Conc_grimontii + Conc_veronii)

# output this data to file
absDensity <- tmp %>% filter(community != "0_0_0_0") %>%
                select(uniqID, Heat, Day, Heat_Day, Recov_Day, CommRich:veronii, Total_density:Conc_veronii)
save(absDensity, file="./intermediate_data/absolute_density_data.RData")

rm(tmp, com, contamin.df)
```

## Plot final time-series

```{r, plot_finalized_timeseries}
for(com in unique(absDensity$community)) {
  plot(ggplot(absDensity %>% filter(community==com) %>%
           select(uniqID, Heat, Day, Conc_putida, Conc_protegens, Conc_grimontii, Conc_veronii) %>%
              pivot_longer(cols=starts_with("Conc"), names_to="species", names_prefix="Conc_", values_to="absDensity"),
         aes(x=Day, y=absDensity, colour=species, group=paste(uniqID,Heat,species))) +
    facet_grid(~Heat) +
    geom_point(alpha=0.2) +
    geom_line(alpha=0.5) +
    scale_colour_manual(values=species_4pal_alphabetical) +
    labs(title=com))
}

# in the analyses below we will be interested in shannon diversity so let's already make a column for that
absDensity$Diversity <- diversity(absDensity[,c("Conc_putida", "Conc_protegens", "Conc_grimontii", "Conc_veronii")],
                                  index = "shannon")

# first let's remove the empty wells as we won't need them anymore
absDensity <- absDensity %>% filter(community != "0_0_0_0")
# Note that there are many 0 and NA values for Total_density
summary(absDensity$Total_density)
# 0's are communities that went extinct altogether and never recovered
absDensity[which(absDensity$Total_density == 0),]
# most NA's are communities below the threshold of detection during heat that later perhaps recovered
absDensity[which(is.na(absDensity$Total_density) & absDensity$Heat>12),]
# other NA's are just missing data (e.g., due to flow cytometry clogs or just plain pipetting mistakes)
absDensity[which(is.na(absDensity$Total_density) & absDensity$Heat<12),]

# the total density data will have to be slightly adjusted for fitting to the models
absDen_forFit <- absDensity %>% filter(Day > 0)
# for the "raw" total density data that will be fitted via negative binomial GLM,
  # keep the 0's in the data
  # but convert NA's into epsilon values (where epsilon is just below the threshold of detection)
below_threshold_rows <- which(is.na(absDen_forFit$Total_density) & absDen_forFit$Heat>12)
absDen_forFit$Total_density[below_threshold_rows] <- (0.25*50/146)
rm(below_threshold_rows)

# re-arrange the levels of Heat so that emmeans can be run:
absDen_forFit$Heat <- as.character(absDen_forFit$Heat)
absDen_forFit$Heat[which(absDen_forFit$Heat == 0)] <- "control"
# !!! emmeans expects the control to be the very *last* level !!!
absDen_forFit$Heat <- factor(absDen_forFit$Heat,
                             levels = c("6", "12", "24", "48", "control"))

# clean up
rm(com)
```

After plotting, I also created another version of the full data that will be used for fitting the data to models: ``. It excludes the Day 0 data (as this will not be analyzed hereafter).

Below, I will analyze the diversity and the productivity (AKA total density) to understand how
they change relative to the no heat control during the resistance and the recovery period.
The diversity is easier to deal with because we can use the Shannon diversity calculation
as implemented by `vegan`.

For the total abundances, there are extinction (AKA 0's) and NA events in this data -- the extinction events in particular are important and meaningful parts of our data!! To deal with this issue, I will distinguish between 0's and NA's by using a $x + \epsilon$ transformation, where $\epsilon$ indicates samples that are below the threshold of detection. I will use $\epsilon$ as 0.25 \* the threshold of detection for
the flow cytometer (which is 50 total fluorescent events in $146\mu L$).

## Load community growth traits expectations

We define "average growth rate of the community" either as the expectation from the inoculated communities (e.g., the quadruplet community has expected growth rate = mean of the 4 species). In other words, this assumes that all species that were inoculated in the community remain at equal ratios and therefore the average growth rate of a community is just the mean of the growth rates of the species that were inoculated there. This is called the `community_expected_mu`. (Recall that if we thought the communities would stay fixed for the equal species ratios that we inoculated them at, then we should use the geometric mean to calculate the community growth rate. We use the arithmetic mean because we *a priori* believe that the communities will tend to be dominated by the faster growing species.)

We also define it as the mean of the realized communities by using the species mean relative densities in the no heat control condition. In other words, this takes into account the actual relative densities of the species that can hang out together across serial transfers and uses that as an expectation of the community's growth rate. This is called the `community_averaged_mu`.

Another trait that we assayed was whether species are sensitive or resistant to 40C heat (resistance is a binary trait: either TRUE or FALSE). We define communities as expected to be resistant when at least one of the inoculated species is resistant. We define communities as expected to be sensitive when none of the species are resistant.

```{r, load_growth_rates30C}
# load in the stationary phase growth rate estimates from Expt1
load("./intermediate_data/expt1--all_growthcurve_data.RData")
rm(ALL_data.df, derivs.df, TTD.df) # keep just the dataframe with the growth rate estimates (mu)

# a look-up table for growth rates at 30C
growthrates.df <- Dil_growthrates.df %>% filter(Inoculum == "Stationary",
                                                Temp == 30,
                                                Sample %in% c("BSC001", "BSC005", "BSC019", "CK101")) %>%
                        arrange(desc(mu))

# a look-up table for resistance to 40C
resist.df <- Dil_growthrates.df %>% filter(Inoculum == "Stationary",
                                           Temp == 40,
                                           Sample %in% c("BSC001", "BSC005", "BSC019", "CK101")) %>%
                mutate(resistant = ifelse(mu>0, 1, 0)) %>%
                  arrange(match(Sample, c("BSC001", "CK101", "BSC019", "BSC005")))

# calculate the average growth rate for the inoculated communities
absDen_forFit <- absDen_forFit %>% mutate(community_expected_mu = (growthrates.df$mu[1]*putida + growthrates.df$mu[2]*protegens + growthrates.df$mu[3]*grimontii + growthrates.df$mu[4]*veronii)/(putida + protegens + grimontii + veronii))

# calculate the average growth rate for the realized communities
temp <- absDensity %>% filter(Heat == 0) %>% group_by(community) %>%
          mutate(relDen_putida = Conc_putida/Total_density,
                 relDen_protegens = Conc_protegens/Total_density,
                 relDen_grimontii = Conc_grimontii/Total_density,
                 relDen_veronii = Conc_veronii/Total_density) %>%
            summarise(relDen_putida = median(relDen_putida, na.rm = TRUE),
                      relDen_protegens = median(relDen_protegens, na.rm = TRUE),
                      relDen_grimontii = median(relDen_grimontii, na.rm = TRUE),
                      relDen_veronii = median(relDen_veronii, na.rm = TRUE)) %>%
              mutate(community_averaged_mu = growthrates.df$mu[1]*relDen_putida + growthrates.df$mu[2]*relDen_protegens + growthrates.df$mu[3]*relDen_grimontii + growthrates.df$mu[4]*relDen_veronii)

# get the community resistances
print(resist.df %>% select(Species, Sample, Temp, mu, resistant)) # recall that only putida is resistant
# so it's easy to get community resistance because it's just the presence/absence of putida

# add the information to the full data set
absDen_forFit <- inner_join(absDen_forFit, temp %>% select(community, community_averaged_mu)) %>%
                  mutate(resistant = putida)

# remember to also add the community growth rates and resistances to the other data frame (this one is used for the extinction analysis because Heat is numeric here)
absDensity <- inner_join(absDensity,
                         absDen_forFit %>%
                           select(community, community_expected_mu, community_averaged_mu, resistant) %>%
                             distinct())

# clean up
rm(temp, Dil_growthrates.df, resist.df, growthrates.df)
```

# Ordination of communities over time

After backing up a bit and thinking about what the main story of the paper could be, I
think the main message that I would like to tell with the paper is that heat duration has
a threshold effect. So while shorter and intermediate heat durations have some effect
during heat that is different from control, communities return to a similar state after
recovery. On the other hand, long duration heat events lead to extinction (i.e., either of
the entire community or of vulnerable species within the community) so the communities
cannot recover anymore. In other words, there's a threshold effect where the amount of
heat (or bacterial) induced killing has gone on for so long than it passes a critical
point and the communities recover to a different state. I don't want to use the term
"tipping point" but for sure the design of our experiment allows us to use phrases like
"threshold effect" and "critical transition" *sensu stricto* (e.g., as explained in
[Munson et al., 2018](https://doi.org/10.1111/nph.15145)).

I think it would be fantastic if I could produce a figure that summarizes the entire data
in a way that builds an argument for the quintessential ball-landscape schematic that
people keep showing when they talk about ecosystem stability to perturbation (e.g., see
schematic in Fig. 2 of [Shade et al., 2012](https://doi.org/10.3389/fmicb.2012.00417) or
the empirical figure in Fig. 3 of [Jurburg et al.,
2017](https://doi.org/10.1038/srep45691)).

Here are some tutorials on ordination:
<https://eddatascienceees.github.io/tutorial-rayrr13/>
<https://ourcodingclub.github.io/tutorials/ordination/>
<https://uw.pressbooks.pub/appliedmultivariatestatistics/chapter/anosim/>
<https://uw.pressbooks.pub/appliedmultivariatestatistics/chapter/visualizing-and-interpreting-ordinations/>

To calculate the Bray-Curtis dissimilarity, we are forced to choose how to deal with NA values (most of
which are found in the resistance time points and so it doesn't really make sense to outright drop them). NA
values exist for two reasons:

1.  "true" missing data where the well was not acquired at all due to technical
    difficulties/mistakes (only a few of this type). I used interpolation to deal with these: use the median value from other replicates at that same community:day:treatment.

2.  below threshold of detection missing data where the total density was too low to
    reliably estimate the cell counts. I replace the NA with the limit of detection of the cytometer (epsilon = 0.25*50/146, as above) and assume equal frequencies of the species that were inoculated in that community.
    
Then, if we just follow the example tutorials directly, with columns = four species and rows =
different communities on different days for different heat treatments, then the data
simply gets split up by species. But that's not what we want to understand in this case.

We want to understand how the communities are changing over time (and as a function of different heat durations) so let's give it the data
as species x time. This can be achieved by widening the data so that we have abundances of
the 4 species during resistance, during early recovery, and during late recovery.

Note that I also had to keep just 3 time points from the control treatment. I chose to keep day 1 (coded as "resistance"), day 3 (coded as "early recovery"), and day 5 (coded as "late recovery") because this way the ordination plot will show the control treatment early, middle, and late in the time series... 

```{r, NMDS_create_wide_matrix}
# go back to the complete data that includes NA values for all 4 species on some days
absDen_forOrd <- absDen_forFit %>% select(-Diversity, -community_expected_mu, -community_averaged_mu, -resistant)
# NA values with Total_density == NA are "true" missing data where I failed to record the flow cytometry measurements on that day due to technical difficulties/mistakes. These can be interpolated by using the median values from the remaining community replicates
  ## get the median values for all communities, days, and heat treatments
median_vals <- absDen_forOrd %>% group_by(Heat, Day, community) %>%
                  summarise(Med_putida = median(Conc_putida, na.rm=TRUE),
                            Med_protegens = median(Conc_protegens, na.rm=TRUE),
                            Med_grimontii = median(Conc_grimontii, na.rm=TRUE),
                            Med_veronii = median(Conc_veronii, na.rm=TRUE))
  ## get the index for the rows with "true" missing values
missing_rows <- which(is.na(absDen_forOrd$Total_density))
  ## loop through the missing values
for(i in missing_rows){
  # find the interpolation value in the table of median values
  temp_med_val <- median_vals[median_vals$Heat == absDen_forOrd$Heat[i] &
                                median_vals$Day == absDen_forOrd$Day[i] &
                                median_vals$community == absDen_forOrd$community[i],]
  # replace the NA values with the median values
  absDen_forOrd$Conc_putida[i] <- temp_med_val$Med_putida
  absDen_forOrd$Conc_protegens[i] <- temp_med_val$Med_protegens
  absDen_forOrd$Conc_grimontii[i] <- temp_med_val$Med_grimontii
  absDen_forOrd$Conc_veronii[i] <- temp_med_val$Med_veronii
  # clean up
  rm(temp_med_val)
}
# clean up
rm(median_vals, missing_rows, i)

# on the other hand, NA values where Total_density is epsilon represent flow cytometry counts that were below the threshold of detection. In this case let's assume 1:1 ratios of inoculated strains at a total density equal to epsilon.
epsilon <- (0.25*50/146)
  ## get the index for the missing value rows below the threshold of detection
missing_rows <- which(is.na(absDen_forOrd$Conc_putida))
  ## CommRich NA values were supposed to indicate some differences but that doesn't really matter for us anymore
absDen_forOrd$CommRich <- absDen_forOrd$putida + absDen_forOrd$protegens + absDen_forOrd$grimontii + absDen_forOrd$veronii
for(i in missing_rows){
  # replace the NA values with epsilon divided by the inoculated species richness
  absDen_forOrd$Conc_putida[i] <- absDen_forOrd$putida[i] * epsilon / absDen_forOrd$CommRich[i]
  absDen_forOrd$Conc_protegens[i] <- absDen_forOrd$protegens[i] * epsilon / absDen_forOrd$CommRich[i]
  absDen_forOrd$Conc_grimontii[i] <- absDen_forOrd$grimontii[i] * epsilon / absDen_forOrd$CommRich[i]
  absDen_forOrd$Conc_veronii[i] <- absDen_forOrd$veronii[i] * epsilon / absDen_forOrd$CommRich[i]
}
# re-order the levels of Heat for better plotting
absDen_forOrd$Heat <- factor(absDen_forOrd$Heat, levels=c("control", "6", "12", "24", "48"))
# finally, we can drop the total density column
absDen_forOrd <- absDen_forOrd %>% select(-Total_density)
rm(epsilon, missing_rows, i)

# first we have to widen the data:
# create a column that indicates the treatment day as resistance, early recovery, or late recovery
absDen_forOrd$trtmt_day <- "resist"
absDen_forOrd$trtmt_day[absDen_forOrd$Recov_Day == 1] <- "early_recov"
absDen_forOrd$trtmt_day[absDen_forOrd$Recov_Day == 2] <- "late_recov"
# ENTIRELY ARBITARARILY: I will keep days 1, 3, and 5 for control
absDen_forOrd$trtmt_day[absDen_forOrd$Heat == "control" & absDen_forOrd$Day == 3] <- "early_recov"
absDen_forOrd$trtmt_day[absDen_forOrd$Heat == "control" & absDen_forOrd$Day == 5] <- "late_recov"
# remove day 1 for 12h, 24h, 48h AND day 2 for 48h.
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == 12 & absDen_forOrd$Day == 1), ]
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == 24 & absDen_forOrd$Day == 1), ]
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == 48 & absDen_forOrd$Day == 1), ]
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == 48 & absDen_forOrd$Day == 2), ]
# also remove day 2 and day 4 for control.
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == "control" & absDen_forOrd$Day == 2), ]
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == "control" & absDen_forOrd$Day == 4), ]

# pivot wider to create a column for each of the 4 species on each of the 3 days
absDen_wide_forOrd <- absDen_forOrd %>% select(-Day, -Heat_Day, -Recov_Day) %>%
                          pivot_wider(names_from = trtmt_day,
                                      values_from = c(Conc_putida, Conc_protegens, Conc_grimontii, Conc_veronii))

# re-name the species abundance over time columns so they are shorter (again for better plotting)
colnames(absDen_wide_forOrd)[9:20] <- c("Pu_Resist", "Pu_earlyR", "Pu_lateR",
                                        "Pt_Resist", "Pt_earlyR", "Pt_lateR",
                                        "Gi_Resist", "Gi_earlyR", "Gi_lateR",
                                        "Vn_Resist", "Vn_earlyR", "Vn_lateR")

```

Now that we have the wide data, let's calculate the distances and do the ordination with NMDS:

```{r, NMDS_analysis_plots_signif}
# The final result depends on the initial random placement of the points 
# set seed to make the results reproducible
set.seed(64576)

# keep just the species abundances
abundance_matrix <- as.matrix(absDen_wide_forOrd[,9:20])

# a function to automatically run the NMDS for k = 1 to 10 so we can choose appropriately small number of dimensions for ordination
NMDS.scree <- function(mat) { #where x is the abundance matrix
  data.frame(k = 1:10,
            # autotransform the data before calculating the bray-curtis dissimilarity
            stress = sapply(1:10, function(x) metaMDS(mat, distance = "bray", k = x, autotransform = TRUE)$stress))
}
scree_out <- NMDS.scree(abundance_matrix)
plot(scree_out)
# k=3 looks great
try.NMDS <- metaMDS(abundance_matrix, distance = "bray", k = 3, autotransform = TRUE, trymax=100)

# check the stress value. It should be < 0.2, ideally even < 0.05. (But too low stress values can indicate too many 0 values)
try.NMDS$stress

# let's get a general idea of what this NMDS is separating...

# plot the results for axis 1 & 2
ordiplot(try.NMDS, type = "n") # create blank ordination plot
orditorp(try.NMDS, display = "sites", cex = 0.5, air = 0.1) # add row numbers in black
orditorp(try.NMDS, display = "species", col="red", air = 0.1) # add species names in red

# plot the results for axis 1 & 3
ordiplot(try.NMDS, choices = c(1,3), type = "n") # create blank ordination plot
orditorp(try.NMDS, choices = c(1,3), display = "sites", cex = 0.5, air = 0.1) # add row numbers in black
orditorp(try.NMDS, choices = c(1,3), display = "species", col="red", air = 0.1) # add species names in red

# plot the results for axis 2 & 3
ordiplot(try.NMDS, choices = c(2,3), type = "n") # create blank ordination plot
orditorp(try.NMDS, choices = c(2,3), display = "sites", cex = 0.5, air = 0.1) # add row numbers in black
orditorp(try.NMDS, choices = c(2,3), display = "species", col="red", air = 0.1) # add species names in red


# we already know that presense/absence of protegens is consistently the most important thing for all communities so let's see if that shows up here.
# Let's switch over to ggplot to be certain that everything is labelled correctly.

# define a function (related to vegan) that finds coordinates for drawing a covariance ellipse
  # CREDIT: THIS COMES FROM ONE OF THE TUTORIALS ABOVE!!!
veganCovEllipse <- function (cov, center = c(0, 0), scale = 1, npoints = 100) {
  theta <- (0:npoints) * 2 * pi/npoints
  Circle <- cbind(cos(theta), sin(theta))
  t(center + scale * t(Circle %*% chol(cov)))
  # finds the centroids and dispersion of the different ellipses based on a grouping factor of your choice
}

nmds_for_ggplot <- cbind(absDen_wide_forOrd[,1:8],
                         as.data.frame(scores(try.NMDS)$sites))
# create a new factor that defines the combination of heat and protegens
nmds_for_ggplot <- nmds_for_ggplot %>% unite("HeatxProtegens", c(Heat, protegens), remove = FALSE)
nmds_for_ggplot$HeatxProtegens <- factor(nmds_for_ggplot$HeatxProtegens,
                                         levels = c("6_0", "6_1", "12_0", "12_1", "24_0", "24_1", "48_0", "48_1", "control_0", "control_1"))



# create empty dataframes to combine NMDS data with ellipse data
ellipse12_df <- ellipse13_df <- ellipse23_df <- data.frame() # numbers indicate the ordination axes
# adding data for ellipses, using HeatxProtegens as a grouping factor
for(g in levels(nmds_for_ggplot$HeatxProtegens)){
  ellipse12_df <- rbind(ellipse12_df, cbind(as.data.frame(with(nmds_for_ggplot[nmds_for_ggplot$HeatxProtegens==g,],
                                                     veganCovEllipse(cov.wt(cbind(NMDS1, NMDS2),
                                                                            wt=rep(1/length(NMDS1),length(NMDS1)))$cov,
                                                                     center=c(mean(NMDS1),mean(NMDS2)))))
                                  , HeatxProtegens=g))
  ellipse13_df <- rbind(ellipse13_df, cbind(as.data.frame(with(nmds_for_ggplot[nmds_for_ggplot$HeatxProtegens==g,],
                                                     veganCovEllipse(cov.wt(cbind(NMDS1, NMDS3),
                                                                            wt=rep(1/length(NMDS1),length(NMDS1)))$cov,
                                                                     center=c(mean(NMDS1),mean(NMDS3)))))
                                  , HeatxProtegens=g))
  ellipse23_df <- rbind(ellipse23_df, cbind(as.data.frame(with(nmds_for_ggplot[nmds_for_ggplot$HeatxProtegens==g,],
                                                     veganCovEllipse(cov.wt(cbind(NMDS2, NMDS3),
                                                                            wt=rep(1/length(NMDS2),length(NMDS2)))$cov,
                                                                     center=c(mean(NMDS2),mean(NMDS3)))))
                                  , HeatxProtegens=g))
}

# now we separate the HeatxProtegens columns:
ellipse12_df <- ellipse12_df %>% separate(HeatxProtegens, c("Heat", "protegens"))
ellipse12_df$Heat <- factor(ellipse12_df$Heat, levels = levels(nmds_for_ggplot$Heat))
ellipse13_df <- ellipse13_df %>% separate(HeatxProtegens, c("Heat", "protegens"))
ellipse13_df$Heat <- factor(ellipse13_df$Heat, levels = levels(nmds_for_ggplot$Heat))
ellipse23_df <- ellipse23_df %>% separate(HeatxProtegens, c("Heat", "protegens"))
ellipse23_df$Heat <- factor(ellipse23_df$Heat, levels = levels(nmds_for_ggplot$Heat))

nmds_for_ggplot$protegens <- as.character(nmds_for_ggplot$protegens) # this needs to be discrete (could also be a factor)

# and finally we can make the plots:
ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS2)) +
    geom_point(aes(color = Heat, shape = protegens), alpha=0.4) + # adding different colours and shapes for points at different distances
    geom_path(data=ellipse12_df, aes(x=NMDS1, y=NMDS2, colour=Heat, linetype=protegens), linewidth=1) + # adding covariance ellipses according to distance # use size argument if ggplot2 < v. 3.4.0
    guides(color = guide_legend(override.aes = list(linetype=rep(NA,5)))) + # removes lines from colour part of the legend
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + # not sure why I need this but I do to over-write the default grey theme
    labs(title="NMDS of all data (4sp & 3 time-points)")
# axes 1 & 2 again showing just the ellipses (bc it's hard to see protegens effects as it's so overlapped)
ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS2)) +
    geom_path(data=ellipse12_df, aes(x=NMDS1, y=NMDS2, colour=Heat, linetype=protegens), linewidth=1) + # plot just the ellipses
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + # not sure why I need this but I do to over-write the default grey theme
    labs(title="NMDS of all data (4sp & 3 time-points)")


ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS3)) +
    geom_point(aes(color = Heat, shape = protegens), alpha=0.4) + 
    geom_path(data=ellipse13_df, aes(x=NMDS1, y=NMDS3, colour=Heat, linetype=protegens), linewidth=1) + 
    guides(color = guide_legend(override.aes = list(linetype=rep(NA,5)))) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + 
    labs(title="NMDS of all data (4sp & 3 time-points)")
# axes 1 & 3 again showing just the ellipses
ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS3)) +
    geom_path(data=ellipse13_df, aes(x=NMDS1, y=NMDS3, colour=Heat, linetype=protegens), linewidth=1) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + 
    labs(title="NMDS of all data (4sp & 3 time-points)")

ggplot(data = nmds_for_ggplot, aes(NMDS2, NMDS3)) +
    geom_point(aes(color = Heat, shape = protegens), alpha=0.4) + 
    geom_path(data=ellipse23_df, aes(x=NMDS2, y=NMDS3, colour=Heat, linetype=protegens), linewidth=1) + 
    guides(color = guide_legend(override.aes = list(linetype=rep(NA,5)))) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + 
    labs(title="NMDS of all data (4sp & 3 time-points)")
# axes 2 & 3 again showing just the ellipses (bc it's hard to see protegens effects as it's so overlapped)
ggplot(data = nmds_for_ggplot, aes(NMDS2, NMDS3)) +
    geom_path(data=ellipse23_df, aes(x=NMDS2, y=NMDS3, colour=Heat, linetype=protegens), linewidth=1) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + 
    labs(title="NMDS of all data (4sp & 3 time-points)")

################
# check significance:
# using a PERMANOVA to test the differences in community composition
# This is a PERmutational Multivariate ANalysis Of VAriance and tests the differences between groups, like an ANOVA, but with lots of variables.
# it is essentially a multivariate analysis of variance used to compare groups of objects
nmdsdata_test_Heat <- adonis2(abundance_matrix ~ Heat, absDen_wide_forOrd,
                              permutations = 999, method = "bray")
print(nmdsdata_test_Heat)

nmdsdata_test_Prot <- adonis2(abundance_matrix ~ protegens, absDen_wide_forOrd,
                              permutations = 999, method = "bray")
print(nmdsdata_test_Prot)

nmdsdata_test_HeatxProt <- adonis2(abundance_matrix ~ Heat * protegens, absDen_wide_forOrd,
                                   permutations = 999, method = "bray")
print(nmdsdata_test_HeatxProt)

# so these are all significant but is that spurious because the dispersion is different btw groups? (e.g., much smaller for protegens)

##############
# check PERMANOVA assumption of homogeneous group variances
# Bray-curtis distance matrix
dist_mat <- vegdist(abundance_matrix, method = "bray")

# use betadisper test to check for multivariate homogeneity of group variances
dispersion <- betadisper(dist_mat, group = paste(absDen_wide_forOrd$Heat, absDen_wide_forOrd$protegens))
permutest(dispersion)

# yeap! We need to try a different test that is robust to heterogenous group variances...

################
# check significance:
# let's test for significance again using ANOSIM (which is another non-parametric test but this time only considering the ranks)
nmdsdata_test2_HeatxProt <- anosim(dist_mat,
                                   grouping = paste(absDen_wide_forOrd$Heat, absDen_wide_forOrd$protegens),
                                   permutations = 999)
plot(nmdsdata_test2_HeatxProt)
summary(nmdsdata_test2_HeatxProt)

# okay, let's say that we are satisfied with this significance testing...

test <- absDen_wide_forOrd
test$Heat <- as.character(levels(test$Heat))[test$Heat]
test$Heat[test$Heat == "control"] <- 0
test$Heat <- as.numeric(test$Heat)

# let's see what the heat gradient looks like
gg_ordiplot(try.NMDS, groups = absDen_wide_forOrd$protegens, plot = TRUE)
gg_envfit(try.NMDS, env = test$Heat, groups = absDen_wide_forOrd$protegens, plot = TRUE, alpha=0.5) # notice this gradient is not significant!!!
gg_envfit(try.NMDS, env = test$Heat, groups = absDen_wide_forOrd$protegens, plot = TRUE, alpha=0.5, choices=c(1,3))
gg_envfit(try.NMDS, env = test$Heat, groups = absDen_wide_forOrd$protegens, plot = TRUE, alpha=0.5, choices=c(2,3))
```

Great!, This summarizes the same result that I found with the other indices: presence of P.protegens is the most important thing. Communities where this species was present look quite similar across different heat treatments. Longer heat durations push the communities toward different direction, until a threshold is reached at the longest heat treatment (48h).

The NMDS ordination results are significant by PERMANOVA but the assumptions of that test might be violated because the dispersal is heterogeneous between groups. I think ANOSIM should be somewhat more robust to this problem because it uses ranks. The NMDS ordination results are significant by ANOSIM.

```{r, NMDS_plot_for_pub}
################################
# Plot figure for main text: Figure 3b
################################
# change protegens values for better plotting
nmds_for_ggplot$P_protegens <- "absent"
nmds_for_ggplot$P_protegens[nmds_for_ggplot$protegens == 1] <- "present"

ellipse12_df$P_protegens <- "absent"
ellipse12_df$P_protegens[ellipse12_df$protegens == 1] <- "present"

ellipse13_df$P_protegens <- "absent"
ellipse13_df$P_protegens[ellipse13_df$protegens == 1] <- "present"

# change Heat values for better plotting
levels(nmds_for_ggplot$Heat)[2:5] <- paste(levels(nmds_for_ggplot$Heat)[2:5], "hrs")
levels(ellipse12_df$Heat)[2:5] <- paste(levels(ellipse12_df$Heat)[2:5], "hrs")
levels(ellipse13_df$Heat)[2:5] <- paste(levels(ellipse13_df$Heat)[2:5], "hrs")

# create the plot of 1 vs 2:
plot1_2 <- ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS2)) +
            geom_point(aes(color = Heat, shape = P_protegens), size=2, alpha=0.4) + # adding different colours and shapes for points at different distances
            geom_path(data=ellipse12_df, aes(x=NMDS1, y=NMDS2, colour=Heat, linetype=P_protegens), linewidth=1) + # adding covariance ellipses according to distance # use size argument if ggplot2 < v. 3.4.0
            guides(color = guide_legend(override.aes = list(linetype=rep(NA,5),# removes lines from colour part of the legend
                                                  alpha=1, size=3)), # make the points opaque and bigger in the colour part of the legend
            shape = guide_legend(override.aes = list(size=3))) + # make the points bigger in the greyscale part of the legend
            scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9)

# plot 1 vs 2 with the legend ... I will extract the legend from here
png(filename="./figures/Fig3_A_legend.png", width = 3.48, height = 3.41, units = "in", res=300)
print(plot1_2)
dev.off()


# plot 1 vs 2 without the legend
png(filename="./figures/Fig3_A_axis1vs2.png", width = 5.35, height = 3.78, units = "in", res=300)
print(plot1_2 + theme(legend.position="none"))
dev.off()

# plot 1 vs 3 without the legend
png(filename="./figures/Fig3_A_axis1vs3.png", width = 5.35, height = 3.78, units = "in", res=300)

ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS3)) +
    geom_point(aes(color = Heat, shape = P_protegens), size=2, alpha=0.4) + 
    geom_path(data=ellipse13_df, aes(x=NMDS1, y=NMDS3, colour=Heat, linetype=P_protegens), linewidth=1) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    theme(legend.position="none")

dev.off()

# clean up
rm(abundance_matrix, scree_out, try.NMDS, nmds_for_ggplot, ellipse12_df, ellipse13_df, ellipse23_df, nmdsdata_test_Heat, nmdsdata_test_Prot, nmdsdata_test_HeatxProt, dist_mat, dispersion, nmdsdata_test2_HeatxProt, test, absDen_forOrd, absDen_wide_forOrd, g, plot1_2)
```

# Richness

Let's summarize the main result that P. protegens dominates all communities where it was inoculated. The species richness is conceptually a good metric for this ... but recall that the flow cytometry data has a some rate of misclassification (in some cases as much as 20% !!!). So we need to use richness estimates that take into account the proportion of species and are more likely to ignore rare species.

## Plot

```{r, richness}
species_div.df <- absDen_forFit %>% mutate(relden_putida = Conc_putida/Total_density,
                                 relden_protegens = Conc_protegens/Total_density,
                                 relden_grimontii = Conc_grimontii/Total_density,
                                 relden_veronii = Conc_veronii/Total_density)
species_div.df <- species_div.df %>% mutate(HillEven_q0 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillEven",
        q=0)),
                              HillEven_q1 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillEven",
        q=1)),
        HillEven_q2 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillEven",
        q=2)),
        HillDiv_q1 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillDiv",
        q=1)),
        HillDiv_q2 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillDiv",
        q=2)))

ggplot(species_div.df,
       aes(y=HillEven_q0, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis", begin=0.1)

ggplot(species_div.df,
       aes(y=HillEven_q1, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis", begin=0.1)

ggplot(species_div.df,
       aes(y=HillEven_q2, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis", begin=0.1)

ggplot(species_div.df,
       aes(y=HillDiv_q1, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis")

ggplot(species_div.df,
       aes(y=HillDiv_q2, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis")
# this one gives infinite values. That's not useful.


################################
# Plot figure for main text: Figure 3b
################################

# The Hill Diversity with q=1 seems useful!! Let's include this plot in the final manuscript:
# re-order  the levels of heat so that the control appears first
species_div.df$Heat <- factor(species_div.df$Heat,
                              levels = levels(species_div.df$Heat)[c(5,1:4)])
# change variable names for nicer plotting
levels(species_div.df$Heat)[2:5] <- paste(levels(species_div.df$Heat)[2:5], "hrs")
species_div.df$Pprot_facet <- ifelse(species_div.df$protegens == 0, "P. protegens absent", "P. protegens present")

# create a data.frame for plotting red rectangles in the background
bckgrd <- data.frame(Heat=levels(species_div.df$Heat),
                     HillDiv_q1 = c(0, rep(2.4, 4)),
                     Day = c(0, rep(0.8, 4))) # all heat treatments start at the same time
test <- rbind(bckgrd,
              data.frame(Heat=levels(species_div.df$Heat),
                         HillDiv_q1 = c(0, rep(2.4, 4)),
                         Day = c(0, 1.1, 1.5, 1.9, 2.9))) # choose end points that look good even if not perfectly accurate

png(filename = "./figures/Fig3_B.png", width = 6.98, height = 4.52, units = "in", res = 300)

ggplot(species_div.df %>% filter(CommRich > 1),
        # exclude monocultures bc richness is not informative for these: their richness will always be equal to 1 even when they go extinct. 
       aes(y=HillDiv_q1, x=Day)) +
  facet_grid(Pprot_facet ~ as.factor(Heat),
             scales = "free_x", # allow x-axis of facets to freely choose their own max values
             space = "free_x") + # allow facet columns to differ in their sizes
  # add red rectangles in the background to indicate heat treatment as in Fig 1.
  geom_ribbon(data=test, aes(ymin=0.95, ymax=2.4), # add a bit of padding above & below the points to look nice
              position = "identity", # not sure this is needed now that I've switched away from geom_area? But it works so I don't care
              fill="#C43131", alpha=0.25) + # use same colour and alpha as in fill for Fig. 1
  # use beeswarm to jitter the points properly (alpha must be set to 0 bc of red rectangles)
  geom_quasirandom(aes(fill=as.factor(CommRich)), # fill as a function of CommRich *must* be inside of this function otherwise it leads to a lot of problems with the geom_ribbon layer
                   pch=21) + # use points with fill and border bc they look nicer here
  scale_fill_viridis_d(option = "viridis", begin=0.85, end=0) +
  scale_x_continuous(breaks=1:5, # tick marks & tick labels only at integers
                     limits=c(0.5,NA), # add a little extra padding on the left side of x-axis bc I think it looks better
                     expand = c(0, 0.2)) + # for some reason this prevents points in the 6h facet from getting squished up against the right border of the facet
  scale_y_continuous(expand = c(0, 0)) + # this prevents ggplot from adding any extra padding in the y-axis; we already added the defauly +/- 0.05 padding manually when we specified geom_ribbon
  labs(y = "Observed Richness", # 1st order Hill Diversity
       fill = "Inoculated\nRichness") +
  guides(fill = guide_legend(override.aes = list(size=3), # make the points bigger in the legend
                             title.hjust = 0.5)) + # justify the 2nd line of legend title & points so they sit nicely under the 1st line
  theme(strip.background = element_rect(fill = "white", colour="black"),
        strip.text = element_text(color = "black"),
        legend.box.spacing = margin(0.5), # stretch plot rightwards & closer to its legend
        legend.title = element_text(size=14)) # make legend title a little smaller

dev.off()

# clean up
rm(species_div.df, bckgrd, test)
```

Notice that for the control condition in the absence of protegens, there is a trend of decreasing species richness over time (e.g., as the communities reach equilibrium).

( ## Analysis?? )

I have effect sizes on richness for resistance and recovery but I'm not sure if this is going to make it into the manuscript... (see the `analyze_temp_serial_transfer_expt--28Oct24.html` if you're interested).

# Extinction

Which communities were most likely to go extinct? How long did the heat duration have to be in order to drive those communities to extinction?

The simplest hypothesis is that heat duration alone explains whether a community happens to go extinct.

A slightly more complex hypothesis from the thermal performance curve data (Fig. 2) would
be that any species that is resistant to heat should be less likely to go extinct, even long duration heat. Therefore we would expect that the presence/absence of the heat resistant species, P. putida, should explain whether a community goes extinct.

Maddy's hypothesis in setting up this experiment was that a higher inoculated species richness would make a community more resistant to heat. So we are going to check whether the inoculated species richness has any effect.

Another hypothesis that emerges from looking at the time series data itself (e.g., the ordination data) is that protegens has a unique effect on all communities where it is present. So let's check that model as well.

## Analysis

```{r, extinction_InnocRichness}
# keep just the data on the last day of each time series
extinct.df <- absDensity %>% filter(Recov_Day == 2)
extinct.df <- rbind(extinct.df,
                    absDensity %>% filter(Heat == 0, Day == 5))
# binary vector of survival or extinction
extinct.df <- extinct.df %>% mutate(survived = ifelse(Total_density > 0, 1, 0))
### note that sample "24-07-08 Epoch G1" has missing data on Day 5 even though we know from the OD data that it survived.
extinct.df$survived[extinct.df$uniqID == "24-07-08 Epoch G1"] <- 1

# make protegens into a factor
extinct.df$protegens <- factor(extinct.df$protegens)

# fit the models
ext_mod.heat <- glmmTMB(survived ~ Heat,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat, plot = TRUE)

ext_mod.heat_plus_rich <- glmmTMB(survived ~ CommRich + Heat,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_plus_rich, plot = TRUE)

ext_mod.heat_plus_prot <- glmmTMB(survived ~ Heat + protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = ext_mod.heat_plus_prot, plot = TRUE)

ext_mod.heat_plus_resist <- glmmTMB(survived ~ Heat + resistant,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_plus_resist, plot = TRUE)


ext_mod.heat_by_rich <- glmmTMB(survived ~ CommRich*Heat,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_by_rich, plot = TRUE)

ext_mod.heat_by_prot <- glmmTMB(survived ~ Heat*protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_by_prot, plot = TRUE)


ext_mod.heat_rich_prot <- glmmTMB(survived ~ CommRich + Heat + protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_rich_prot, plot = TRUE)

ext_mod.rich_heatXprot <- glmmTMB(survived ~ CommRich + Heat*protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.rich_heatXprot, plot = TRUE)

#anova(ext_mod.heat, ext_mod.heat_plus_rich)
#anova(ext_mod.heat, ext_mod.heat_by_rich)
#anova(ext_mod.heat_plus_rich, ext_mod.heat_by_rich)

AIC(ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_resist) %>% arrange(AIC)
AICc(ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_resist) %>% arrange(AICc)
BIC(ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_resist) %>% arrange(BIC)

summary(ext_mod.heat_rich_prot)
summary(ext_mod.heat_plus_prot)

anova(ext_mod.heat, ext_mod.heat_plus_prot)
anova(ext_mod.heat_plus_prot, ext_mod.heat_rich_prot)

# and let's report the R-squared for this 
efronRSquared(residual = residuals(ext_mod.heat_plus_prot, type="response"), 
              predicted = predict(ext_mod.heat_plus_prot, type="response"), 
              statistic = "EfronRSquared")
```

This tells us that the most important predictors are: 1. the duration of the heat event and 2. the presence/absence of protegens in the inoculated community. This model explains about 50% of the variation in the data. We have little power to detect an effect of inoculated community richness on the extinction.

A final possibility is that the growth rates of the different communities can explain whether they go extinct. Let's check if the average growth rate of the community at 30C can predict its extinction...

```{r, extinction_InnocRichness}
ext_mod.expect_mu <- glmmTMB(survived ~ Heat + community_expected_mu,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.expect_mu, plot = TRUE)

ext_mod.exptmu_prot <- glmmTMB(survived ~ Heat + community_expected_mu + protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = ext_mod.exptmu_prot, plot = TRUE)

ext_mod.averaged_mu <- glmmTMB(survived ~ Heat + community_averaged_mu,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.averaged_mu, plot = TRUE)

ext_mod.avemu_prot <- glmmTMB(survived ~ Heat + community_averaged_mu + protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = ext_mod.avemu_prot, plot = TRUE)

ext_mod.exptmu_prot_resist <- glmmTMB(survived ~ Heat + community_expected_mu + protegens + resistant,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.exptmu_prot_resist, plot = TRUE)

summary(ext_mod.exptmu_prot)
summary(ext_mod.avemu_prot)
# this model does not converge bc of the experimental design (some predictor combinations are unitary)
summary(ext_mod.exptmu_prot_resist)

## this should go in a single table instead of 3 diferent ones:
AIC(ext_mod.heat, ext_mod.heat_plus_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_resist, ext_mod.expect_mu, ext_mod.averaged_mu, ext_mod.exptmu_prot, ext_mod.avemu_prot, ext_mod.exptmu_prot_resist) %>% arrange(AIC)
AICc(ext_mod.heat, ext_mod.heat_plus_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_resist, ext_mod.expect_mu, ext_mod.averaged_mu, ext_mod.exptmu_prot, ext_mod.avemu_prot, ext_mod.exptmu_prot_resist) %>% arrange(AICc)
BIC(ext_mod.heat, ext_mod.heat_plus_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_resist, ext_mod.expect_mu, ext_mod.averaged_mu, ext_mod.exptmu_prot, ext_mod.avemu_prot, ext_mod.exptmu_prot_resist) %>% arrange(BIC)

# wow, I'm shocked that this growth rate model is actually better. Let's double check that...
anova(ext_mod.heat_plus_prot, ext_mod.exptmu_prot)
anova(ext_mod.heat_plus_prot, ext_mod.avemu_prot)

# and let's report the R-squared
efronRSquared(residual = residuals(ext_mod.exptmu_prot, type="response"), 
              predicted = predict(ext_mod.exptmu_prot, type="response"), 
              statistic = "EfronRSquared")

efronRSquared(residual = residuals(ext_mod.avemu_prot, type="response"), 
              predicted = predict(ext_mod.avemu_prot, type="response"), 
              statistic = "EfronRSquared")
```

Ok, so neither the community_expected_mu nor the community_averaged_mu are as good predictors as just heat duration and presence/absence of protegens. But when we add the presence/absence of protegens to either the community_expected_mu or community_averaged_mu, we get very good models that are significantly better than the one reported above. e.g., The model with just heat duration and protegens presence/absence explains 50% of the variation, while models explain 70-75% of the variation. Another way to report this result is to give the $\delta$AIC or $\delta$BIC.

Although the community_averaged_mu is a better fit, I prefer to use the community_expected_mu because it fits better in the framework of "can we predict microbial community dynamics from the species traits?" Moreover, the fit (at least for the extinction data) is not that much worse for the community_expected_mu as compared to the community_averaged_mu -- and both are insufficient to explain the data without the effect of protegens anyway!

```{r, remove_community_averaged_mu}
absDensity <- absDensity %>% select(-community_averaged_mu)
absDen_forFit <- absDen_forFit %>% select(-community_averaged_mu)
extinct.df <- extinct.df %>% select(-community_averaged_mu)
```

## Plot

Plot the preferred model against the data: growth on final day ~ heat duration + protegens present + community_expected_mu

```{r, extinction_plot}
# create data.frame for plotting
extinct.df <- cbind(extinct.df,
                    predicted = predict(ext_mod.exptmu_prot, type="response"))
# plot the predictions against the data
plot(ggplot(extinct.df,
        aes(x=as.factor(Heat),
            y=survived,
            colour=community_expected_mu, 
            group=as.factor(community_expected_mu))) +
    facet_wrap(. ~ protegens,
               labeller = as_labeller(c(`0`="P. protegens absent",
                                        `1`="P. protegens present"))) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_line(aes(y = predicted)) +
    geom_jitter(alpha=0.6, width=0.1, height = 0.25) +
      # would be nice to use beeswarm package but not sure how to do that as y is factorial here but numeric for the model predictions
    scale_y_continuous(breaks = c(0, 1)) +
    scale_colour_viridis_c(option = "inferno", end=0.85) +
    labs(x="Heat duration (hrs)",
         y="Growth in well on last day?", colour="Community\nExpected\nGrowth Rate"))

# plot the effect sizes of the preferred model
extinct_forplot <- data.frame(confint(ext_mod.exptmu_prot))
colnames(extinct_forplot)[1:2] <- c("loCI", "hiCI")
extinct_forplot$predictor <- as.factor(rownames(extinct_forplot))
# protegens effect size is not significant and has giant CI that obscure other estimates
ggplot(extinct_forplot,
       aes(x = Estimate, y = predictor)) +
  geom_vline(xintercept = 0, colour="grey") +
  geom_point() +
  geom_errorbarh(aes(xmin = loCI, xmax = hiCI), height=0)

# plot the effect sizes again without protegens
ggplot(extinct_forplot %>% filter(predictor != "protegens1"),
       aes(x = Estimate, y = predictor)) +
  geom_vline(xintercept = 0, colour="grey") +
  geom_point() +
  geom_errorbarh(aes(xmin = loCI, xmax = hiCI), height=0)

# clean up
rm(ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_resist, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.expect_mu, ext_mod.averaged_mu, ext_mod.exptmu_prot, ext_mod.avemu_prot, extinct_forplot, ext_mod.exptmu_prot_resist)
```

# Shannon diversity

How is community diversity impacted during and after heat? Here we will have to be mindful to control for inoculated community richness as a nuisance variable (i.e., because we will always expect to see (less) more diversity in communities that were inoculated with more (less) species. But this is just part of our experimental design; we're not interested in this effect per se).

Let's first plot the Shannon diversity directly to get an idea of what we're dealing with:

```{r, shannonD_prelim_plot}
ggplot(absDen_forFit %>% filter(CommRich > 1), # monocultures are meaningless for diversity
       aes(y=Diversity, x=Day, fill=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_quasirandom(alpha=0.7, pch=21) +
  scale_fill_viridis_d(option = "viridis", begin=0.85, end=0) +
  labs(y = "Shannon Diversity",
       fill = "Inoculated\nRichness")

ggplot(absDen_forFit %>% filter(CommRich > 1), # monocultures are meaningless for diversity
       aes(y=Diversity, x=Day, fill=community_expected_mu)) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_quasirandom(alpha=0.7, pch=21) +
  scale_fill_viridis_c(option = "inferno", end=0.85) +
  labs(y = "Shannon Diversity",
       fill = "Expected\nCommunity mu")
```

## Choose GLM Family

Maddy and Gerard suggest that I use the full model to estimate effect size then emmeans to
estimate the effect size post-hoc. I'm tailouring this analysis on the example script that
Nico sent me (`Script_simplified for Hermina.R`).

One unique attribute of my experimental design is that the Day used as control differs
with heat duration (e.g., last day of recovery for 6h heat duration is Day 3 but last day
of recovery for 48h is Day 5). Our solution for this is to run separate models
for each heat treatment with its respective controls (i.e., 4 models in total). To make
sure that the effect sizes will be directly comparable across the models (especially with
respect to the standard deviation), Gerard suggested that I scale the whole data prior to
splitting it up into 4 (but not centering it as that will give me negative values that I
can't really use a ). Finally, if/when testing for significance it will then be necessary
to control for multiple testing (e.g., using a Bonferroni correction).

Note that for diversity I am considering CommRich as a numeric (which assumes a linear effect of community richness, e.g., where 4 species is 2 * the effect of 2 species).
Initially I tried playing around with CommRich as an ordered (& unordered) factor. But I
found that `glmmTMB` was choosing to drop different predictors because it was upset that
my model was overparameterized. This was particularly annoying as it was dropping the
estimates for the control treatments...

The first thing we need to do is to pick which GLM family of distributions looks best for our data:

```{r, diversity_glmFamily}
# remove the monocultures from the data
diversity_forFit <- absDen_forFit %>% filter(CommRich > 1) %>% # diversity is nonsense for monocultures
                        select(-Total_density, -Conc_putida, -Conc_protegens, -Conc_grimontii, -Conc_veronii)

# scale the data by its standard deviation
diversity_forFit$Diversity_scale <- scale(diversity_forFit$Diversity,
                                          scale = sd(diversity_forFit$Diversity, na.rm =  TRUE),
                                          center = FALSE)

# the max re-scaled value is 5.38 and 38% of the data is 0's
# so try gamma and lognormal distributions (maybe also Gaussian just to check that it's a bad fit?)
summary(diversity_forFit$Diversity_scale)
sum(diversity_forFit$Diversity_scale == 0, na.rm = TRUE) / length(diversity_forFit$Diversity_scale)


# let's compare different GLM families
try_gaussian <- glmmTMB(Diversity_scale ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                        data = diversity_forFit,
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_gaussian, plot = TRUE)

print("gamma family with zero-inflated model:")
try_gamma0 <- glmmTMB(Diversity_scale ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                     data = diversity_forFit,
                     family = ziGamma,
                     ziformula = ~1, # this needs to be added because there are 0 values in the data
                     control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_gamma0, plot = TRUE)

print("lognormal family with zero-inflated model:")
try_lognorm0 <- glmmTMB(Diversity_scale ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                       data = diversity_forFit,
                       family = lognormal,
                       ziformula = ~1, # this needs to be added because there are 0 values in the data
                       control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_lognorm0, plot = TRUE)

print("log(x+1) transformed data, lognormal family with zero-inflated model:")
try_LOGlognorm0 <- glmmTMB(log(Diversity_scale+1) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                          data = diversity_forFit,
                          family = lognormal,
                          ziformula = ~1, # I'm keeping this as 0-inflated lognormal alone was already over-dispersed. So I want to see if the log(x+1) transformation sufficiently brings in the long tail.
                          control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_LOGlognorm0, plot = TRUE)

try_negbinom <- glmmTMB(as.integer(Diversity_scale*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                       data = diversity_forFit,
                       family = nbinom2,
                       control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom, plot = TRUE)

try_negbinom0 <- glmmTMB(as.integer(Diversity_scale*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                         data = diversity_forFit,
                         family = nbinom2,
                         ziformula = ~1, # try zero inflated distribution
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom0, plot = TRUE)

try_poisson <- glmmTMB(as.integer(Diversity_scale*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                         data = diversity_forFit,
                         family = genpois,
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson, plot = TRUE)

try_poisson0 <- glmmTMB(as.integer(Diversity_scale*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                         data = diversity_forFit,
                         family = genpois,
                         ziformula = ~1, # try zero inflated distribution
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson0, plot = TRUE)

################################################

# I would generally prefer to use a zero inflated distribution but those are annoying to calculate effect sizes for (by posthoc emmeans). So let's do the classic hack and 
  # find the smallest non-zero value in the rescaled diversity estimate
smallest_diversity <- min(diversity_forFit$Diversity_scale[diversity_forFit$Diversity_scale != 0], na.rm=TRUE)
# now add 1/100th of that value to all the diversity estimates and re-do the fit for the family that looked best above
diversity_forFit$Diversity_scalePLUSepsilon <- diversity_forFit$Diversity_scale + smallest_diversity/100

print("data transformed (D + epsilon), lognormal family:")
# this looks just awful:
try_lognorm_plusE <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                             data = diversity_forFit,
                             family = lognormal,
                             control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_lognorm_plusE, plot = TRUE)

#Let's see if we can make things better in any way by using a log transformation? (this model looked the best with zero-inflation above)
try_lognorm_logPLUSe <- glmmTMB(log(Diversity_scale+1+smallest_diversity) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                          data = diversity_forFit,
                          family = lognormal,
                          control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_lognorm_logPLUSe, plot = TRUE)

# let's just try other families...
try_gaussian_plusE <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                        data = diversity_forFit,
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_gaussian_plusE, plot = TRUE)

print("data transformed (D + epsilon), gamma family:")
try_gamma_plusE <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                     data = diversity_forFit,
                     family = Gamma,
                     control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_gamma_plusE, plot = TRUE)

try_negbinom_plusE <- glmmTMB(as.integer(Diversity_scalePLUSepsilon*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                       data = diversity_forFit,
                       family = nbinom2,
                       control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom_plusE, plot = TRUE)

try_poisson_plusE <- glmmTMB(as.integer(Diversity_scalePLUSepsilon*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                         data = diversity_forFit,
                         family = genpois,
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson_plusE, plot = TRUE)


# let's check this with AIC and BIC
AIC(try_gaussian, try_gamma0, try_lognorm0, try_LOGlognorm0,
    try_negbinom, try_negbinom0, try_poisson, try_poisson0, try_lognorm_plusE, try_lognorm_logPLUSe, try_gaussian_plusE, try_gamma_plusE, try_negbinom_plusE, try_poisson_plusE) %>% arrange(AIC)
BIC(try_gaussian, try_gamma0, try_lognorm0, try_LOGlognorm0,
    try_negbinom, try_negbinom0, try_poisson, try_poisson0, try_lognorm_plusE, try_lognorm_logPLUSe, try_gaussian_plusE, try_gamma_plusE, try_negbinom_plusE, try_poisson_plusE) %>% arrange(BIC)

# clean up
rm(try_gaussian, try_gamma0, try_lognorm0, try_LOGlognorm0, try_negbinom, try_negbinom0, try_poisson, try_poisson0, try_lognorm_plusE, try_lognorm_logPLUSe, try_gaussian_plusE, try_gamma_plusE, try_negbinom_plusE, try_poisson_plusE, smallest_diversity)
```

According to the residuals, the zero-inflated negative binomial and the zero-inflated
lognormal are about equally okay-ish. (We could also take the AIC & BIC values in
consideration for our decision but that is far less important.) My preference would be to go with the zero-inflated lognormal. The reason for this is because my
understanding is that the most important thing to consider when selecting a GLM family is
which family would *a priori* be the most natural choice. For diversity data, the Gamma or
lognormal distributions are the most natural choices *a priori* because, for 4 species,
the Shannon diversity data is a continuous variable between 0 and 1.386294. Therefore I
think it makes sense to choose the lognormal (even if its residuals are not perfect).

For this data I would generally prefer to use one of the zero inflated distributions but we are interested in the effect sizes (emmeans & posthoc analyses). The problem with the zero inflated distribution is that it leads to very confusing looking effect sizes when I do the downstream analyses.
This is because the effect sizes are split up over the conditional and the
zero-inflated parts of the model. The overall effect size is: (1 - zi)\*(cond mean). See
[this stackover flow
thread](https://stackoverflow.com/questions/62351158/contrasts-with-zero-inflated-glmmtmb). The practical problem is that it just looks really weird for this data.

To get around this issue, I have transformed the re-scaled data to add a small value (`min(rescaled diversity)/100`) to all diversity estimates such
that we remove the zero's. This way we will no longer need to use a zero-inflated part in
the model and the effect sizes will be simpler to explain. (Especially because I think I
am losing a lot of power for the 48h treatment as a result of almost everything literally being 0.)

My final decision is to use the $D + \epsilon$ transformed data and a lognormal family of distributions. It doesn't have the best residuals but it strikes the right compromise of *a priori* justifiable as well as useful and interpretable for the analysis we're interested in.

Finally, note that in the model fitting above I consider Day as a numeric predictor. This is
because I want to decide on the GLM family by considering the complete data. (& I was having
problems with Day as an un/ordered factor...)

## Compare fit of different models to data subsets

For the rest of the analysis below, I consider the effect of day
(which is called `Trtmt_Day`) as a factor representing either resistance (i.e., on the
last day of heat) or recovery. To do this I need to subset the data to include only resistance, 1st day post-heat, and last day post-heat for the heat treatment. And I need to keep exactly the same days of the control treatment. I create this data subset for each heat treatment duration (so 4 data subsets in total).

We want to find which model is the best fit for all the data subsets.

Note that it's not possible to fit any models with protegens present **and** heat resistant both as predictors!!

```{r, diversityPlus_subset&modelfit}
# a hacky function to get the number of parameters in the model
npar_of_glmmTMB_fit <- function(modfit)
  length(modfit$fit$parfull)
  
# a function to fit the different models to the subsetted data:
fit_diversity_models <- function(data_subset) {
  # create list for output
  output.ls <- list()
  
  # this is the simplest model. I'm fitting it to check for colinearity
  output.ls[["simple"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat + Trtmt_Day + protegens + community_expected_mu,
                               data = data_subset,
                               family = lognormal,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  # this is another simple model to check for colinearity
  output.ls[["simple resist"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat + Trtmt_Day + community_expected_mu + resistant,
                               data = data_subset,
                               family = lognormal,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  # this is our null model:
  output.ls[["H0"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day,
                               data = data_subset,
                               family = lognormal,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  # this is another null model just to confirm that CommRich has NO interactions with heat
  output.ls[["H0: *CommRich"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day,
                                          data = data_subset,
                                          family = lognormal,
                                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["+resist"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day + resistant,
                                   data = data_subset,
                                   family = lognormal,
                                   control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

  output.ls[["*resist"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*resistant,
                                   data = data_subset,
                                   family = lognormal,
                                   control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["+prot"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day + protegens,
                                  data = data_subset,
                                  family = lognormal,
                                  control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*prot"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*protegens,
                                  data = data_subset,
                                  family = lognormal,
                                  control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["+mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day + community_expected_mu,
                                data = data_subset,
                                family = lognormal,
                                control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*community_expected_mu,
                                data = data_subset,
                                family = lognormal,
                                control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["+prot +mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day + protegens + community_expected_mu,
                                      data = data_subset,
                                      family = lognormal,
                                      control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["+resist +mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day + resistant + community_expected_mu,
                                        data = data_subset,
                                        family = lognormal,
                                        control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*prot +mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*protegens + community_expected_mu,
                                      data = data_subset,
                                      family = lognormal,
                                      control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*prot + prot*mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*protegens + protegens*community_expected_mu,
                                            data = data_subset,
                                            family = lognormal,
                                            control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*mu +prot"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*community_expected_mu + protegens,
                                      data = data_subset,
                                      family = lognormal,
                                      control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*mu + mu*prot"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*community_expected_mu + community_expected_mu*protegens,
                                          data = data_subset,
                                          family = lognormal,
                                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*mu +resist"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*community_expected_mu + resistant,
                                        data = data_subset,
                                        family = lognormal,
                                        control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*mu + mu*resist"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*community_expected_mu + community_expected_mu*resistant,
                                            data = data_subset,
                                            family = lognormal,
                                            control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*resist +mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*resistant + community_expected_mu,
                                        data = data_subset,
                                        family = lognormal,
                                        control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

  output.ls[["*resist + resist*mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*resistant + resistant*community_expected_mu,
                                                data = data_subset,
                                                family = lognormal,
                                                control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  output.ls[["*prot*mu"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*protegens*community_expected_mu,
                                     data = data_subset,
                                     family = lognormal,
                                     control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  output.ls[["*mu*resist"]] <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich + Heat*Trtmt_Day*community_expected_mu*resistant,
                                       data = data_subset,
                                       family = lognormal,
                                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  return(output.ls)
}

# a function to plot model predictions against the data
  # INOCULATED COMMUNITY RICHNESS is plotted as different colours
plot_model_pred.CommRich <- function(mod_list, mod_name){
  # create data.frame for plotting
  divers_predict <- cbind(mod_list[[mod_name]]$frame,
                          predicted=predict(mod_list[[mod_name]], type="response"))
  # change the first column name for easier plotting
  colnames(divers_predict)[1] <- "observed"
  # create the plot
  out <- ggplot(divers_predict, 
                aes(x=Trtmt_Day, y=observed, colour=as.factor(CommRich))) +
         facet_grid(protegens~Heat) +
         geom_jitter(alpha=0.4) +
         geom_line(aes(y=predicted, group=as.factor(CommRich))) +
         scale_y_log10() +
         scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
         labs(y="Shannon diversity (rescaled)",
              colour="CommRich",
              title=paste(mod_name, "model predictions"))
  return(out)
  rm(divers_predict)
}

# a function to plot model predictions against the data
  # EXPECTED COMMUNITY MU is plotted as different colours
plot_model_pred.MU <- function(mod_list, mod_name){
  # create data.frame for plotting
  divers_predict <- cbind(mod_list[[mod_name]]$frame,
                          predicted=predict(mod_list[[mod_name]], type="response"))
  # change the first column name for easier plotting
  colnames(divers_predict)[1] <- "observed"
  # create the plot
  out <- ggplot(divers_predict, 
                aes(x=Trtmt_Day, y=observed,
                    colour=community_expected_mu, group=as.factor(community_expected_mu))) +
         facet_grid(protegens~Heat) +
         geom_jitter(alpha=0.4) +
         geom_line(aes(y=predicted)) +
         scale_y_log10() +
         scale_colour_viridis_c(option = "inferno", end=0.85) +
         labs(y="Shannon diversity (rescaled)",
              colour="Expected\nCommunity mu",
              title=paste(mod_name, "model predictions"))
  return(out)
  rm(divers_predict)
}

####################
# 6h heat duration
####################
# grab just the treatment with its associated control data
diversity_6h <- rbind(diversity_forFit %>% filter(Heat == "6"),
                      diversity_forFit %>% filter(Heat == "control", Day < 4))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_6h$Trtmt_Day <- "resist"
diversity_6h$Trtmt_Day[diversity_6h$Day == 2] <- "recov_1"
diversity_6h$Trtmt_Day[diversity_6h$Day == 3] <- "recov_2"

# fit different models:
div_mods6h <- fit_diversity_models(diversity_6h)

# check the simplest possible models for multicolinearity
check_collinearity(div_mods6h[["simple"]])
check_collinearity(div_mods6h[["simple resist"]])

# check the fit of the overall preferred model
simulateResiduals(fittedModel = div_mods6h[["*prot + prot*mu"]], plot = TRUE)
summary(div_mods6h[["*prot + prot*mu"]])

# print a summary table of the model fits
data.frame(pars = sapply(div_mods6h, npar_of_glmmTMB_fit),
                           AIC = sapply(div_mods6h, AIC),
                           AICc = sapply(div_mods6h, AICc),
                           BIC = sapply(div_mods6h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# print some of the top model predictions
print(plot_model_pred.CommRich(mod_list=div_mods6h, mod_name="+prot"))
print(plot_model_pred.MU(mod_list=div_mods6h, mod_name="*prot + prot*mu"))
print(plot_model_pred.MU(mod_list=div_mods6h, mod_name="*prot*mu"))

####################
# 12h heat duration
####################
# grab just the treatment with its associated control data
diversity_12h <- rbind(diversity_forFit %>% filter(Heat == "12", Day > 1),
                       diversity_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_12h$Trtmt_Day <- "resist"
diversity_12h$Trtmt_Day[diversity_12h$Day == 3] <- "recov_1"
diversity_12h$Trtmt_Day[diversity_12h$Day == 4] <- "recov_2"

# fit different models:
div_mods12h <- fit_diversity_models(diversity_12h)

# check the simplest possible models for multicolinearity
check_collinearity(div_mods12h[["simple"]])
check_collinearity(div_mods12h[["simple resist"]])

# check the fit of the overall preferred model
simulateResiduals(fittedModel = div_mods12h[["*prot + prot*mu"]], plot = TRUE)
summary(div_mods12h[["*prot + prot*mu"]])

# print a summary table of the model fits
data.frame(pars = sapply(div_mods12h, npar_of_glmmTMB_fit),
                           AIC = sapply(div_mods12h, AIC),
                           AICc = sapply(div_mods12h, AICc),
                           BIC = sapply(div_mods12h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# print some of the top model predictions
print(plot_model_pred.CommRich(mod_list=div_mods12h, mod_name="*prot"))
print(plot_model_pred.MU(mod_list=div_mods12h, mod_name="*prot + prot*mu"))
print(plot_model_pred.MU(mod_list=div_mods12h, mod_name="*prot*mu"))

####################
# 24h heat duration
####################
# grab just the treatment with its associated control data
diversity_24h <- rbind(diversity_forFit %>% filter(Heat == "24", Day > 1),
                       diversity_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_24h$Trtmt_Day <- "resist"
diversity_24h$Trtmt_Day[diversity_24h$Day == 3] <- "recov_1"
diversity_24h$Trtmt_Day[diversity_24h$Day == 4] <- "recov_2"

# fit different models:
div_mods24h <- fit_diversity_models(diversity_24h)

# check the simplest possible models for multicolinearity
check_collinearity(div_mods24h[["simple"]])
check_collinearity(div_mods24h[["simple resist"]])

# check the fit of the overall preferred model
simulateResiduals(fittedModel = div_mods24h[["*prot + prot*mu"]], plot = TRUE)
summary(div_mods24h[["*prot + prot*mu"]])

# print a summary table of the model fits
data.frame(pars = sapply(div_mods24h, npar_of_glmmTMB_fit),
                           AIC = sapply(div_mods24h, AIC),
                           AICc = sapply(div_mods24h, AICc),
                           BIC = sapply(div_mods24h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# print some of the top model predictions
print(plot_model_pred.CommRich(mod_list=div_mods24h, mod_name="*prot"))
print(plot_model_pred.MU(mod_list=div_mods24h, mod_name="*prot + prot*mu"))
print(plot_model_pred.MU(mod_list=div_mods24h, mod_name="*prot*mu"))


####################
# 48h heat duration
####################
# grab just the treatment with its associated control data
diversity_48h <- rbind(diversity_forFit %>% filter(Heat == "48", Day > 2),
                       diversity_forFit %>% filter(Heat == "control", Day > 2))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_48h$Trtmt_Day <- "resist"
diversity_48h$Trtmt_Day[diversity_48h$Day == 4] <- "recov_1"
diversity_48h$Trtmt_Day[diversity_48h$Day == 5] <- "recov_2"

# fit different models:
div_mods48h <- fit_diversity_models(diversity_48h)

# check the simplest possible models for multicolinearity
check_collinearity(div_mods48h[["simple"]])
check_collinearity(div_mods48h[["simple resist"]])

# check the fit of the overall preferred model
simulateResiduals(fittedModel = div_mods48h[["*prot + prot*mu"]], plot = TRUE)
summary(div_mods48h[["*prot + prot*mu"]])

# print a summary table of the model fits
data.frame(pars = sapply(div_mods48h, npar_of_glmmTMB_fit),
           AIC = sapply(div_mods48h, AIC),
           AICc = sapply(div_mods48h, AICc),
           BIC = sapply(div_mods48h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# print some of the top model predictions
print(plot_model_pred.CommRich(mod_list=div_mods48h, mod_name="*prot"))
print(plot_model_pred.MU(mod_list=div_mods48h, mod_name="*prot + prot*mu"))
print(plot_model_pred.MU(mod_list=div_mods48h, mod_name="*prot +mu"))


####################
# Select the best model across all data subsets
####################
# for each information criterion, get the average across all data subsets
meanIC <- data.frame(pars = sapply(div_mods48h, npar_of_glmmTMB_fit),
                     AIC = sapply(div_mods6h, AIC) + sapply(div_mods12h, AIC) + sapply(div_mods24h, AIC) + sapply(div_mods48h, AIC),
                     AICc = sapply(div_mods6h, AICc) + sapply(div_mods12h, AICc) + sapply(div_mods24h, AICc) + sapply(div_mods48h, AICc),
                     BIC = sapply(div_mods6h, BIC) + sapply(div_mods12h, BIC) + sapply(div_mods24h, BIC) + sapply(div_mods48h, BIC)) %>%
            mutate(AIC = AIC/4,
                   AICc = AICc/4,
                   BIC = BIC/4) %>%
              mutate(dAIC = min(AIC)-AIC,
                     dAICc = min(AICc)-AICc,
                     dBIC = min(BIC)-BIC)
meanIC %>% arrange(BIC)

# clean up
rm(meanIC)
```

As expected, inoculated community richness has a positive effect on diversity, protegens has a negative effect, and community growth rate has a negative effect. There are interactions with presence/absence of heat but the trouble is that we don't want the model to be overly complex (see model predicts for the most complex model above; the predictions are just rubbish).

The short heat duration data tends to prefer the community growth rate as an interaction effect with heat, especially for 12h where slow communities had much more diversity in the presence of heat for some reason. As the heat duration gets longer, the data tends to prefer protegens as an interaction effect with heat, especially for 48h where protegens *presence* now shows more diversity than absence (i.e., because there are extinction events in the absence of protegens).

From the model predictions, we can see why it really doesn't make sense to use the very complex model (`*prot*mu`). I'm going to use the `*prot + prot*mu` model for the analysis below because this model includes all the predictors of interest, is not unnecessarily complex, it's the 2nd best fit for each data subset, and it's the 2nd best fit across the entire data (after the too complex `*prot*mu`).

## Calculate effect sizes

Even if I wanted to calculate effect sizes for the most complex model, I wouldn't be able to do it because there's too many NA values during resistance at 48h heat. This leads to nonest values:

```{r, diversityPlus_try48hcomplexmodel}
emmeans(div_mods48h[["*prot*mu"]],
                   ~ Heat | CommRich + Trtmt_Day*protegens*community_expected_mu,
                   data = diversity_48h, type = "response")
```


```{r, diversityPlus_effectSize}
# use emmeans to get the effect size during heat as compared to control for each of the treatment days AND conditional on protegens
emm_6h <- emmeans(div_mods6h[["*prot + prot*mu"]],
                  ~ Heat | CommRich + Trtmt_Day*protegens + community_expected_mu*protegens,
                  data = diversity_6h, type = "response")
effect_6h <- eff_size(emm_6h, sigma(div_mods6h[["*prot + prot*mu"]]),
                      edf = df.residual(div_mods6h[["*prot + prot*mu"]]))

emm_12h <- emmeans(div_mods12h[["*prot + prot*mu"]],
                   ~ Heat | CommRich + Trtmt_Day*protegens + community_expected_mu*protegens,
                   data = diversity_12h, type = "response")
effect_12h <- eff_size(emm_12h, sigma(div_mods12h[["*prot + prot*mu"]]),
                       edf = df.residual(div_mods12h[["*prot + prot*mu"]]))

emm_24h <- emmeans(div_mods24h[["*prot + prot*mu"]],
                   ~ Heat | CommRich + Trtmt_Day*protegens + community_expected_mu*protegens,
                   data = diversity_24h, type = "response")
effect_24h <- eff_size(emm_24h, sigma(div_mods24h[["*prot + prot*mu"]]),
                       edf = df.residual(div_mods24h[["*prot + prot*mu"]]))

emm_48h <- emmeans(div_mods48h[["*prot + prot*mu"]],
                   ~ Heat | CommRich + Trtmt_Day*protegens + community_expected_mu*protegens,
                   data = diversity_48h, type = "response")
effect_48h <- eff_size(emm_48h, sigma(div_mods48h[["*prot + prot*mu"]]),
                       edf = df.residual(div_mods48h[["*prot + prot*mu"]]))

# a function that extracts the confidence intervals from eff_size *** contingent on protegens ***
get_effsize_CIs <- function(eff_size_object, heat_trtmt) {
  data.frame(Heat = heat_trtmt,
             CommRich = confint(eff_size_object)[[2]],
             Trtmt_Day = confint(eff_size_object)[[3]],
             protegens = confint(eff_size_object)[[4]],
             expected_mu = confint(eff_size_object)[[5]], #
             est = confint(eff_size_object)[[6]], #[[5]],
             loCI = confint(eff_size_object)[[9]], #[[8]],
             hiCI = confint(eff_size_object)[[10]]) #[[9]])
}

# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
div_effects_protegens <- data.frame()
div_effects_protegens <- rbind(div_effects_protegens,
                              get_effsize_CIs(effect_6h, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects_protegens$Trtmt_Day <- factor(div_effects_protegens$Trtmt_Day,
                                          levels = c("resist", "recov_1", "recov_2"))
levels(div_effects_protegens$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot conditional part of the model
ggplot(div_effects_protegens,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day, shape = as.logical(protegens))) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(title = "CommRich + Heat*Trtmt_Day*prot + com_expect_mu*prot",
       x = "Effect Size on Shannon Diversity",
       shape = "protegens\npresent?",
       y="Heat duration")

# we can do a posthoc on this to illustrate statistically significant effects
posthocPROT_6h <- emmeans(effect_6h, pairwise ~ Trtmt_Day*protegens, data = diversity_6h)
posthocPROT_12h <- emmeans(effect_12h, pairwise ~ Trtmt_Day*protegens, data = diversity_12h)
posthocPROT_24h <- emmeans(effect_24h, pairwise ~ Trtmt_Day*protegens, data = diversity_24h)
posthocPROT_48h <- emmeans(effect_48h, pairwise ~ Trtmt_Day*protegens, data = diversity_48h)

# a function that extracts the confidence intervals from a post-hoc object
get_posthoc <- function(posthoc_object, heat_trtmt) {
  output <- multcomp::cld(posthoc_object, alpha=0.05/4, Letters = letters) %>%
              data.frame() %>%
                select(-df)
  colnames(output)[3:7] <- c("est", "SE", "loCI", "hiCI", "groups")
  output$Heat <- heat_trtmt
  return(output)
}

# create a data.frame for plotting
div_effects_protegens <- data.frame()
div_effects_protegens <- rbind(div_effects_protegens,
                               get_posthoc(posthocPROT_6h, heat_trtmt = 6),
                               get_posthoc(posthocPROT_12h, heat_trtmt = 12),
                               get_posthoc(posthocPROT_24h, heat_trtmt = 24),
                               get_posthoc(posthocPROT_48h, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects_protegens$Trtmt_Day <- factor(div_effects_protegens$Trtmt_Day,
                                          levels = c("resist", "recov_1", "recov_2"))
levels(div_effects_protegens$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot with group labels
ggplot(div_effects_protegens,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day, shape=as.logical(protegens))) +
  facet_grid(~protegens) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-2.5, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Shannon Diversity",
       y="Heat duration",
       shape = "protegens\npresent?",
       title = "CommRich + Heat*Trtmt_Day*prot + com_expect_mu*prot")

# Note that there is a significant interaction between Treatment Day & protegens!
# I think that it may still be okay to average over the effects of protegens because they are not crossed

# anyway we can still average over the effect of protegens
# we can do a posthoc on this to illustrate statistically significant effects
posthoc_6h <- emmeans(effect_6h, pairwise ~ Trtmt_Day, data = diversity_6h)
posthoc_12h <- emmeans(effect_12h, pairwise ~ Trtmt_Day, data = diversity_12h)
posthoc_24h <- emmeans(effect_24h, pairwise ~ Trtmt_Day, data = diversity_24h)
posthoc_48h <- emmeans(effect_48h, pairwise ~ Trtmt_Day, data = diversity_48h)

# a function that extracts the confidence intervals from a post-hoc object
  ## we need to redefine the function because the colnames have changed now
get_posthoc <- function(posthoc_object, heat_trtmt) {
  output <- multcomp::cld(posthoc_object, alpha=0.05/4, Letters = letters) %>%
              data.frame() %>%
                select(-df)
  colnames(output)[2:6] <- c("est", "SE", "loCI", "hiCI", "groups")
  output$Heat <- heat_trtmt
  return(output)
}
# create a data.frame for plotting
div_effects <- data.frame()
div_effects <- rbind(div_effects,
                     get_posthoc(posthoc_6h, heat_trtmt = 6),
                     get_posthoc(posthoc_12h, heat_trtmt = 12),
                     get_posthoc(posthoc_24h, heat_trtmt = 24),
                     get_posthoc(posthoc_48h, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects$Trtmt_Day <- factor(div_effects$Trtmt_Day,
                                         levels = c("resist", "recov_1", "recov_2"))
levels(div_effects$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot with group labels
ggplot(div_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-2.5, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Shannon Diversity",
       y="Heat duration",
       title = "protegens as non-focal predictor (i.e., marginalized)")

#######
# finally, we will do a series of pairwise two-tailed t-tests to compare between heat durations
#######
  # reminder to myself: I tried this as a series of z-tests and that made things more optimistic (aka LESS conservative). The t-test is indeed the more conservative option among the parametric tests.
  # I also looked into whether it's possible to do a Mann-Whitney test (aka Wilcoxon signed-rank test). But, since that is a *non-parametric test*, by definition you would need raw data to run it (i.e., *not* summary statistics). So I'm a bit confused about whether & how to run a non-parametric test...

# a function that approximates the sample size from each data subset
estimate_n <- function(data_subset, CommRich = FALSE) {
  if(CommRich == 0) {
    # get the number of unique ID's present on recovery day 2 for the heat treatment
    # then divide this by 4 as we want to know the average sample size across CommRich
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2",]$uniqID))/4
  }
  if(CommRich == 1){ # do the same thing for specified values of CommRich
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2" & data_subset$CommRich == 1,]$uniqID))/4
  }
  if(CommRich == 2){
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2" & data_subset$CommRich == 2,]$uniqID))/4
  }
  if(CommRich == 3){
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2" & data_subset$CommRich == 3,]$uniqID))/4
  }
  if(CommRich == 4){
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2" & data_subset$CommRich == 4,]$uniqID))/4
  }
  return(output)
}
# a function that runs two-tailed t-test between row numbers of diversity_effects df
run_ttest <- function(row_x, row_y,
                      summary_stats_df){
  ttest_object <- tsum.test(mean.x = summary_stats_df$est[row_x],
                            s.x = summary_stats_df$SE[row_x],
                            n.x = summary_stats_df$n[row_x],
                            mean.y = summary_stats_df$est[row_y],
                            s.y = summary_stats_df$SE[row_y],
                            n.y = summary_stats_df$n[row_y],
                            alternative="two.sided")
  return(data.frame(t_statistic = ttest_object$statistic,
                    df = ttest_object$parameters,
                    pvalue = ttest_object$p.value))
}

# estimate the sample sizes
temp <- div_effects # copy the effects to temp
div_effects <- rbind(temp %>% filter(Heat == 6) %>% mutate(n = estimate_n(diversity_6h)),
                     temp %>% filter(Heat == 12) %>% mutate(n = estimate_n(diversity_12h)),
                     temp %>% filter(Heat == 24) %>% mutate(n = estimate_n(diversity_24h)),
                     temp %>% filter(Heat == 48) %>% mutate(n = estimate_n(diversity_48h)))
rm(temp)
# estimate the SD from the SE
div_effects <- div_effects %>% mutate(SD = SE * sqrt(n)) %>%
    # re-order by Heat and Trtmt_Day
                          arrange(Heat, Trtmt_Day)

# all pairwise combinations of comparisons between the same treatment day for different durations
temp <- t(combn(c(1,4,7,10), 2))
combos <- rbind(temp, temp+1, temp+2)
rm(temp)
# loop through all the combinations and do the t-tests
divEffects_ttests <- data.frame()
for(i in 1:nrow(combos)){
  divEffects_ttests <- rbind(divEffects_ttests,
                             run_ttest(row_x = combos[i,1],
                                       row_y = combos[i,2],
                                       summary_stats_df = div_effects))
}
divEffects_ttests$adjusted_p <- p.adjust(divEffects_ttests$pvalue, method = "bonferroni")
divEffects_ttests$Trtmt_Day <- div_effects$Trtmt_Day[combos[,1]]
divEffects_ttests$Heat_1 <- div_effects$Heat[combos[,1]]
divEffects_ttests$Heat_2 <- div_effects$Heat[combos[,2]]

print(divEffects_ttests)
# these p-values seem overly optimistic. Use alpha = 1*10^-3

# cleanup
rm(div_mods6h, div_mods12h, div_mods24h, div_mods48h,
   emm_6h, emm_12h, emm_24h, emm_48h, effect_6h, effect_12h, effect_24h, effect_48h,
   posthocPROT_6h, posthocPROT_12h, posthocPROT_24h, posthocPROT_48h, posthoc_6h, posthoc_12h, posthoc_24h, posthoc_48h,
   temp, combos, divEffects_ttests,
   div_effects_protegens, div_effects)
```

We don't see any significant decoupling here between the effect size during resistance as compared to during recovery.

The main effect that we see is that diversity really drops down a lot for the 48h heat pulse. This is mostly due to loss of the most sensitive species but it can also be driven by extinction of entire replicates. This data includes all replicates, even ones that went extinct altogether. Both monocultures and extinct replicates will have a final Shannon diversity of 0. So let's repeat the analysis to show that it's **not** the extinct wells that are driving the low effect size at 48h duration.

## Repeat diversity analysis removing extinct replicates

Let's show that the strong effect at 48h is not due entirely to the presence of the extinct reps. I will re-do the entire analysis above but this time using only the data **without** the extinct replicates.

```{r, diversityPlus_removeExtinct}
# add a column indicating whether the replicate survived
  # but first we need to remove $Heat because it's a factor for diversity but numeric for extinctions and cannot be *_joined
tmp_div <- diversity_forFit %>% select(-Heat)
tmp_div <- inner_join(tmp_div,
                      extinct.df %>% select(uniqID, survived),
                      by = c("uniqID"))
diversity_forFit$survived <- tmp_div$survived
rm(tmp_div)

# keep just the diversity values that did not go extinct
diversity_forFit <- diversity_forFit %>% filter(survived == 1)

####################
# 6h heat duration
####################
# grab just the treatment with its associated control data
diversity_6h <- rbind(diversity_forFit %>% filter(Heat == "6"),
                      diversity_forFit %>% filter(Heat == "control", Day < 4))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_6h$Trtmt_Day <- "resist"
diversity_6h$Trtmt_Day[diversity_6h$Day == 2] <- "recov_1"
diversity_6h$Trtmt_Day[diversity_6h$Day == 3] <- "recov_2"

# fit different models:
div_mods6h <- fit_diversity_models(diversity_6h)

# check the simplest possible models for multicolinearity
check_collinearity(div_mods6h[["simple"]])
check_collinearity(div_mods6h[["simple resist"]])

# check the fit of the overall preferred model
simulateResiduals(fittedModel = div_mods6h[["*prot + prot*mu"]], plot = TRUE)
summary(div_mods6h[["*prot + prot*mu"]])

# print a summary table of the model fits
data.frame(pars = sapply(div_mods6h, npar_of_glmmTMB_fit),
                           AIC = sapply(div_mods6h, AIC),
                           AICc = sapply(div_mods6h, AICc),
                           BIC = sapply(div_mods6h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# print some of the top model predictions
print(plot_model_pred.CommRich(mod_list=div_mods6h, mod_name="+prot"))
print(plot_model_pred.MU(mod_list=div_mods6h, mod_name="*prot + prot*mu"))
print(plot_model_pred.MU(mod_list=div_mods6h, mod_name="*prot*mu"))

####################
# 12h heat duration
####################
# grab just the treatment with its associated control data
diversity_12h <- rbind(diversity_forFit %>% filter(Heat == "12", Day > 1),
                       diversity_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_12h$Trtmt_Day <- "resist"
diversity_12h$Trtmt_Day[diversity_12h$Day == 3] <- "recov_1"
diversity_12h$Trtmt_Day[diversity_12h$Day == 4] <- "recov_2"

# fit different models:
div_mods12h <- fit_diversity_models(diversity_12h)

# check the simplest possible models for multicolinearity
check_collinearity(div_mods12h[["simple"]])
check_collinearity(div_mods12h[["simple resist"]])

# check the fit of the overall preferred model
simulateResiduals(fittedModel = div_mods12h[["*prot + prot*mu"]], plot = TRUE)
summary(div_mods12h[["*prot + prot*mu"]])

# print a summary table of the model fits
data.frame(pars = sapply(div_mods12h, npar_of_glmmTMB_fit),
                           AIC = sapply(div_mods12h, AIC),
                           AICc = sapply(div_mods12h, AICc),
                           BIC = sapply(div_mods12h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# print some of the top model predictions
print(plot_model_pred.CommRich(mod_list=div_mods12h, mod_name="*prot"))
print(plot_model_pred.MU(mod_list=div_mods12h, mod_name="*prot + prot*mu"))
print(plot_model_pred.MU(mod_list=div_mods12h, mod_name="*prot*mu"))

####################
# 24h heat duration
####################
# grab just the treatment with its associated control data
diversity_24h <- rbind(diversity_forFit %>% filter(Heat == "24", Day > 1),
                       diversity_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_24h$Trtmt_Day <- "resist"
diversity_24h$Trtmt_Day[diversity_24h$Day == 3] <- "recov_1"
diversity_24h$Trtmt_Day[diversity_24h$Day == 4] <- "recov_2"

# fit different models:
div_mods24h <- fit_diversity_models(diversity_24h)

# check the simplest possible models for multicolinearity
check_collinearity(div_mods24h[["simple"]])
check_collinearity(div_mods24h[["simple resist"]])

# check the fit of the overall preferred model
simulateResiduals(fittedModel = div_mods24h[["*prot + prot*mu"]], plot = TRUE)
summary(div_mods24h[["*prot + prot*mu"]])

# print a summary table of the model fits
data.frame(pars = sapply(div_mods24h, npar_of_glmmTMB_fit),
                           AIC = sapply(div_mods24h, AIC),
                           AICc = sapply(div_mods24h, AICc),
                           BIC = sapply(div_mods24h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# print some of the top model predictions
print(plot_model_pred.CommRich(mod_list=div_mods24h, mod_name="*prot"))
print(plot_model_pred.MU(mod_list=div_mods24h, mod_name="*prot + prot*mu"))
print(plot_model_pred.MU(mod_list=div_mods24h, mod_name="*prot*mu"))


####################
# 48h heat duration
####################
# grab just the treatment with its associated control data
diversity_48h <- rbind(diversity_forFit %>% filter(Heat == "48", Day > 2),
                       diversity_forFit %>% filter(Heat == "control", Day > 2))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_48h$Trtmt_Day <- "resist"
diversity_48h$Trtmt_Day[diversity_48h$Day == 4] <- "recov_1"
diversity_48h$Trtmt_Day[diversity_48h$Day == 5] <- "recov_2"

# fit different models:
div_mods48h <- fit_diversity_models(diversity_48h)

# check the simplest possible models for multicolinearity
check_collinearity(div_mods48h[["simple"]])
check_collinearity(div_mods48h[["simple resist"]])

# check the fit of the overall preferred model
simulateResiduals(fittedModel = div_mods48h[["*prot + prot*mu"]], plot = TRUE)
summary(div_mods48h[["*prot + prot*mu"]])

# print a summary table of the model fits
data.frame(pars = sapply(div_mods48h, npar_of_glmmTMB_fit),
           AIC = sapply(div_mods48h, AIC),
           AICc = sapply(div_mods48h, AICc),
           BIC = sapply(div_mods48h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# print some of the top model predictions
print(plot_model_pred.CommRich(mod_list=div_mods48h, mod_name="*prot"))
print(plot_model_pred.MU(mod_list=div_mods48h, mod_name="*prot + prot*mu"))
print(plot_model_pred.MU(mod_list=div_mods48h, mod_name="*prot +mu"))


####################
# Select the best model across all data subsets
####################
# for each information criterion, get the average across all data subsets
meanIC <- data.frame(pars = sapply(div_mods48h, npar_of_glmmTMB_fit),
                     AIC = sapply(div_mods6h, AIC) + sapply(div_mods12h, AIC) + sapply(div_mods24h, AIC) + sapply(div_mods48h, AIC),
                     AICc = sapply(div_mods6h, AICc) + sapply(div_mods12h, AICc) + sapply(div_mods24h, AICc) + sapply(div_mods48h, AICc),
                     BIC = sapply(div_mods6h, BIC) + sapply(div_mods12h, BIC) + sapply(div_mods24h, BIC) + sapply(div_mods48h, BIC)) %>%
            mutate(AIC = AIC/4,
                   AICc = AICc/4,
                   BIC = BIC/4) %>%
              mutate(dAIC = min(AIC)-AIC,
                     dAICc = min(AICc)-AICc,
                     dBIC = min(BIC)-BIC)
meanIC %>% arrange(BIC)

# clean up
rm(meanIC)

####################
# get the emmeans
####################
# use emmeans to get the effect size during heat as compared to control for each of the treatment days AND conditional on protegens
emm_6h <- emmeans(div_mods6h[["*prot + prot*mu"]],
                  ~ Heat | CommRich + Trtmt_Day*protegens + community_expected_mu*protegens,
                  data = diversity_6h, type = "response")
effect_6h <- eff_size(emm_6h, sigma(div_mods6h[["*prot + prot*mu"]]), edf = df.residual(div_mods6h[["*prot + prot*mu"]]))

emm_12h <- emmeans(div_mods12h[["*prot + prot*mu"]],
                   ~ Heat | CommRich + Trtmt_Day*protegens + community_expected_mu*protegens,
                   data = diversity_12h, type = "response")
effect_12h <- eff_size(emm_12h, sigma(div_mods12h[["*prot + prot*mu"]]), edf = df.residual(div_mods12h[["*prot + prot*mu"]]))

emm_24h <- emmeans(div_mods24h[["*prot + prot*mu"]],
                   ~ Heat | CommRich + Trtmt_Day*protegens + community_expected_mu*protegens,
                   data = diversity_24h, type = "response")
effect_24h <- eff_size(emm_24h, sigma(div_mods24h[["*prot + prot*mu"]]), edf = df.residual(div_mods24h[["*prot + prot*mu"]]))

emm_48h <- emmeans(div_mods48h[["*prot + prot*mu"]],
                   ~ Heat | CommRich + Trtmt_Day*protegens + community_expected_mu*protegens,
                   data = diversity_48h, type = "response")
effect_48h <- eff_size(emm_48h, sigma(div_mods48h[["*prot + prot*mu"]]), edf = df.residual(div_mods48h[["*prot + prot*mu"]]))

# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
div_effects_protegens <- data.frame()
div_effects_protegens <- rbind(div_effects_protegens,
                              get_effsize_CIs(effect_6h, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects_protegens$Trtmt_Day <- factor(div_effects_protegens$Trtmt_Day,
                                          levels = c("resist", "recov_1", "recov_2"))
levels(div_effects_protegens$Trtmt_Day) <- c("Resistance", "Early Recovery", "LAte Recovery")

# plot conditional part of the model
ggplot(div_effects_protegens,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day, shape = as.logical(protegens))) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(title = "Extinct reps removed!",
       x = "Effect Size on Shannon Diversity",
       shape = "protegens\npresent?",
       y="Heat duration")

# anyway we can still average over the effect of protegens
# we can do a posthoc on this to illustrate statistically significant effects
posthoc_6h <- emmeans(effect_6h, pairwise ~ Trtmt_Day, data = diversity_6h)
posthoc_12h <- emmeans(effect_12h, pairwise ~ Trtmt_Day, data = diversity_12h)
posthoc_24h <- emmeans(effect_24h, pairwise ~ Trtmt_Day, data = diversity_24h)
posthoc_48h <- emmeans(effect_48h, pairwise ~ Trtmt_Day, data = diversity_48h)

# create a data.frame for plotting
div_effects <- data.frame()
div_effects <- rbind(div_effects,
                     get_posthoc(posthoc_6h, heat_trtmt = 6),
                     get_posthoc(posthoc_12h, heat_trtmt = 12),
                     get_posthoc(posthoc_24h, heat_trtmt = 24),
                     get_posthoc(posthoc_48h, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects$Trtmt_Day <- factor(div_effects$Trtmt_Day,
                                         levels = c("resist", "recov_1", "recov_2"))
levels(div_effects$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot with group labels
ggplot(div_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-2.5, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Shannon Diversity",
       y="Heat duration",
       title = "protegens as non-focal predictor. Extinct reps removed!")

################################
# Plot figure for main text: Figure 4a
################################
fig4a <- ggplot(div_effects,
                aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
          geom_vline(xintercept = 0, colour="darkgrey") +
          geom_point(position = position_dodge(width = 0.5)) +
          geom_errorbarh(position = position_dodge(width = 0.5),
                         aes(xmin = loCI, xmax = hiCI), height = 0.15) +
          scale_x_continuous(limits = c(-4.35, 0.9), expand = c(0,0)) +
          scale_colour_manual(values=trtmt_pal) +
          labs(x = "Effect Size on Shannon Diversity",
               y="Heat Duration (hrs)",
               colour = "Treatment\nDay")

print(fig4a)

png(filename="./figures/Fig4_legend.png", width = 3.40, height = 2.90, units = "in", res=300)
print(fig4a)
dev.off()

png(filename="./figures/Fig4A_plot.png", width = 4.48, height = 2.61, units = "in", res=300)
print(fig4a + theme(legend.position="none"))
dev.off()

# cleanup
#rm(diversity_forFit, diversity_6h, diversity_12h, diversity_24h, diversity_48h,
#   div_mods6h, div_mods12h, div_mods24h, div_mods48h,
#   emm_6h, emm_12h, emm_24h, emm_48h, effect_6h, effect_12h, effect_24h, effect_48h,
#   posthoc_6h, posthoc_12h, posthoc_24h, posthoc_48h,
#   get_effsize_CIs, get_posthoc, 
#   div_effects_protegens, div_effects,
#   plot_model_pred.CommRich, plot_model_pred.MU, fig4a)
```

The overall results are the same as we found above. This shows that excluding the replicates that went extinct had no impact on the overall results.

# Productivity

How is total community density impacted during and after heat? Let's first plot it directly to get an idea of what we're dealing with:

```{r, shannonD_prelim_plot}
ggplot(absDen_forFit %>% filter(!is.na(Total_density)), # remove NA values
       aes(y=Total_density, x=Day, fill=community_expected_mu)) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_quasirandom(alpha=0.7, pch=21) +
  scale_fill_viridis_c(option = "inferno", end=0.85) +
  #scale_y_log10(#) +
  scale_y_continuous(trans=scales::pseudo_log_trans(base = 10), # this prevents 0's from getting lost
                     breaks = 10^(-1:3)) + 
  labs(y = "Productivity",
       fill = "Expected\nCommunity mu")
```


## Choose GLM Family

Now we repeat the same type of emmeans analysis as we did for diversity but using the
total density (aka a proxy of productivity). In this case I am *a priori* more comfortable
with using Poisson or negative binomial family because the total density is more like
counts data.

Remember that total densities below the threshold of detection from wells that *DID*
recover during the recovery phase (i.e., those that did *not* go extinct) have values of
epsilon corresponding to the threshold of detection. (Remaining NA values represent
missing data due to pipetting mistakes or clogs during flow cytometry.) Below threshold of
detection total density values (i.e., epsilons) make up the majority of observations
during resistance for the longest heat duration. See a further discussion in the section
below.

```{r, productivity_glmFamily}
# scale the data by its standard deviation
absDen_forFit$TotDensity_scale <- scale(absDen_forFit$Total_density,
                                        scale = sd(absDen_forFit$Total_density, na.rm = TRUE),
                                        center = FALSE)
# the max scaled value is ~7.9 and almost 3% of the data is 0 values
summary(absDen_forFit$TotDensity_scale)
sum(absDen_forFit$TotDensity_scale == 0) / length(absDen_forFit$TotDensity_scale)
# in fact, the total density data is even more long-tailed than the diversity data. I guess that makes sense as there is a max value for the possible diversity with 4 species.
hist(absDen_forFit$TotDensity_scale)

# re-arrange the levels so that emmeans can be run:
absDen_forFit$Heat <- as.character(absDen_forFit$Heat)
absDen_forFit$Heat[which(absDen_forFit$Heat == 0)] <- "control"
# !!! emmeans expects the control to be the very *last* level !!!
absDen_forFit$Heat <- factor(absDen_forFit$Heat,
                             levels = c("6", "12", "24", "48", "control"))
# let's keep CommRich and Day as numeric for now while we look for the best fitting GLM family

# let's compare different GLM families
try_gaussian <- glmmTMB(TotDensity_scale ~ CommRich*Heat*Day*protegens,
                        data = absDen_forFit,
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_gaussian, plot = TRUE)

try_gamma <- glmmTMB(TotDensity_scale ~ CommRich*Heat*Day*protegens,
                     data = absDen_forFit,
                     family = ziGamma,
                     ziformula = ~1, # this needs to be added because there are 0 values in the data
                     control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_gamma, plot = TRUE)


try_lognorm <- glmmTMB(TotDensity_scale ~ CommRich*Heat*Day*protegens,
                       data = absDen_forFit,
                       family = lognormal,
                       ziformula = ~1, # this needs to be added because there are 0 values in the data
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_lognorm, plot = TRUE)

try_LOGlognorm <- glmmTMB(log(TotDensity_scale + 1) ~ CommRich*Heat*Day*protegens,
                          data = absDen_forFit,
                          family = lognormal,
                          ziformula = ~1, # I'm keeping this as 0-inflated lognormal alone was already over-dispersed. So I want to see if the log(x+1) transformation sufficiently brings in the long tail.
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_LOGlognorm, plot = TRUE)

try_negbinom <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Day*protegens,
                        data = absDen_forFit,
                        family = nbinom2,
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom, plot = TRUE)

try_negbinom0 <- glmmTMB(as.integer(Total_density * 1000) ~ CommRich*Heat*Day*protegens,
                         data = absDen_forFit,
                         family = nbinom2,
                         ziformula = ~1, # try zero inflated distribution
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom0, plot = TRUE)

try_poisson <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Day*protegens,
                       data = absDen_forFit,
                       family = genpois,
                       control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson, plot = TRUE)

try_poisson0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Day*protegens,
                        data = absDen_forFit,
                        family = genpois,
                        ziformula = ~1, # try zero inflated distribution
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson0, plot = TRUE)

# let's check this with AIC and BIC
AIC(try_gaussian, try_gamma, try_lognorm, try_LOGlognorm,
    try_negbinom, try_negbinom0, try_poisson, try_poisson0) %>% arrange(AIC)
BIC(try_gaussian, try_gamma, try_lognorm, try_LOGlognorm,
    try_negbinom, try_negbinom0, try_poisson, try_poisson0) %>% arrange(BIC)

# clean up
rm(try_gaussian, try_gamma, try_lognorm, try_LOGlognorm, try_negbinom, try_negbinom0, try_poisson, try_poisson0)
```

Okay, so let's go for the Poisson family. Its residuals look a little worse than the
log(x+1) transformed lognormal... But I feel really sketched out by the latter model.
Whereas the Poisson is the type of family that I might expect to see for count-style data
like the Total density.

## Compare fit of different models to data subsets

Now we will do the same thing we did for diversity: split up the data into subsets by heat pulse duration, calculate the best fit information criteria (i.e., AIC and BIC) for each data subset, and get the average across the entire data.

```{r, productivity_subset&modelfit}
# a function to fit the different models to the subsetted data:
fit_productivity_models <- function(data_subset) {
  # create list for output
  output.ls <- list()
  
  # this is the simplest model. I'm fitting it to check for colinearity
  output.ls[["simple"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich + Heat + Trtmt_Day + protegens + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  # this is another simple model to check for colinearity
  output.ls[["simple resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich + Heat + Trtmt_Day + community_expected_mu + resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  # this is our null model:
  output.ls[["H0"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  # CommRich as an effect:
  output.ls[["+CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
    output.ls[["*CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  # Resistance to 40C as an effect:
  output.ls[["+resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
    output.ls[["*resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
  # protegens presence as an effect:
  output.ls[["+prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
    output.ls[["*prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
    # expected community growth rate as an effect:
    output.ls[["+mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
  
    output.ls[["*mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    # interactions of CommRich with resistance
    output.ls[["+CommRich +resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + CommRich + resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich +resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich + resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich*resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich*resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

    output.ls[["*CommRich + CommRich*resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich + CommRich*resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*resist +CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*resistant + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*resist + resist*CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*resistant + resistant*CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    # interactions of CommRich with protegens
    output.ls[["+CommRich +prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + CommRich + protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich*prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich*protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich +prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich + protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich + CommRich*prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich + CommRich*protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot +CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot + prot*CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens + protegens*CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    # interactions of CommRich with expected community growth rate
    output.ls[["+CommRich +mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + CommRich + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich*mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich*community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich +mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich + CommRich*mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich + CommRich*community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu +CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu + mu*CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + community_expected_mu*CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    # interactions of resistance with expected community growth rate
    output.ls[["+resist +mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + resistant + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*resist*mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*resistant*community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*resist +mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*resistant + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*resist + resist*mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*resistant + resistant*community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu +resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu + mu*resist"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + community_expected_mu*resistant,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    # interactions of protegens with expected community growth rate
    output.ls[["+prot +mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day + protegens + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot*mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens*community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot +mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot + prot*mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens + protegens*community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu +prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu + mu*prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + community_expected_mu*protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    # I need to consider models with even more predictors:
    # e.g., with CommRich, mu, and resist 
    output.ls[["*prot +mu +CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens + community_expected_mu + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot*mu +CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens*community_expected_mu + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot + mu*CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens + community_expected_mu*CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot + prot*mu +CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens + protegens*community_expected_mu + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot +mu + prot*CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens + community_expected_mu + protegens*CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*prot*CommRich +mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*protegens*CommRich + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu +prot +CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + protegens + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu + mu*prot +CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + community_expected_mu*protegens + CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu +prot + mu*CommRich"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu + protegens + community_expected_mu*CommRich,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*mu*CommRich +prot"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*community_expected_mu*CommRich + protegens,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
    
    output.ls[["*CommRich +prot +mu"]] <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ Heat*Trtmt_Day*CommRich + protegens + community_expected_mu,
                               data = data_subset,
                               family = genpois,
                               control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

  return(output.ls)
}

# a function to plot model predictions against the data
  # there's no colours. Just the facets for heat & protegens
plot_model_pred.nocolours <- function(mod_list, mod_name){
  # create data.frame for plotting
  absDen_predict <- cbind(mod_list[[mod_name]]$frame,
                          predicted=predict(mod_list[[mod_name]], type="response"))
  # change the first column name for easier plotting
  colnames(absDen_predict)[1] <- "observed"
  # create the plot
  out <- ggplot(absDen_predict, 
                aes(x=Trtmt_Day, y=observed)) +
         facet_grid(protegens ~ Heat) +
         geom_jitter(alpha=0.4) +
         geom_line(aes(y=predicted, group=paste(Heat, protegens)), colour="red") +
         scale_y_log10() +
         scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
         labs(y="Absolute Density (rescaled)",
              title=paste(mod_name, "model predictions"))
  return(out)
  rm(absDen_predict)
}

# a function to plot model predictions against the data
  # INOCULATED COMMUNITY RICHNESS is plotted as different colours
plot_model_pred.CommRich <- function(mod_list, mod_name){
  # create data.frame for plotting
  absDen_predict <- cbind(mod_list[[mod_name]]$frame,
                          predicted=predict(mod_list[[mod_name]], type="response"))
  # change the first column name for easier plotting
  colnames(absDen_predict)[1] <- "observed"
  # create the plot
  out <- ggplot(absDen_predict, 
                aes(x=Trtmt_Day, y=observed, colour=as.factor(CommRich))) +
         facet_grid(protegens ~ Heat) +
         geom_jitter(alpha=0.4) +
         geom_line(aes(y=predicted, group=as.factor(CommRich))) +
         scale_y_log10() +
         scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
         labs(y="Absolute Density (rescaled)",
              colour="CommRich",
              title=paste(mod_name, "model predictions"))
  return(out)
  rm(absDen_predict)
}

# a function to plot model predictions against the data
  # EXPECTED COMMUNITY MU is plotted as different colours
plot_model_pred.MU <- function(mod_list, mod_name){
  # create data.frame for plotting
  absDen_predict <- cbind(mod_list[[mod_name]]$frame,
                          predicted=predict(mod_list[[mod_name]], type="response"))
  # change the first column name for easier plotting
  colnames(absDen_predict)[1] <- "observed"
  # create the plot
  out <- ggplot(absDen_predict, 
                aes(x=Trtmt_Day, y=observed,
                    colour=community_expected_mu, group=as.factor(community_expected_mu))) +
         facet_grid(protegens ~ Heat) +
         geom_jitter(alpha=0.4) +
         geom_line(aes(y=predicted)) +
         scale_y_log10() +
         scale_colour_viridis_c(option = "inferno", end=0.85) +
         labs(y="Absolute Density (rescaled)",
              colour="Expected\nCommunity mu",
              title=paste(mod_name, "model predictions"))
  return(out)
  rm(absDen_predict)
}

####################
# 6h heat duration
####################
# grab just the treatment with its associated control data
absDen_6h <- rbind(absDen_forFit %>% filter(Heat == "6"),
                   absDen_forFit %>% filter(Heat == "control", Day < 4))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_6h$Trtmt_Day <- "resist"
absDen_6h$Trtmt_Day[absDen_6h$Day == 2] <- "recov_1"
absDen_6h$Trtmt_Day[absDen_6h$Day == 3] <- "recov_2"
# appropriately distinguish between numbers and factors
absDen_6h$Trtmt_Day <- as.factor(absDen_6h$Trtmt_Day)
absDen_6h$Heat <- droplevels(absDen_6h$Heat)
absDen_6h$resistant <- as.factor(absDen_6h$resistant)
absDen_6h$protegens <- as.factor(absDen_6h$protegens)

# fit different models:
absDen_mods6h <- fit_productivity_models(absDen_6h)

# check the simplest possible models for multicolinearity
check_collinearity(absDen_mods6h[["simple"]])
check_collinearity(absDen_mods6h[["simple resist"]])

# print a summary table of the model fits
data.frame(pars = sapply(absDen_mods6h, npar_of_glmmTMB_fit),
                           AIC = sapply(absDen_mods6h, AIC),
                           AICc = sapply(absDen_mods6h, AICc),
                           BIC = sapply(absDen_mods6h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# plot the best model for 6h:
print(plot_model_pred.MU(mod_list=absDen_mods6h, mod_name="*mu +prot"))
# plot the best model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods6h, mod_name="*prot*mu +CommRich"))
# plot the preferred model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods6h, mod_name="*prot +mu + prot*CommRich"))


# check the fit and estimates of the best model for the complete data:
simulateResiduals(fittedModel = absDen_mods6h[["*prot*mu +CommRich"]], plot = TRUE)
summary(absDen_mods6h[["*prot*mu +CommRich"]])
# check the fit and estimates of the preferred model for the complete data:
simulateResiduals(fittedModel = absDen_mods6h[["*prot*mu +CommRich"]], plot = TRUE)
summary(absDen_mods6h[["*prot +mu + prot*CommRich"]])



####################
# 12h heat duration
####################
# grab just the treatment with its associated control data
absDen_12h <- rbind(absDen_forFit %>% filter(Heat == "12", Day > 1),
                       absDen_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_12h$Trtmt_Day <- "resist"
absDen_12h$Trtmt_Day[absDen_12h$Day == 3] <- "recov_1"
absDen_12h$Trtmt_Day[absDen_12h$Day == 4] <- "recov_2"
# appropriately distinguish between numbers and factors
absDen_12h$Trtmt_Day <- as.factor(absDen_12h$Trtmt_Day)
absDen_12h$Heat <- droplevels(absDen_12h$Heat)
absDen_12h$resistant <- as.factor(absDen_12h$resistant)
absDen_12h$protegens <- as.factor(absDen_12h$protegens)

# fit different models:
absDen_mods12h <- fit_productivity_models(absDen_12h)

# check the simplest possible models for multicolinearity
check_collinearity(absDen_mods12h[["simple"]])
check_collinearity(absDen_mods12h[["simple resist"]])

# print a summary table of the model fits
data.frame(pars = sapply(absDen_mods12h, npar_of_glmmTMB_fit),
                           AIC = sapply(absDen_mods12h, AIC),
                           AICc = sapply(absDen_mods12h, AICc),
                           BIC = sapply(absDen_mods12h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# plot the best model for 12h:
print(plot_model_pred.MU(mod_list=absDen_mods12h, mod_name="*mu +prot"))
# plot the best model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods12h, mod_name="*prot*mu +CommRich"))
# plot the preferred model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods12h, mod_name="*prot +mu + prot*CommRich"))


# check the fit and estimates of the best model for the complete data:
simulateResiduals(fittedModel = absDen_mods12h[["*prot*mu +CommRich"]], plot = TRUE)
summary(absDen_mods12h[["*prot*mu +CommRich"]])
# check the fit and estimates of the preferred model for the complete data:
simulateResiduals(fittedModel = absDen_mods12h[["*prot +mu + prot*CommRich"]], plot = TRUE)
simulateResiduals(fittedModel = absDen_mods12h[["*prot*mu +CommRich"]], plot = TRUE)
summary(absDen_mods12h[["*prot +mu + prot*CommRich"]])



####################
# 24h heat duration
####################
# grab just the treatment with its associated control data
absDen_24h <- rbind(absDen_forFit %>% filter(Heat == "24", Day > 1),
                       absDen_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_24h$Trtmt_Day <- "resist"
absDen_24h$Trtmt_Day[absDen_24h$Day == 3] <- "recov_1"
absDen_24h$Trtmt_Day[absDen_24h$Day == 4] <- "recov_2"
# appropriately distinguish between numbers and factors
absDen_24h$Trtmt_Day <- as.factor(absDen_24h$Trtmt_Day)
absDen_24h$Heat <- droplevels(absDen_24h$Heat)
absDen_24h$resistant <- as.factor(absDen_24h$resistant)
absDen_24h$protegens <- as.factor(absDen_24h$protegens)

# fit different models:
absDen_mods24h <- fit_productivity_models(absDen_24h)

# check the simplest possible models for multicolinearity
check_collinearity(absDen_mods24h[["simple"]])
check_collinearity(absDen_mods24h[["simple resist"]])

# print a summary table of the model fits
data.frame(pars = sapply(absDen_mods24h, npar_of_glmmTMB_fit),
                           AIC = sapply(absDen_mods24h, AIC),
                           AICc = sapply(absDen_mods24h, AICc),
                           BIC = sapply(absDen_mods24h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# plot a top model that is unique for 24h:
  # note that this is the 3rd best model for 24h:
print(plot_model_pred.CommRich(mod_list=absDen_mods24h, mod_name="*prot*CommRich +mu"))
# plot the best model for the complete data:
  # note that this is ALSO the 1st best model for 24h:
print(plot_model_pred.MU(mod_list=absDen_mods24h, mod_name="*prot*mu +CommRich"))
# plot the preferred model for the complete data:
  # note that this is ALSO the 2nd best model for 24h:
print(plot_model_pred.MU(mod_list=absDen_mods24h, mod_name="*prot +mu + prot*CommRich"))


# check the fit and estimates of the best model for the complete data:
simulateResiduals(fittedModel = absDen_mods24h[["*prot*mu +CommRich"]], plot = TRUE)
summary(absDen_mods24h[["*prot*mu +CommRich"]])
# check the fit and estimates of the preferred model for the complete data:
simulateResiduals(fittedModel = absDen_mods24h[["*prot +mu + prot*CommRich"]], plot = TRUE)
summary(absDen_mods24h[["*prot +mu + prot*CommRich"]])


####################
# 48h heat duration
####################
# grab just the treatment with its associated control data
absDen_48h <- rbind(absDen_forFit %>% filter(Heat == "48", Day > 2),
                       absDen_forFit %>% filter(Heat == "control", Day > 2))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_48h$Trtmt_Day <- "resist"
absDen_48h$Trtmt_Day[absDen_48h$Day == 4] <- "recov_1"
absDen_48h$Trtmt_Day[absDen_48h$Day == 5] <- "recov_2"
# appropriately distinguish between numbers and factors
absDen_48h$Trtmt_Day <- as.factor(absDen_48h$Trtmt_Day)
absDen_48h$Heat <- droplevels(absDen_48h$Heat)
absDen_48h$resistant <- as.factor(absDen_48h$resistant)
absDen_48h$protegens <- as.factor(absDen_48h$protegens)

# fit different models:
absDen_mods48h <- fit_productivity_models(absDen_48h)

# check the simplest possible models for multicolinearity
check_collinearity(absDen_mods48h[["simple"]])
check_collinearity(absDen_mods48h[["simple resist"]])

# print a summary table of the model fits
data.frame(pars = sapply(absDen_mods48h, npar_of_glmmTMB_fit),
                           AIC = sapply(absDen_mods48h, AIC),
                           AICc = sapply(absDen_mods48h, AICc),
                           BIC = sapply(absDen_mods48h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)


# plot the best model for 48h:
print(plot_model_pred.CommRich(mod_list=absDen_mods48h, mod_name="*prot +CommRich"))
# plot the best model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods48h, mod_name="*prot*mu +CommRich"))
# plot the preferred model for the complete data:
print(plot_model_pred.CommRich(mod_list=absDen_mods48h, mod_name="*prot +mu + prot*CommRich"))

# check the fit and estimates of the best model for the complete data:
simulateResiduals(fittedModel = absDen_mods48h[["*prot*mu +CommRich"]], plot = TRUE)
summary(absDen_mods48h[["*prot*mu +CommRich"]])
# check the fit and estimates of the preferred model for the complete data:
simulateResiduals(fittedModel = absDen_mods48h[["*prot +mu + prot*CommRich"]], plot = TRUE)
summary(absDen_mods48h[["*prot +mu + prot*CommRich"]])


####################
# Select the best model across all data subsets
####################
# for each information criterion, get the average across all data subsets
meanIC <- data.frame(pars = sapply(absDen_mods48h, npar_of_glmmTMB_fit),
                     AIC = sapply(absDen_mods6h, AIC) + sapply(absDen_mods12h, AIC) + sapply(absDen_mods24h, AIC) + sapply(absDen_mods48h, AIC),
                     AICc = sapply(absDen_mods6h, AICc) + sapply(absDen_mods12h, AICc) + sapply(absDen_mods24h, AICc) + sapply(absDen_mods48h, AICc),
                     BIC = sapply(absDen_mods6h, BIC) + sapply(absDen_mods12h, BIC) + sapply(absDen_mods24h, BIC) + sapply(absDen_mods48h, BIC)) %>%
            mutate(AIC = AIC/4,
                   AICc = AICc/4,
                   BIC = BIC/4) %>%
              mutate(dAIC = min(AIC)-AIC,
                     dAICc = min(AICc)-AICc,
                     dBIC = min(BIC)-BIC)
meanIC %>% arrange(BIC)

meanIC %>% arrange(AIC)

# clean up
rm(meanIC)
```

Both AIC and BIC agree that the best model is "\*prot\*mu +CommRich". However, this model is rather complex. I'm less concerned with the danger of over-fitting as each data subset has between 360 - 393 observations (so we are still within the rule-of-thumb of 10 to 15 observations per parameter). But I am very concerned about the number of interaction in this complex model. Recall that its full formula is ~ Heat\*Trtmt_Day\*protegens\*community_expected_mu + CommRich. So it has a 4-way interaction!!! YIKES!

I really don't want to use a model with this many interactions because this usually leads to poor estimates (which cannot be fixed with a posthoc analysis). Or, at the very least it leads to parameter estimates that are very hard to interpret (see above).

I feel confident that the complex model "\*prot\*mu +CommRich" is quite close to the "Truth" because the short heat pulse data prefer the interaction Heat\*Trtmt_Day\*community_expected_mu while the long heat pulse data prefer the interaction Heat\*Trtmt_Day\*protegens. So I think this complex model is the only one that's able to accommodate both of these effects across all heat pulse durations. The problem is that it's too complex to be interpretable.

Therefore, I will use the BIC as the criteria for selecting a model that still captures the main effects but is much simpler. The model "\*prot +mu + prot\*CommRich" has a negligible $\Delta$BIC (recall that [the rule of thumb for BIC model selection criteria ](https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118856406.app5) is that $\Delta$BIC $<2$ is not worth mentioning). (The $\Delta$AIC is substantially different between this simpler model and the best one but in general AIC is favouring all the most complex models anyway (which is expected as AIC penalizes less for model complexity). Here we really are much more interested in BIC because we need to simplify our model.)


I think I should modify the structure of the analysis as follows:

## Effect sizes for complex model

Check whether the most complex model is working okay here,

```{r, productivity_try48hcomplexmodel}
emmeans(absDen_mods48h[["*prot*mu +CommRich"]],
                   ~ Heat | CommRich + Trtmt_Day*protegens*community_expected_mu,
                   data = absDen_48h, type = "response")
```

Let's check whether the effect sizes of the complex model are consistent with those of the simpler model.

```{r, productivity_effectSize_complexModel}
# plot the effect size contingent on protegens
effect_6h_protegens <- eff_size(emmeans(absDen_mods6h[["*prot*mu +CommRich"]], ~ Heat | CommRich + Trtmt_Day*protegens*community_expected_mu, data = absDen_6h),
                                sigma(absDen_mods6h[["*prot*mu +CommRich"]]),
                                edf = df.residual(absDen_mods6h[["*prot*mu +CommRich"]]))
effect_12h_protegens <- eff_size(emmeans(absDen_mods12h[["*prot*mu +CommRich"]], ~ Heat | CommRich + Trtmt_Day*protegens*community_expected_mu, data = absDen_12h),
                                sigma(absDen_mods12h[["*prot*mu +CommRich"]]),
                                edf = df.residual(absDen_mods12h[["*prot*mu +CommRich"]]))
effect_24h_protegens <- eff_size(emmeans(absDen_mods24h[["*prot*mu +CommRich"]], ~ Heat | CommRich + Trtmt_Day*protegens*community_expected_mu, data = absDen_24h),
                                sigma(absDen_mods24h[["*prot*mu +CommRich"]]),
                                edf = df.residual(absDen_mods24h[["*prot*mu +CommRich"]]))
effect_48h_protegens <- eff_size(emmeans(absDen_mods48h[["*prot*mu +CommRich"]], ~ Heat | CommRich + Trtmt_Day*protegens*community_expected_mu, data = absDen_48h),
                                sigma(absDen_mods48h[["*prot*mu +CommRich"]]),
                                edf = df.residual(absDen_mods48h[["*prot*mu +CommRich"]]))


# a function that extracts the confidence intervals from eff_size contingent on protegens
get_effsize_CIs <- function(eff_size_object, heat_trtmt) {
  data.frame(Heat = heat_trtmt,
             CommRich = confint(eff_size_object)[[2]],
             Trtmt_Day = confint(eff_size_object)[[3]],
             protegens = confint(eff_size_object)[[4]],
             community_expected_mu = confint(eff_size_object)[[5]],
             effect_est = confint(eff_size_object)[[6]], #[[5]],
             effect_loCI = confint(eff_size_object)[[9]], #[[8]],
             effect_hiCI = confint(eff_size_object)[[10]]) #[[9]])
}

# create a data.frame for plotting marginal effect sizes using a forest plot
productivity_protegens <- data.frame()
productivity_protegens <- rbind(productivity_protegens,
                              get_effsize_CIs(effect_6h_protegens, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h_protegens, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h_protegens, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h_protegens, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_protegens$Trtmt_Day <- factor(productivity_protegens$Trtmt_Day,
                                         levels = c("resist", "recov_1", "recov_2"))
levels(productivity_protegens$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

#plot
ggplot(productivity_protegens,
       aes(x = effect_est, y = as.factor(Heat), colour = Trtmt_Day, shape = protegens)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = effect_loCI, xmax = effect_hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Heat duration (hrs)",
       shape = "protegens\npresent?",
       title = "*prot*mu +CommRich")

# But we are not interested in the details of protegens. Let's do the post-hoc by averaging across the effects of protegens.

posthoc_6h <- emmeans(effect_6h_protegens,
                      pairwise ~ Trtmt_Day,
                      data = absDen_6h)
posthoc_12h <- emmeans(effect_12h_protegens,
                       pairwise ~ Trtmt_Day,
                       data = absDen_12h)
posthoc_24h <- emmeans(effect_24h_protegens,
                       pairwise ~ Trtmt_Day,
                       data = absDen_24h)
posthoc_48h <- emmeans(effect_48h_protegens,
                       pairwise ~ Trtmt_Day,
                       data = absDen_48h)

# a function that extracts the confidence intervals from a post-hoc object *WITHOUT* protegens
get_posthoc_NOprot <- function(posthoc_object, heat_trtmt) {
  output <- multcomp::cld(posthoc_object, alpha=0.05/4, Letters = letters) %>%
              data.frame() %>%
                select(-df)
  colnames(output)[2:6] <- c("est", "SE", "loCI", "hiCI", "groups")
  output$Heat <- heat_trtmt
  return(output)
}

# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
productivity_effects <- data.frame()
productivity_effects <- rbind(productivity_effects,
                              get_posthoc_NOprot(posthoc_6h, heat_trtmt = 6),
                              get_posthoc_NOprot(posthoc_12h, heat_trtmt = 12),
                              get_posthoc_NOprot(posthoc_24h, heat_trtmt = 24),
                              get_posthoc_NOprot(posthoc_48h, heat_trtmt = 48))

# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_effects$Trtmt_Day <- factor(productivity_effects$Trtmt_Day,
                                         levels = c("resist", "recov_1", "recov_2"))
levels(productivity_effects$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot
ggplot(productivity_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-0.009, label=groups)) +
  #scale_x_continuous(breaks=c(-0.006, -0.003, 0), limits=c(-0.01, 0.003)) + 
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Heat duration (hrs)",
       title = "Averaged across protegens (with extinct reps)")
```

Okay so it seems that adding the community growth rate as a predictor kinda changes everything. We still see decoupling between resistance and recovery but now the time frame of it varies depending on community composition. For the communities without protegens, decoupling happens sooner (e.g., between resistance and early recovery at 12h), is maximized sooner (at 24h), and then disappears completely by 48h (i.e., because of extinction). On the other hand, we already saw in the extinction analysis above that communities with protegens are protected from extinction for all heat pulse durations investigated here. For these communities, decoupling only begins to happen much later (at 48h). Although these results are more complex than what I had previously reported, they make more sense.

## Simpler model: Calculate effect sizes conditional on protegens

Recall that we are NOT interested in reporting the results from the most complex model. Are the effect sizes of the preferred, less complex model ("\*prot +mu + prot\*CommRich") consistent with those above?

```{r, productivity_effectSize_protegens}
# plot the effect size contingent on protegens
effect_6h_protegens <- eff_size(emmeans(absDen_mods6h[["*prot +mu + prot*CommRich"]], ~ Heat | Trtmt_Day*protegens + community_expected_mu + protegens*CommRich, data = absDen_6h),
                                sigma(absDen_mods6h[["*prot +mu + prot*CommRich"]]),
                                edf = df.residual(absDen_mods6h[["*prot +mu + prot*CommRich"]]))
effect_12h_protegens <- eff_size(emmeans(absDen_mods12h[["*prot +mu + prot*CommRich"]], ~ Heat | Trtmt_Day*protegens + community_expected_mu + protegens*CommRich, data = absDen_12h),
                                sigma(absDen_mods12h[["*prot +mu + prot*CommRich"]]),
                                edf = df.residual(absDen_mods12h[["*prot +mu + prot*CommRich"]]))
effect_24h_protegens <- eff_size(emmeans(absDen_mods24h[["*prot +mu + prot*CommRich"]], ~ Heat | Trtmt_Day*protegens + community_expected_mu + protegens*CommRich, data = absDen_24h),
                                sigma(absDen_mods24h[["*prot +mu + prot*CommRich"]]),
                                edf = df.residual(absDen_mods24h[["*prot +mu + prot*CommRich"]]))
effect_48h_protegens <- eff_size(emmeans(absDen_mods48h[["*prot +mu + prot*CommRich"]], ~ Heat | Trtmt_Day*protegens + community_expected_mu + protegens*CommRich, data = absDen_48h),
                                sigma(absDen_mods48h[["*prot +mu + prot*CommRich"]]),
                                edf = df.residual(absDen_mods48h[["*prot +mu + prot*CommRich"]]))

# a function that extracts the confidence intervals from eff_size contingent on protegens
get_effsize_CIs <- function(eff_size_object, heat_trtmt) {
  data.frame(Heat = heat_trtmt,
             CommRich = confint(eff_size_object)[[5]],
             Trtmt_Day = confint(eff_size_object)[[2]],
             protegens = confint(eff_size_object)[[3]],
             community_expected_mu = confint(eff_size_object)[[4]],
             effect_est = confint(eff_size_object)[[6]], #[[5]],
             effect_loCI = confint(eff_size_object)[[9]], #[[8]],
             effect_hiCI = confint(eff_size_object)[[10]]) #[[9]])
}

# create a data.frame for plotting marginal effect sizes using a forest plot
productivity_protegens <- data.frame()
productivity_protegens <- rbind(productivity_protegens,
                              get_effsize_CIs(effect_6h_protegens, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h_protegens, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h_protegens, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h_protegens, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_protegens$Trtmt_Day <- factor(productivity_protegens$Trtmt_Day,
                                         levels = c("resist", "recov_1", "recov_2"))
levels(productivity_protegens$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

#plot
ggplot(productivity_protegens,
       aes(x = effect_est, y = as.factor(Heat), colour = Trtmt_Day, shape = protegens)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = effect_loCI, xmax = effect_hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Heat duration (hrs)",
       shape = "protegens\npresent?",
       title = "*prot +mu + prot*CommRich")

# we can do a posthoc on this to illustrate statistically significant effects
posthocPROT_6h <- emmeans(effect_6h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_6h)
posthocPROT_12h <- emmeans(effect_12h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_12h)
posthocPROT_24h <- emmeans(effect_24h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_24h)
posthocPROT_48h <- emmeans(effect_48h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_48h)

# a function that extracts the confidence intervals from a post-hoc object *WITH* protegens
get_posthoc_YESprot <- function(posthoc_object, heat_trtmt) {
  output <- multcomp::cld(posthoc_object, alpha=0.05/4, Letters = letters) %>%
              data.frame() %>%
                select(-df)
  colnames(output)[3:7] <- c("est", "SE", "loCI", "hiCI", "groups")
  output$Heat <- heat_trtmt
  return(output)
}

# create a data.frame for plotting
prod_effects_protegens <- data.frame()
prod_effects_protegens <- rbind(prod_effects_protegens,
                               get_posthoc_YESprot(posthocPROT_6h, heat_trtmt = 6),
                               get_posthoc_YESprot(posthocPROT_12h, heat_trtmt = 12),
                               get_posthoc_YESprot(posthocPROT_24h, heat_trtmt = 24),
                               get_posthoc_YESprot(posthocPROT_48h, heat_trtmt = 48))

## note that for the decoupling plots I am using a Bonferroni-corrected alpha.
# let's get those confidence intervals because we will need them to plot decoupling below:
posthocPROT_6h_WIDER <- emmeans(effect_6h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_6h, level=0.9875)
posthocPROT_12h_WIDER <- emmeans(effect_12h_protegens, pairwise ~ Trtmt_Day*protegens, data = posthocPROT_12h, level=0.9875)
posthocPROT_24h_WIDER <- emmeans(effect_24h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_24h, level=0.9875)
posthocPROT_48h_WIDER <- emmeans(effect_48h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_48h, level=0.9875)
# put these wider CIs into a table
widerCIs <- data.frame()
widerCIs <- rbind(widerCIs,
                  get_posthoc_YESprot(posthocPROT_6h_WIDER, heat_trtmt = 6),
                  get_posthoc_YESprot(posthocPROT_12h_WIDER, heat_trtmt = 12),
                  get_posthoc_YESprot(posthocPROT_24h_WIDER, heat_trtmt = 24),
                  get_posthoc_YESprot(posthocPROT_48h_WIDER, heat_trtmt = 48))
# rename the columns to remind us that this is the Bonferroni corrected alpha
colnames(widerCIs)[5:6] <- c("loCI_bonAlpha", "hiCI_bonAlpha")
# combine the wider CIs with the effect sizes
prod_effects_protegens <- inner_join(prod_effects_protegens,
                                     widerCIs %>% select(-SE))
rm(widerCIs)


# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
prod_effects_protegens$Trtmt_Day <- factor(prod_effects_protegens$Trtmt_Day,
                                          levels = c("resist", "recov_1", "recov_2"))
levels(prod_effects_protegens$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot with group labels
ggplot(prod_effects_protegens,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day, shape=protegens)) +
  facet_grid( ~ protegens) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-0.0035, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y="Heat duration",
       shape = "protegens\npresent?",
       title = "*prot +mu + prot*CommRich")
```


### Decoupling conditional on protegens

```{r, decoupling_protegens}
###########################
# first define some functions
###########################

# a function that takes the multcomp::cld letters from 2 groups and returns TRUE when no letters are shared (or FALSE when any letter is shared)
are_groups_different <- function(group1, group2) {
  # convert the groups columns into TRUE/FALSE columns indicating significant difference between resistance and recovery effect sizes
  first_group <- group1 %>%
                  # remove any white space
                  str_trim() %>%
                    # split the string up into single characters
                    strsplit(x=., split = character(0))
  second_group <- group2 %>%
                  # remove any white space
                  str_trim() %>%
                    # split the string up into single letters
                    strsplit(x=., split = character(0))
  # test if any letters are common. If there are, then they are NOT different so return FALSE (and vice versa).
  return( !any(first_group[[1]] %in% second_group[[1]]) )
}

# a function to calculate distance from the point (x, y) to the line y = x: positive values are ABOVE the line and negative values are BELOW the line.
# this is used to calculate decoupling
dist_to_xyline <- function(x, y) {
  (y - x) / sqrt(2)  # distance formula derived from y = x line
}

# a function to estimate mean decoupling and its confidence intervals given mean and SYMMETRIC confidence intervals for resistance and recovery.
# Note that I can use the univariate confidence intervals only by assuming there's no correlation between resistance and recovery (which is exactly the opposite of the whole point of coupling)
# ...also, beware the the CI's come from a posthoc so they are more conservative that the real CI's...
estimate_decoupling <- function(resist_est, resist_hiCI,
                                recov_est, recov_hiCI) {
  # check the input values
  if(resist_hiCI < resist_est)
    stop("`resist_hiCI` must be the *UPPER* confidence interval on resistance.")
  if(recov_hiCI < recov_est)
    stop("`recov_hiCI` must be the *UPPER* confidence interval on recovery.")
  
  # get the co-ordinates that define the ellipse
  x0 <- resist_est # x-coordinate of the center of the ellipse
  y0 <- recov_est # y-coordinate of the center of the ellipse
  a <- resist_hiCI - resist_est # semi-major axis: horizontal radius
  b <- recov_hiCI - recov_est # semi-major axis: vertical radius
  
  # generate points on the perimeter of the ellipse
  theta <- seq(0, 2 * pi, length.out = 360)  # angles
  x_ellipse <- x0 + a * cos(theta)  # x-coordinates on the ellipse
  y_ellipse <- y0 + b * sin(theta)   # y-coordinates on the ellipse
  
  # decoupling measures the distance between the point and the y=x line
  mean <- dist_to_xyline(x0, y0)
  
  # do the same for all points on the ellipse defining the CI
  distances <- dist_to_xyline(x_ellipse, y_ellipse)
  
  # maximum and minimum distances define the hiCI and loCI, respectively
  hiCI <- max(distances)
  loCI <- min(distances)
  
  return(c(est_decoupling = mean, loCI_decoupling = loCI, hiCI_decoupling = hiCI))
}
# positive values are ABOVE the y=x line and negative values are BELOW the y=x line


#############################
# Decoupling
#############################

decoupling_productivity <- prod_effects_protegens %>% select(-loCI, -hiCI)
# keep just the Bonferroni-corrected (wider) confidence intervals
decoupling_productivity <- decoupling_productivity %>%
                              rename(loCI = loCI_bonAlpha,
                                     hiCI = hiCI_bonAlpha)
# for easier coding, rename the levels of Trtmt_Day
levels(decoupling_productivity$Trtmt_Day) <- c("resist", "early_recov", "late_recov")

# create data.frame for plotting
decoupling_productivity <- decoupling_productivity %>%
                              pivot_wider(names_from = Trtmt_Day,
                                          values_from = c(est, loCI, hiCI, SE, groups))

# columns that indicate if resistance is significantly different from recovery
decoupling_productivity$early_recov_VS_resist <- mapply(are_groups_different,
                                                        decoupling_productivity$groups_early_recov,
                                                        decoupling_productivity$groups_resist)
decoupling_productivity$late_recov_VS_resist <- mapply(are_groups_different,
                                                       decoupling_productivity$groups_late_recov,
                                                       decoupling_productivity$groups_resist)
# clean up extra columns
decoupling_productivity <- decoupling_productivity %>% select(-groups_resist, -groups_early_recov, -groups_late_recov)


# first plot the decoupling on early recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
  facet_grid(~protegens) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_early_recov, ymax = hiCI_early_recov), width=0) +
  # center the plot on 0,0:
  scale_x_continuous(limits = c(-0.008, 0.008), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.0045, 0.0045), expand = c(0, 0)) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (with extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Early Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# here's another way to plot it where the confidence intervals are shown as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
    facet_grid(~protegens) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_early_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_early_recov - est_early_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (with extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Early Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")


# next plot the decoupling on later recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
  facet_grid(~protegens) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_late_recov, ymax = hiCI_late_recov), width=0) +
  # center the plot on 0,0:
  scale_x_continuous(limits = c(-0.008, 0.008), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.0036, 0.0036), expand = c(0, 0)) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (with extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Late Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# late recovery with CI plotted as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
    facet_grid(~protegens) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_late_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_late_recov - est_late_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (with extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Late Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# finally estimate decoupling by getting the distance to the y=x line
# calculate decoupling between resistance and early recovery
early_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_early_recov,
                                  recov_hiCI = hiCI_early_recov)))
# add annotation
early_decoupling <- cbind(decoupling_productivity[,1:2],
                          early_decoupling)

ggplot(early_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling)) +
  facet_grid(~protegens) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  labs(title = "Early recovery (WITH extinct reps)",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")


# calculate decoupling between resistance and late recovery
late_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_late_recov,
                                  recov_hiCI = hiCI_late_recov)))
# add annotation
late_decoupling <- cbind(decoupling_productivity[,1:2],
                          late_decoupling)

ggplot(late_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling)) +
  facet_grid(~protegens) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  labs(title = "Late recovery (WITH extinct reps)",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")
```

## Simpler model: Calculate the effect sizes marginal on protegens

```{r, productivity_effectSize}
##############################
# effect sizes with protegens as non-focal
##############################
# But we are not interested in the details of protegens. Let's do the post-hoc again now averaging across the effects of protegens.

posthoc_6h <- emmeans(effect_6h_protegens, pairwise ~ Trtmt_Day, data = absDen_6h)
posthoc_12h <- emmeans(effect_12h_protegens, pairwise ~ Trtmt_Day, data = absDen_12h)
posthoc_24h <- emmeans(effect_24h_protegens, pairwise ~ Trtmt_Day, data = absDen_24h)
posthoc_48h <- emmeans(effect_48h_protegens, pairwise ~ Trtmt_Day, data = absDen_48h)

# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
productivity_effects <- data.frame()
productivity_effects <- rbind(productivity_effects,
                              get_posthoc_NOprot(posthoc_6h, heat_trtmt = 6),
                              get_posthoc_NOprot(posthoc_12h, heat_trtmt = 12),
                              get_posthoc_NOprot(posthoc_24h, heat_trtmt = 24),
                              get_posthoc_NOprot(posthoc_48h, heat_trtmt = 48))


## note that for the decoupling plots I am using a Bonferroni-corrected alpha.
# let's get those confidence intervals because we will need them to plot decoupling below:
posthoc_6h_WIDER <- emmeans(effect_6h_protegens, pairwise ~ Trtmt_Day, data = absDen_6h, level=0.9875)
posthoc_12h_WIDER <- emmeans(effect_12h_protegens, pairwise ~ Trtmt_Day, data = absDen_12h, level=0.9875)
posthoc_24h_WIDER <- emmeans(effect_24h_protegens, pairwise ~ Trtmt_Day, data = absDen_24h, level=0.9875)
posthoc_48h_WIDER <- emmeans(effect_48h_protegens, pairwise ~ Trtmt_Day, data = absDen_48h, level=0.9875)
# put these wider CIs into a table
widerCIs <- data.frame()
widerCIs <- rbind(widerCIs,
                  get_posthoc_NOprot(posthoc_6h_WIDER, heat_trtmt = 6),
                  get_posthoc_NOprot(posthoc_12h_WIDER, heat_trtmt = 12),
                  get_posthoc_NOprot(posthoc_24h_WIDER, heat_trtmt = 24),
                  get_posthoc_NOprot(posthoc_48h_WIDER, heat_trtmt = 48))
# rename the columns to remind us that this is the Bonferroni corrected alpha
colnames(widerCIs)[4:5] <- c("loCI_bonAlpha", "hiCI_bonAlpha")
# combine the wider CIs with the effect sizes
productivity_effects <- inner_join(productivity_effects,
                                   widerCIs %>% select(-SE))
rm(widerCIs)

# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_effects$Trtmt_Day <- factor(productivity_effects$Trtmt_Day,
                                         levels = c("resist", "recov_1", "recov_2"))
levels(productivity_effects$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot with group labels
ggplot(productivity_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-0.008, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Heat duration (hrs)",
       title = "protegens as non-focal predictor (i.e., marginalized)")


#######
# finally, we will do a series of pairwise two-tailed t-tests to compare between heat durations
#######

# estimate the sample sizes
temp <- productivity_effects # copy the effects to temp
productivity_effects <- rbind(temp %>% filter(Heat == 6) %>% mutate(n = estimate_n(absDen_6h)),
                     temp %>% filter(Heat == 12) %>% mutate(n = estimate_n(absDen_12h)),
                     temp %>% filter(Heat == 24) %>% mutate(n = estimate_n(absDen_24h)),
                     temp %>% filter(Heat == 48) %>% mutate(n = estimate_n(absDen_48h)))
rm(temp)

# estimate the SD from the SE
productivity_effects <- productivity_effects %>% mutate(SD = SE * sqrt(n)) %>%
    # re-order by Heat and Trtmt_Day
                          arrange(Heat, Trtmt_Day)

# all pairwise combinations of comparisons between the same treatment day for different durations
temp <- t(combn(c(1,4,7,10), 2))
combos <- rbind(temp, temp+1, temp+2)
rm(temp)
# loop through all the combinations and do the t-tests
prodEffects_ttests <- data.frame()
for(i in 1:nrow(combos)){
  prodEffects_ttests <- rbind(prodEffects_ttests,
                             run_ttest(row_x = combos[i,1],
                                       row_y = combos[i,2],
                                       summary_stats_df = productivity_effects))
}
prodEffects_ttests$adjusted_p <- p.adjust(prodEffects_ttests$pvalue, method = "bonferroni")
prodEffects_ttests$Trtmt_Day <- productivity_effects$Trtmt_Day[combos[,1]]
prodEffects_ttests$Heat_1 <- productivity_effects$Heat[combos[,1]]
prodEffects_ttests$Heat_2 <- productivity_effects$Heat[combos[,2]]

print(prodEffects_ttests)
# these p-values seem overly optimistic. Use alpha = 1*10^-3
```

Awesome! The effect sizes of the simpler model are indeed consistent with those from the complex model. We will focus on the parameter estimates and effect sizes from the simpler model.

### Decoupling marginal on protegens


```{r, decoupling}
decoupling_productivity <- productivity_effects %>% select(-loCI, -hiCI)
# keep just the Bonferroni-corrected (wider) confidence intervals
decoupling_productivity <- decoupling_productivity %>%
                              rename(loCI = loCI_bonAlpha,
                                     hiCI = hiCI_bonAlpha)
# for easier coding, rename the levels of Trtmt_Day
levels(decoupling_productivity$Trtmt_Day) <- c("resist", "early_recov", "late_recov")

# create data.frame for plotting
decoupling_productivity <- decoupling_productivity %>%
                            select(-n, -SD) %>%
                              pivot_wider(names_from = Trtmt_Day,
                                          values_from = c(est, loCI, hiCI, SE, groups))

# columns that indicate if resistance is significantly different from recovery
decoupling_productivity$early_recov_VS_resist <- mapply(are_groups_different,
                                                        decoupling_productivity$groups_early_recov,
                                                        decoupling_productivity$groups_resist)
decoupling_productivity$late_recov_VS_resist <- mapply(are_groups_different,
                                                       decoupling_productivity$groups_late_recov,
                                                       decoupling_productivity$groups_resist)
# clean up extra columns
decoupling_productivity <- decoupling_productivity %>% select(-groups_resist, -groups_early_recov, -groups_late_recov)


# first plot the decoupling on early recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
  #facet_grid(~CommRich) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_early_recov, ymax = hiCI_early_recov), width=0) +
  # center the plot on 0,0:
  scale_x_continuous(limits = c(-0.008, 0.008), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.0045, 0.0045), expand = c(0, 0)) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (with extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Early Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# here's another way to plot it where the confidence intervals are shown as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_early_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_early_recov - est_early_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (with extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Early Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")


# next plot the decoupling on later recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_late_recov, ymax = hiCI_late_recov), width=0) +
  # center the plot on 0,0:
  scale_x_continuous(limits = c(-0.008, 0.008), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.0036, 0.0036), expand = c(0, 0)) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (with extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Late Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# late recovery with CI plotted as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_late_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_late_recov - est_late_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (with extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Late Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# finally estimate decoupling by getting the distance to the y=x line
# calculate decoupling between resistance and early recovery
early_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_early_recov,
                                  recov_hiCI = hiCI_early_recov)))
# add annotation
early_decoupling <- cbind(decoupling_productivity[,1:2],
                          early_decoupling)

ggplot(early_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling)) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  labs(title = "Early recovery (WITH extinct reps)",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")


# calculate decoupling between resistance and late recovery
late_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_late_recov,
                                  recov_hiCI = hiCI_late_recov)))
# add annotation
late_decoupling <- cbind(decoupling_productivity[,1:2],
                          late_decoupling)

ggplot(late_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling)) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  labs(title = "Late recovery (WITH extinct reps)",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")

# clean up
rm(absDen_6h, absDen_12h, absDen_24h, absDen_48h, absDen_mods6h, absDen_mods12h, absDen_mods24h, absDen_mods48h,
   combos, decoupling_productivity, early_decoupling, late_decoupling,
   effect_6h, effect_12h, effect_24h, effect_48h, effect_6h_protegens, effect_12h_protegens, effect_24h_protegens, effect_48h_protegens,
   posthoc_6h, posthoc_12h, posthoc_24h, posthoc_48h, posthoc_6h_WIDER, posthoc_12h_WIDER, posthoc_24h_WIDER, posthoc_48h_WIDER, posthocPROT_6h, posthocPROT_12h, posthocPROT_24h, posthocPROT_48h, posthocPROT_6h_WIDER, posthocPROT_12h_WIDER, posthocPROT_24h_WIDER, posthocPROT_48h_WIDER,
   prod_effects_protegens, prodEffects_ttests, productivity_effects, productivity_protegens)
```

## Repeat analysis removing extinct replicates

Let's remove the extinct replicates to focus just on the data where the communities survived.

```{r, prod_NOextinct_subset&modelfit}
# add a column indicating whether the replicate survived
  # but first we need to remove $Heat because it's a factor for diversity but numeric for extinctions and cannot be *_joined
tmp_div <- absDen_forFit %>% select(-Heat)
tmp_div <- inner_join(tmp_div,
                      extinct.df %>% select(uniqID, survived),
                      by = c("uniqID"))
absDen_forFit$survived <- tmp_div$survived
rm(tmp_div)

# keep just the diversity values that did not go extinct
absDen_forFit <- absDen_forFit %>% filter(survived == 1)

####################
# 6h heat duration
####################
# grab just the treatment with its associated control data
absDen_6h <- rbind(absDen_forFit %>% filter(Heat == "6"),
                   absDen_forFit %>% filter(Heat == "control", Day < 4))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_6h$Trtmt_Day <- "resist"
absDen_6h$Trtmt_Day[absDen_6h$Day == 2] <- "recov_1"
absDen_6h$Trtmt_Day[absDen_6h$Day == 3] <- "recov_2"
# appropriately distinguish between numbers and factors
absDen_6h$Trtmt_Day <- as.factor(absDen_6h$Trtmt_Day)
absDen_6h$Heat <- droplevels(absDen_6h$Heat)
absDen_6h$resistant <- as.factor(absDen_6h$resistant)
absDen_6h$protegens <- as.factor(absDen_6h$protegens)

# fit different models:
absDen_mods6h <- fit_productivity_models(absDen_6h)

# check the simplest possible models for multicolinearity
check_collinearity(absDen_mods6h[["simple"]])
check_collinearity(absDen_mods6h[["simple resist"]])

# print a summary table of the model fits
data.frame(pars = sapply(absDen_mods6h, npar_of_glmmTMB_fit),
                           AIC = sapply(absDen_mods6h, AIC),
                           AICc = sapply(absDen_mods6h, AICc),
                           BIC = sapply(absDen_mods6h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# plot the best model for 6h:
print(plot_model_pred.MU(mod_list=absDen_mods6h, mod_name="*mu +prot"))
# plot the best model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods6h, mod_name="*prot*mu +CommRich"))
# plot the preferred model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods6h, mod_name="*prot +mu + prot*CommRich"))


# check the fit and estimates of the best model for the complete data:
#simulateResiduals(fittedModel = absDen_mods6h[["*prot*mu +CommRich"]], plot = TRUE)
#summary(absDen_mods6h[["*prot*mu +CommRich"]])
# check the fit and estimates of the preferred model for the complete data:
simulateResiduals(fittedModel = absDen_mods6h[["*prot*mu +CommRich"]], plot = TRUE)
summary(absDen_mods6h[["*prot +mu + prot*CommRich"]])



####################
# 12h heat duration
####################
# grab just the treatment with its associated control data
absDen_12h <- rbind(absDen_forFit %>% filter(Heat == "12", Day > 1),
                       absDen_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_12h$Trtmt_Day <- "resist"
absDen_12h$Trtmt_Day[absDen_12h$Day == 3] <- "recov_1"
absDen_12h$Trtmt_Day[absDen_12h$Day == 4] <- "recov_2"
# appropriately distinguish between numbers and factors
absDen_12h$Trtmt_Day <- as.factor(absDen_12h$Trtmt_Day)
absDen_12h$Heat <- droplevels(absDen_12h$Heat)
absDen_12h$resistant <- as.factor(absDen_12h$resistant)
absDen_12h$protegens <- as.factor(absDen_12h$protegens)

# fit different models:
absDen_mods12h <- fit_productivity_models(absDen_12h)

# check the simplest possible models for multicolinearity
check_collinearity(absDen_mods12h[["simple"]])
check_collinearity(absDen_mods12h[["simple resist"]])

# print a summary table of the model fits
data.frame(pars = sapply(absDen_mods12h, npar_of_glmmTMB_fit),
                           AIC = sapply(absDen_mods12h, AIC),
                           AICc = sapply(absDen_mods12h, AICc),
                           BIC = sapply(absDen_mods12h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# plot the best model for 12h:
print(plot_model_pred.MU(mod_list=absDen_mods12h, mod_name="*mu +prot"))
# plot the best model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods12h, mod_name="*prot*mu +CommRich"))
# plot the preferred model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods12h, mod_name="*prot +mu + prot*CommRich"))


# check the fit and estimates of the best model for the complete data:
#simulateResiduals(fittedModel = absDen_mods12h[["*prot*mu +CommRich"]], plot = TRUE)
#summary(absDen_mods12h[["*prot*mu +CommRich"]])
# check the fit and estimates of the preferred model for the complete data:
simulateResiduals(fittedModel = absDen_mods12h[["*prot*mu +CommRich"]], plot = TRUE)
summary(absDen_mods12h[["*prot +mu + prot*CommRich"]])



####################
# 24h heat duration
####################
# grab just the treatment with its associated control data
absDen_24h <- rbind(absDen_forFit %>% filter(Heat == "24", Day > 1),
                       absDen_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_24h$Trtmt_Day <- "resist"
absDen_24h$Trtmt_Day[absDen_24h$Day == 3] <- "recov_1"
absDen_24h$Trtmt_Day[absDen_24h$Day == 4] <- "recov_2"
# appropriately distinguish between numbers and factors
absDen_24h$Trtmt_Day <- as.factor(absDen_24h$Trtmt_Day)
absDen_24h$Heat <- droplevels(absDen_24h$Heat)
absDen_24h$resistant <- as.factor(absDen_24h$resistant)
absDen_24h$protegens <- as.factor(absDen_24h$protegens)

# fit different models:
absDen_mods24h <- fit_productivity_models(absDen_24h)

# check the simplest possible models for multicolinearity
check_collinearity(absDen_mods24h[["simple"]])
check_collinearity(absDen_mods24h[["simple resist"]])

# print a summary table of the model fits
data.frame(pars = sapply(absDen_mods24h, npar_of_glmmTMB_fit),
                           AIC = sapply(absDen_mods24h, AIC),
                           AICc = sapply(absDen_mods24h, AICc),
                           BIC = sapply(absDen_mods24h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)

# plot the best model for the 24h data without extinctions:
print(plot_model_pred.CommRich(mod_list=absDen_mods24h, mod_name="*prot*mu +CommRich"))
# plot the best model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods24h, mod_name="*prot*mu +CommRich"))
# plot the preferred model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods24h, mod_name="*prot +mu + prot*CommRich"))


# check the fit and estimates of the best model for the complete data:
#simulateResiduals(fittedModel = absDen_mods24h[["*prot*mu +CommRich"]], plot = TRUE)
#summary(absDen_mods24h[["*prot*mu +CommRich"]])
# check the fit and estimates of the preferred model for the complete data:
simulateResiduals(fittedModel = absDen_mods24h[["*prot +mu + prot*CommRich"]], plot = TRUE)
summary(absDen_mods24h[["*prot +mu + prot*CommRich"]])


####################
# 48h heat duration
####################
# grab just the treatment with its associated control data
absDen_48h <- rbind(absDen_forFit %>% filter(Heat == "48", Day > 2),
                       absDen_forFit %>% filter(Heat == "control", Day > 2))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_48h$Trtmt_Day <- "resist"
absDen_48h$Trtmt_Day[absDen_48h$Day == 4] <- "recov_1"
absDen_48h$Trtmt_Day[absDen_48h$Day == 5] <- "recov_2"
# appropriately distinguish between numbers and factors
absDen_48h$Trtmt_Day <- as.factor(absDen_48h$Trtmt_Day)
absDen_48h$Heat <- droplevels(absDen_48h$Heat)
absDen_48h$resistant <- as.factor(absDen_48h$resistant)
absDen_48h$protegens <- as.factor(absDen_48h$protegens)

# fit different models:
absDen_mods48h <- fit_productivity_models(absDen_48h)

# check the simplest possible models for multicolinearity
check_collinearity(absDen_mods48h[["simple"]])
check_collinearity(absDen_mods48h[["simple resist"]])

# print a summary table of the model fits
data.frame(pars = sapply(absDen_mods48h, npar_of_glmmTMB_fit),
                           AIC = sapply(absDen_mods48h, AIC),
                           AICc = sapply(absDen_mods48h, AICc),
                           BIC = sapply(absDen_mods48h, BIC)) %>%
                    mutate(dAIC = min(AIC)-AIC,
                           dAICc = min(AICc)-AICc,
                           dBIC = min(BIC)-BIC) %>% arrange(BIC)


# plot the best model for 48h:
print(plot_model_pred.CommRich(mod_list=absDen_mods48h, mod_name="+CommRich +prot"))
# plot the best model for the complete data:
print(plot_model_pred.MU(mod_list=absDen_mods48h, mod_name="*prot*mu +CommRich"))
# plot the preferred model for the complete data:
print(plot_model_pred.CommRich(mod_list=absDen_mods48h, mod_name="*prot +mu + prot*CommRich"))

# check the fit and estimates of the best model for the complete data:
#simulateResiduals(fittedModel = absDen_mods48h[["*prot*mu +CommRich"]], plot = TRUE)
#summary(absDen_mods48h[["*prot*mu +CommRich"]])
# check the fit and estimates of the preferred model for the complete data:
simulateResiduals(fittedModel = absDen_mods48h[["*prot +mu + prot*CommRich"]], plot = TRUE)
summary(absDen_mods48h[["*prot +mu + prot*CommRich"]])


####################
# Select the best model across all data subsets
####################
# for each information criterion, get the average across all data subsets
meanIC <- data.frame(pars = sapply(absDen_mods48h, npar_of_glmmTMB_fit),
                     AIC = sapply(absDen_mods6h, AIC) + sapply(absDen_mods12h, AIC) + sapply(absDen_mods24h, AIC) + sapply(absDen_mods48h, AIC),
                     AICc = sapply(absDen_mods6h, AICc) + sapply(absDen_mods12h, AICc) + sapply(absDen_mods24h, AICc) + sapply(absDen_mods48h, AICc),
                     BIC = sapply(absDen_mods6h, BIC) + sapply(absDen_mods12h, BIC) + sapply(absDen_mods24h, BIC) + sapply(absDen_mods48h, BIC)) %>%
            mutate(AIC = AIC/4,
                   AICc = AICc/4,
                   BIC = BIC/4) %>%
              mutate(dAIC = min(AIC)-AIC,
                     dAICc = min(AICc)-AICc,
                     dBIC = min(BIC)-BIC)
meanIC %>% arrange(BIC)

meanIC %>% arrange(AIC)

# clean up
rm(meanIC)
```

Annoyingly enough, the best model *as well as* the preferred model are changed now that we consider the data without any extinctions...

Welp, I'm going to still use the "\*prot +mu + prot\*CommRich" model. There's a different result now for late recovery at 48h: a positive effect of heat duration. When I tried to do the downstream analysis with the model preferred for the extinction data, "", the results were the same. So this seems to be describing a trend that's present in the data and not an issue with the model selection itself.

### Calculate effect sizes conditional on protegens (No extinctions)

```{r, prod_NOextinct_effectSize_protegens}
# plot the effect size contingent on protegens
  # use the same model as above, "*prot +mu + prot*CommRich"
effect_6h_protegens <- eff_size(emmeans(absDen_mods6h[["*prot +mu + prot*CommRich"]], ~ Heat | Trtmt_Day*protegens + community_expected_mu + protegens*CommRich, data = absDen_6h),
                                sigma(absDen_mods6h[["*prot +mu + prot*CommRich"]]),
                                edf = df.residual(absDen_mods6h[["*prot +mu + prot*CommRich"]]))
effect_12h_protegens <- eff_size(emmeans(absDen_mods12h[["*prot +mu + prot*CommRich"]], ~ Heat | Trtmt_Day*protegens + community_expected_mu + protegens*CommRich, data = absDen_12h),
                                sigma(absDen_mods12h[["*prot +mu + prot*CommRich"]]),
                                edf = df.residual(absDen_mods12h[["*prot +mu + prot*CommRich"]]))
effect_24h_protegens <- eff_size(emmeans(absDen_mods24h[["*prot +mu + prot*CommRich"]], ~ Heat | Trtmt_Day*protegens + community_expected_mu + protegens*CommRich, data = absDen_24h),
                                sigma(absDen_mods24h[["*prot +mu + prot*CommRich"]]),
                                edf = df.residual(absDen_mods24h[["*prot +mu + prot*CommRich"]]))
effect_48h_protegens <- eff_size(emmeans(absDen_mods48h[["*prot +mu + prot*CommRich"]], ~ Heat | Trtmt_Day*protegens + community_expected_mu + protegens*CommRich, data = absDen_48h),
                                sigma(absDen_mods48h[["*prot +mu + prot*CommRich"]]),
                                edf = df.residual(absDen_mods48h[["*prot +mu + prot*CommRich"]]))


# use the overall preferred model for the data excluding extinctions:
#effect_6h_protegens <- eff_size(emmeans(absDen_mods6h[["*prot +CommRich"]], ~ Heat | Trtmt_Day*protegens + CommRich, data = absDen_6h),
#                                sigma(absDen_mods6h[["*prot +CommRich"]]),
#                                edf = df.residual(absDen_mods6h[["*prot +CommRich"]]))
#effect_12h_protegens <- eff_size(emmeans(absDen_mods12h[["*prot +CommRich"]], ~ Heat | #Trtmt_Day*protegens + CommRich, data = absDen_12h),
#                                sigma(absDen_mods12h[["*prot +CommRich"]]),
#                                edf = df.residual(absDen_mods12h[["*prot +CommRich"]]))
#effect_24h_protegens <- eff_size(emmeans(absDen_mods24h[["*prot +CommRich"]], ~ Heat | #Trtmt_Day*protegens + CommRich, data = absDen_24h),
#                                sigma(absDen_mods24h[["*prot +CommRich"]]),
#                                edf = df.residual(absDen_mods24h[["*prot +CommRich"]]))
#effect_48h_protegens <- eff_size(emmeans(absDen_mods48h[["*prot +CommRich"]], ~ Heat | Trtmt_Day*protegens + CommRich, data = absDen_48h),
#                                sigma(absDen_mods48h[["*prot +CommRich"]]),
#                                edf = df.residual(absDen_mods48h[["*prot +CommRich"]]))

# a function that extracts the confidence intervals from eff_size contingent on protegens
get_effsize_CIs <- function(eff_size_object, heat_trtmt) {
  data.frame(Heat = heat_trtmt,
             # this function will fail with "*prot +CommRich"
             # so you need to modify the code as indicated in the commented out bits:
             CommRich = confint(eff_size_object)[[5]], #[[4]],
             Trtmt_Day = confint(eff_size_object)[[2]], #[[2]],
             protegens = confint(eff_size_object)[[3]],
             community_expected_mu = confint(eff_size_object)[[4]], # this whole line needs to be removed
             effect_est = confint(eff_size_object)[[5]],
             effect_loCI = confint(eff_size_object)[[8]],
             effect_hiCI = confint(eff_size_object)[[9]])
}

# create a data.frame for plotting marginal effect sizes using a forest plot
productivity_protegens <- data.frame()
productivity_protegens <- rbind(productivity_protegens,
                              get_effsize_CIs(effect_6h_protegens, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h_protegens, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h_protegens, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h_protegens, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_protegens$Trtmt_Day <- factor(productivity_protegens$Trtmt_Day,
                                         levels = c("resist", "recov_1", "recov_2"))
levels(productivity_protegens$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

#plot
ggplot(productivity_protegens,
       aes(x = effect_est, y = as.factor(Heat), colour = Trtmt_Day, shape = protegens)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = effect_loCI, xmax = effect_hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Heat duration (hrs)",
       shape = "protegens\npresent?",
       title = "*prot +mu + prot*CommRich")

# we can do a posthoc on this to illustrate statistically significant effects
posthocPROT_6h <- emmeans(effect_6h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_6h)
posthocPROT_12h <- emmeans(effect_12h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_12h)
posthocPROT_24h <- emmeans(effect_24h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_24h)
posthocPROT_48h <- emmeans(effect_48h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_48h)

# create a data.frame for plotting
prod_effects_protegens <- data.frame()
prod_effects_protegens <- rbind(prod_effects_protegens,
                               get_posthoc_YESprot(posthocPROT_6h, heat_trtmt = 6),
                               get_posthoc_YESprot(posthocPROT_12h, heat_trtmt = 12),
                               get_posthoc_YESprot(posthocPROT_24h, heat_trtmt = 24),
                               get_posthoc_YESprot(posthocPROT_48h, heat_trtmt = 48))

## note that for the decoupling plots I am using a Bonferroni-corrected alpha.
# let's get those confidence intervals because we will need them to plot decoupling below:
posthocPROT_6h_WIDER <- emmeans(effect_6h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_6h, level=0.9875)
posthocPROT_12h_WIDER <- emmeans(effect_12h_protegens, pairwise ~ Trtmt_Day*protegens, data = posthocPROT_12h, level=0.9875)
posthocPROT_24h_WIDER <- emmeans(effect_24h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_24h, level=0.9875)
posthocPROT_48h_WIDER <- emmeans(effect_48h_protegens, pairwise ~ Trtmt_Day*protegens, data = absDen_48h, level=0.9875)
# put these wider CIs into a table
widerCIs <- data.frame()
widerCIs <- rbind(widerCIs,
                  get_posthoc_YESprot(posthocPROT_6h_WIDER, heat_trtmt = 6),
                  get_posthoc_YESprot(posthocPROT_12h_WIDER, heat_trtmt = 12),
                  get_posthoc_YESprot(posthocPROT_24h_WIDER, heat_trtmt = 24),
                  get_posthoc_YESprot(posthocPROT_48h_WIDER, heat_trtmt = 48))
# rename the columns to remind us that this is the Bonferroni corrected alpha
colnames(widerCIs)[5:6] <- c("loCI_bonAlpha", "hiCI_bonAlpha")
# combine the wider CIs with the effect sizes
prod_effects_protegens <- inner_join(prod_effects_protegens,
                                     widerCIs %>% select(-SE))
rm(widerCIs)


# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
prod_effects_protegens$Trtmt_Day <- factor(prod_effects_protegens$Trtmt_Day,
                                          levels = c("resist", "recov_1", "recov_2"))
levels(prod_effects_protegens$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot with group labels
ggplot(prod_effects_protegens,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day, shape=protegens)) +
  facet_grid( ~ protegens) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-0.0035, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y="Heat duration",
       shape = "protegens\npresent?",
       title = "*prot +mu + prot*CommRich")
```


#### Decoupling conditional on protegens (No extinctions)

```{r, decoupling_NOextinct_protegens}
decoupling_productivity <- prod_effects_protegens %>% select(-loCI, -hiCI)
# keep just the Bonferroni-corrected (wider) confidence intervals
decoupling_productivity <- decoupling_productivity %>%
                              rename(loCI = loCI_bonAlpha,
                                     hiCI = hiCI_bonAlpha)
# for easier coding, rename the levels of Trtmt_Day
levels(decoupling_productivity$Trtmt_Day) <- c("resist", "early_recov", "late_recov")

# create data.frame for plotting
decoupling_productivity <- decoupling_productivity %>%
                              pivot_wider(names_from = Trtmt_Day,
                                          values_from = c(est, loCI, hiCI, SE, groups))

# columns that indicate if resistance is significantly different from recovery
decoupling_productivity$early_recov_VS_resist <- mapply(are_groups_different,
                                                        decoupling_productivity$groups_early_recov,
                                                        decoupling_productivity$groups_resist)
decoupling_productivity$late_recov_VS_resist <- mapply(are_groups_different,
                                                       decoupling_productivity$groups_late_recov,
                                                       decoupling_productivity$groups_resist)
# clean up extra columns
decoupling_productivity <- decoupling_productivity %>% select(-groups_resist, -groups_early_recov, -groups_late_recov)


# first plot the decoupling on early recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
  facet_grid(~protegens) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_early_recov, ymax = hiCI_early_recov), width=0) +
  # center the plot on 0,0:
  scale_x_continuous(limits = c(-0.025, 0.025), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.0025, 0.0025), expand = c(0, 0)) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (NO extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Early Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# here's another way to plot it where the confidence intervals are shown as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
    facet_grid(~protegens) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_early_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_early_recov - est_early_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (NO extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Early Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")


# next plot the decoupling on later recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
  facet_grid(~protegens) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_late_recov, ymax = hiCI_late_recov), width=0) +
  # center the plot on 0,0:
  scale_x_continuous(limits = c(-0.025, 0.025), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.005, 0.005), expand = c(0, 0)) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (NO extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Late Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# late recovery with CI plotted as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
    facet_grid(~protegens) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_late_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_late_recov - est_late_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (NO extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Late Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# finally estimate decoupling by getting the distance to the y=x line
# calculate decoupling between resistance and early recovery
early_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_early_recov,
                                  recov_hiCI = hiCI_early_recov)))
# add annotation
early_decoupling <- cbind(decoupling_productivity[,1:2],
                          early_decoupling)

ggplot(early_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling)) +
  facet_grid(~protegens) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  labs(title = "Early recovery (NO extinct reps)",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")


# calculate decoupling between resistance and late recovery
late_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_late_recov,
                                  recov_hiCI = hiCI_late_recov)))
# add annotation
late_decoupling <- cbind(decoupling_productivity[,1:2],
                          late_decoupling)

ggplot(late_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling)) +
  facet_grid(~protegens) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  labs(title = "Late recovery (NO extinct reps)",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")
```


### Calculate the effect sizes marginal on protegens (No extinctions)


```{r, prod_NOextinct_effectSize}
##############################
# effect sizes with protegens as non-focal
##############################
# But we are not interested in the details of protegens. Let's do the post-hoc again now averaging across the effects of protegens.

posthoc_6h <- emmeans(effect_6h_protegens, pairwise ~ Trtmt_Day, data = absDen_6h)
posthoc_12h <- emmeans(effect_12h_protegens, pairwise ~ Trtmt_Day, data = absDen_12h)
posthoc_24h <- emmeans(effect_24h_protegens, pairwise ~ Trtmt_Day, data = absDen_24h)
posthoc_48h <- emmeans(effect_48h_protegens, pairwise ~ Trtmt_Day, data = absDen_48h)

# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
productivity_effects <- data.frame()
productivity_effects <- rbind(productivity_effects,
                              get_posthoc_NOprot(posthoc_6h, heat_trtmt = 6),
                              get_posthoc_NOprot(posthoc_12h, heat_trtmt = 12),
                              get_posthoc_NOprot(posthoc_24h, heat_trtmt = 24),
                              get_posthoc_NOprot(posthoc_48h, heat_trtmt = 48))


## note that for the decoupling plots I am using a Bonferroni-corrected alpha.
# let's get those confidence intervals because we will need them to plot decoupling below:
posthoc_6h_WIDER <- emmeans(effect_6h_protegens, pairwise ~ Trtmt_Day, data = absDen_6h, level=0.9875)
posthoc_12h_WIDER <- emmeans(effect_12h_protegens, pairwise ~ Trtmt_Day, data = absDen_12h, level=0.9875)
posthoc_24h_WIDER <- emmeans(effect_24h_protegens, pairwise ~ Trtmt_Day, data = absDen_24h, level=0.9875)
posthoc_48h_WIDER <- emmeans(effect_48h_protegens, pairwise ~ Trtmt_Day, data = absDen_48h, level=0.9875)
# put these wider CIs into a table
widerCIs <- data.frame()
widerCIs <- rbind(widerCIs,
                  get_posthoc_NOprot(posthoc_6h_WIDER, heat_trtmt = 6),
                  get_posthoc_NOprot(posthoc_12h_WIDER, heat_trtmt = 12),
                  get_posthoc_NOprot(posthoc_24h_WIDER, heat_trtmt = 24),
                  get_posthoc_NOprot(posthoc_48h_WIDER, heat_trtmt = 48))
# rename the columns to remind us that this is the Bonferroni corrected alpha
colnames(widerCIs)[4:5] <- c("loCI_bonAlpha", "hiCI_bonAlpha")
# combine the wider CIs with the effect sizes
productivity_effects <- inner_join(productivity_effects,
                                   widerCIs %>% select(-SE))
rm(widerCIs)

# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_effects$Trtmt_Day <- factor(productivity_effects$Trtmt_Day,
                                         levels = c("resist", "recov_1", "recov_2"))
levels(productivity_effects$Trtmt_Day) <- c("Resistance", "Early Recovery", "Late Recovery")

# plot with group labels
ggplot(productivity_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-0.008, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Heat duration (hrs)",
       title = "protegens as non-focal predictor (i.e., marginalized)")


#######
# finally, we will do a series of pairwise two-tailed t-tests to compare between heat durations
#######

# estimate the sample sizes
temp <- productivity_effects # copy the effects to temp
productivity_effects <- rbind(temp %>% filter(Heat == 6) %>% mutate(n = estimate_n(absDen_6h)),
                     temp %>% filter(Heat == 12) %>% mutate(n = estimate_n(absDen_12h)),
                     temp %>% filter(Heat == 24) %>% mutate(n = estimate_n(absDen_24h)),
                     temp %>% filter(Heat == 48) %>% mutate(n = estimate_n(absDen_48h)))
rm(temp)

# estimate the SD from the SE
productivity_effects <- productivity_effects %>% mutate(SD = SE * sqrt(n)) %>%
    # re-order by Heat and Trtmt_Day
                          arrange(Heat, Trtmt_Day)

# all pairwise combinations of comparisons between the same treatment day for different durations
temp <- t(combn(c(1,4,7,10), 2))
combos <- rbind(temp, temp+1, temp+2)
rm(temp)
# loop through all the combinations and do the t-tests
prodEffects_ttests <- data.frame()
for(i in 1:nrow(combos)){
  prodEffects_ttests <- rbind(prodEffects_ttests,
                             run_ttest(row_x = combos[i,1],
                                       row_y = combos[i,2],
                                       summary_stats_df = productivity_effects))
}
prodEffects_ttests$adjusted_p <- p.adjust(prodEffects_ttests$pvalue, method = "bonferroni")
prodEffects_ttests$Trtmt_Day <- productivity_effects$Trtmt_Day[combos[,1]]
prodEffects_ttests$Heat_1 <- productivity_effects$Heat[combos[,1]]
prodEffects_ttests$Heat_2 <- productivity_effects$Heat[combos[,2]]

print(prodEffects_ttests)
# these p-values seem overly optimistic. Use alpha = 1*10^-3


################################
# Plot figure for main text: Figure 4b
################################
png(filename="./figures/Fig4B_plot.png", width = 4.48, height = 2.61, units = "in", res=300)
ggplot(productivity_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.15) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Productivity",
       y="Heat Duration (hrs)",
       colour = "Treatment\nDay") +
  theme(legend.position="none")
dev.off()
```


#### Decoupling marginal on protegens (No extinctions)

```{r, decoupling_NOextinct}
decoupling_productivity <- productivity_effects %>% select(-loCI, -hiCI)
# keep just the Bonferroni-corrected (wider) confidence intervals
decoupling_productivity <- decoupling_productivity %>%
                              rename(loCI = loCI_bonAlpha,
                                     hiCI = hiCI_bonAlpha)
# for easier coding, rename the levels of Trtmt_Day
levels(decoupling_productivity$Trtmt_Day) <- c("resist", "early_recov", "late_recov")

# create data.frame for plotting
decoupling_productivity <- decoupling_productivity %>%
                            select(-n, -SD) %>%
                              pivot_wider(names_from = Trtmt_Day,
                                          values_from = c(est, loCI, hiCI, SE, groups))

# columns that indicate if resistance is significantly different from recovery
decoupling_productivity$early_recov_VS_resist <- mapply(are_groups_different,
                                                        decoupling_productivity$groups_early_recov,
                                                        decoupling_productivity$groups_resist)
decoupling_productivity$late_recov_VS_resist <- mapply(are_groups_different,
                                                       decoupling_productivity$groups_late_recov,
                                                       decoupling_productivity$groups_resist)
# clean up extra columns
decoupling_productivity <- decoupling_productivity %>% select(-groups_resist, -groups_early_recov, -groups_late_recov)


# first plot the decoupling on early recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
  #facet_grid(~CommRich) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_early_recov, ymax = hiCI_early_recov), width=0) +
  # center the plot on 0,0:
  scale_x_continuous(limits = c(-0.019, 0.019), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-0.0014, 0.0014), expand = c(0, 0)) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (NO extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Early Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# here's another way to plot it where the confidence intervals are shown as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_early_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_early_recov - est_early_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (NO extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Early Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")


# next plot the decoupling on later recovery
fig5a <- ggplot(decoupling_productivity,
            aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
          geom_hline(yintercept = 0, colour="grey") +
          geom_vline(xintercept = 0, colour="grey") +
          geom_abline(slope = 1) +
          geom_point(shape=21, size=2, aes(fill=as.factor(late_recov_VS_resist))) +
          geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
          geom_errorbar(aes(ymin = loCI_late_recov, ymax = hiCI_late_recov), width=0) +
          # center the plot on 0,0:
          scale_x_continuous(limits = c(-0.019, 0.019), expand = c(0, 0)) +
          scale_y_continuous(limits = c(-0.003, 0.003), expand = c(0, 0)) +
          scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
          scale_fill_manual(values=c("white", "black")) +
          labs(x = "Resistance",
               y = "Late Recovery",
               colour = "Heat\nDuration",
               fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

print(fig5a + labs(title = "Decoupling of productivity (NO extinct reps)"))

################################
# Plot figure for main text: Figure 5a
################################
png(filename="./figures/Fig5A.png", width = 6.25, height = 3.68, units = "in", res=300)
print(fig5a)
dev.off()

# late recovery with CI plotted as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_late_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_late_recov - est_late_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (NO extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Late Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# finally estimate decoupling by getting the distance to the y=x line
# calculate decoupling between resistance and early recovery
early_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_early_recov,
                                  recov_hiCI = hiCI_early_recov)))
# add annotation
early_decoupling <- cbind(decoupling_productivity[,1:2],
                          early_decoupling)

ggplot(early_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling)) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  labs(title = "Early recovery (NO extinct reps)",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")


# calculate decoupling between resistance and late recovery
late_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_late_recov,
                                  recov_hiCI = hiCI_late_recov)))
# add annotation
late_decoupling <- cbind(decoupling_productivity[,c(1:2, 15)],
                          late_decoupling)

# plot for main text:
fig5b <- ggplot(late_decoupling,
                aes(x = as.factor(Heat), y = est_decoupling)) +
          geom_hline(yintercept = 0, colour = "grey") +
          geom_point(position = position_dodge(width = 0.5)) +
          geom_errorbar(aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                        width=0.05) +
          labs(y = "Decoupling",
               x = "Heat Duration (hrs)")
print(fig5b + labs(title = "Late recovery (NO extinct reps)"))

################################
# Plot figure for main text: Figure 5b
################################
png(filename="./figures/Fig5B.png", width = 4.7, height = 2.0, units = "in", res=300)
print(fig5b)
dev.off()


# clean up
rm(absDen_6h, absDen_12h, absDen_24h, absDen_48h, absDen_mods6h, absDen_mods12h, absDen_mods24h, absDen_mods48h,
   combos, decoupling_productivity, early_decoupling, late_decoupling,
   effect_6h, effect_12h, effect_24h, effect_48h, effect_6h_protegens, effect_12h_protegens, effect_24h_protegens, effect_48h_protegens,
   posthoc_6h, posthoc_12h, posthoc_24h, posthoc_48h, posthoc_6h_WIDER, posthoc_12h_WIDER, posthoc_24h_WIDER, posthoc_48h_WIDER, posthocPROT_6h, posthocPROT_12h, posthocPROT_24h, posthocPROT_48h, posthocPROT_6h_WIDER, posthocPROT_12h_WIDER, posthocPROT_24h_WIDER, posthocPROT_48h_WIDER,
   prod_effects_protegens, prodEffects_ttests, productivity_effects, productivity_protegens,
 fig5a, fig5b)
```

