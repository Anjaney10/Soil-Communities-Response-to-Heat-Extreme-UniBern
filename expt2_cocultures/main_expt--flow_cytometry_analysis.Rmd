---
title: "Resistance & recovery 4-sp communities: Analyze flow cytometry data"
author: "Hermina"
Date: "2025-02-19"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_float:
      smooth_scroll: false
  pdf_document:
    toc: true
editor_options: 
  markdown: 
    wrap: 90
---

# Introduction

This document is updated from `analyze_temp_serial_transfer_expt--28Oct24.Rmd`.

There are 5 treatments: no heat (5 days serial transfer), 6h heat, 12h heat, 24h heat, and
48h heat. Each of these is setup with 5 technical replicates that was initially inoculated
to about equal ratios (on Day 0).

Summary of choices: Based on the number of cells observed in true blank wells, I only
include data from wells with \>50 cells. Based on the flow cytometry data from Day 0 (which is estimated from the blanks in the OD data to have never experienced any contamination events), a
misclassification rate of 1% is assumed. Contaminated replicates are
defined as having a substantially higher % of a species that was not inoculated in that well (i.e., than expected from this misclassification rate). When a contaminated well was detected, all time points associated with that well were removed from the data.

# Load & Annotate Data

After loading the environment, I will load all of the flow cytometry cell count data and information about extinct wells from the OD data. Data is also annotated.

```{r, loadEnv}
library(tidyverse)
library(readxl) # for importing data directly from Excel sheet
library(RColorBrewer) # for changing the colours of plots
library(ggbeeswarm) # for beeswarm plots
library(vegan) # to estimate diversity and for ordination (NMDS)
library(ggordiplots) # for ggplotting ellipses around treatment group centroids during ordination
library(chemodiv) # for estimating species richness
#library(lme4)  <-- not sure this is needed?
library(glmmTMB) # for fitting and trouble-shooting GLM's
library(DHARMa) # for plotting the residuals when using glmmTMB
library(rcompanion) # for r-squared estimates of GLM's
library(effsize) # for post-hoc estimate of effect sizes
library(emmeans) # (ditto as above)
library(BSDA) # for pairwise t-tests to compare effect sizes between data subsets
#library(partitionBEFsp) # for paritioning the biodiversity effects
#library(ape) # for ordination ??
library(ggforce) # for plotting ellipses in ggplot

# print the complete info about packages and versions currently loaded in the environment:
sessionInfo()

# set theme for all plots
fave_theme <- theme_light() + # see other options at https://ggplot2.tidyverse.org/reference/ggtheme.html
              theme(text = element_text(size=15), # larger text size for titles & axes
                    panel.grid.major = element_blank(), # remove major gridlines
                    panel.grid.minor = element_blank()) # remove minor gridlines
theme_set(fave_theme)

# define a palette for plotting the 4 species
species_4pal_alphabetical = palette.colors(8, palette = "R4")[c(3, 5, 7, 2)] #in alphabetical order
species_4pal_speed = palette.colors(8, palette = "R4")[c(7, 5, 3, 2)] #from fast to slow

#### THIS NEEDS TO BE UPDATED BELOW!!!
# define a palette for plotting the 3 treatment days
trtmt_pal = brewer.pal(4, "Set2")[c(1,3:4)]

# define a palette for plotting the heat duration
control_to_48h_pal <- scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9)

# define a palette for plotting the inoculated community richness
CommRich_pal <- scale_colour_viridis_d(option = "viridis", begin=0.2, end=0.95)

# define a function to find the mode of a vector. Credit to https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

Load the cell counts data from the summary Excel files created by FCS Express. Then load
the well volume data from the summary .csv files created by the Attune software.

```{r, loadData}
# a function to load the fluorescent counts data (from .xlsx files created by )
import_flow_counts <- function(file)
  return(as.data.frame(
          read_excel(path=file, sheet="FCS Express Report",
                    # replace the column names as follows:
                    col_names = c("Filename",
                                  "Gate1", "Count_grimontii",
                                  "Gate2", "Count_putida",
                                  "Gate3", "Count_protegens",
                                  "Gate4","Count_veronii")))
  )
# a function to load and parse the volume data
import_flow_volume <- function(file) {
  raw.csv <- read.csv(file)
  # keep the volume info and just enough data to identify the sample. Then remove resultant redundant rows
  vol_data <- raw.csv %>% select(Plate, Sample, Volume) %>% unique()
}

# a function to loop through the folders containing the data files, open the .xlsx and .csv files and combine their data
import_from_files <- function(dir_vector){
  # initiatize variables
  raw_counts <- raw_vols <- data.frame()
  # loop through each directory
  for(dir in dir_vector){
    # get all the file names
    files_v <- list.files(dir)
    
    # identify the excel files
    files_excel <- files_v[endsWith(files_v, ".xlsx")]
    # and loop through all of them to extract their data
    TMPraw_counts <- data.frame()
    for(val in files_excel){
      TMPraw_counts <- rbind(TMPraw_counts, import_flow_counts(paste0(dir, "/", val)))
    }
    
    # identify the csv files
    files_csv <- files_v[endsWith(files_v, ".csv")]
        # and loop through all of them to extract their data
    TMPraw_vols <- data.frame()
    for(val in files_csv){
      TMPraw_vols <- rbind(TMPraw_vols, import_flow_volume(paste0(dir, "/", val)))
    }
    
    # concatenate the data from counts and from vols
    raw_counts <- rbind(raw_counts, TMPraw_counts)
    raw_vols <- rbind(raw_vols, TMPraw_vols)
    rm(TMPraw_counts, TMPraw_vols)
  }
  return(list(raw_counts, raw_vols))
}

# get all of the raw data:
list_rawdata <- import_from_files(c("./data/serial_transf--2July24", "./data/serial_transf--8July24", "./data/serial_transf--5Aug24", "./data/serial_transf--19Aug24"))
```

Now we can process the data to create unique ID's for each sample. This info needs
to be parsed from the Filename column for the flow counts data (i.e., excel files) and
from the Plate column for the flow volumes data (i.e., csv files).

```{r, processData}
# start with flow counts data:
  # I got confused and now there are rows containing the column names. Get rid of those...
list_rawdata[[1]] <- list_rawdata[[1]][-grep("Filename", list_rawdata[[1]]$Filename),]
  # Day0 has a different pattern in the Filename column so let's process those rows first
Day0 <- list_rawdata[[1]][grep("Day0", list_rawdata[[1]]$Filename),] %>% separate_wider_regex(Filename,
                                                                                              c(Date="24-0\\d-\\d{2}", " Day", Day="\\d",
                                                                                                ".*dilution_", Well="\\w\\d+", "\\.acs compensated"))
  # Now process the Filename column for the other days
NOTday0 <- list_rawdata[[1]][-grep("Day0", list_rawdata[[1]]$Filename),] %>% separate_wider_regex(Filename,
                                                                                              c(Date="24-0\\d-\\d{2}", " Day", Day="\\d", " -- ",
                                                                                                Incubator="\\w+", "\\.plate", Plate="\\d", 
                                                                                                ".*dilution_", Well="\\w\\d+", "\\.acs compensated"))
# Put the flow counts data back together into a single data.frame:
raw_Counts <- rbind(Day0 %>% mutate(Incubator=NA, Plate=0), # add in the 2 extra empty columns that are missing from Day0
                    NOTday0) %>% select(-Gate1, -Gate2, -Gate3, -Gate4)
rm(Day0, NOTday0)


# then do a similar thing for the volume data:
  # Day0 has a different pattern in the Plate column so let's process those rows first
Day0 <- list_rawdata[[2]][grep("Day0", list_rawdata[[2]]$Plate),] %>% separate_wider_regex(Plate,
                                                                                           c(Date="24-0\\d-\\d{2}", " Day", Day="\\d", ".*"))
  # Now process the Plate column for the other days
NOTday0 <- list_rawdata[[2]][-grep("Day0", list_rawdata[[2]]$Plate),] %>% separate_wider_regex(Plate,
                                                                                              c(Date="24-0\\d-\\d{2}", " Day", Day="\\d", " -- ",
                                                                                                Incubator="\\w+", "\\.plate", Plate="\\d"))
# Put the flow volumes data back together into a single data.frame:
raw_Vol <- rbind(Day0 %>% mutate(Incubator=NA, Plate=0), # add in the 2 extra empty columns that are missing from Day0
                 NOTday0) %>% rename(Well = Sample) # rename this column for consistency with the Counts data
rm(Day0, NOTday0)

# We can now combine the counts and volume data
  # here I need to use left join because we don't have volume data for Day 0 on 24-07-02 !!!!!!
raw_data <- left_join(raw_Counts, raw_Vol,
                      by=c("Date", "Day", "Well", "Incubator", "Plate"))
rm(raw_Counts, raw_Vol)


# add annotation specifying the Heat treatment and the Incubator
  # For 2July24: all samples were subjected to 6h of heat
  # For 8July24: samples in the Epoch plate reader are control (no heat)
  #              Samples in the H1 plate reader are 48h of heat
  # For 5Aug24: all samples were subjected to 12h of heat
  # For 19Aug24: all samples were subjected to 24h of heat
raw_data$Heat <- 0
raw_data$Heat[which(raw_data$Date == "24-07-02")] <- 6
raw_data$Heat[which(raw_data$Date == "24-07-08" & raw_data$Incubator == "H1")] <- 48
raw_data$Heat[which(raw_data$Date == "24-08-05")] <- 12
raw_data$Heat[which(raw_data$Date == "24-08-19")] <- 24

# change the variable classes for data analysis
raw_data$Count_grimontii <- as.numeric(raw_data$Count_grimontii)
raw_data$Count_putida <- as.numeric(raw_data$Count_putida)
raw_data$Count_protegens <- as.numeric(raw_data$Count_protegens)
raw_data$Count_veronii <- as.numeric(raw_data$Count_veronii)
```

Finally, we can annotate the data with the sample information for each well. Note that
there are different plate layouts for Day0 (same for all dates) And the experiment from
24-07-02 uses a different layout as compared to the rest of the data (see the layout png file in the corresponding data subfolder).
...But, also, I made
other mistakes too so there's modified layouts for that too! XP

```{r, annotateData}
# the "Plate1" layout is used for all days >0 (except for 24-07-02)
layout.plate1 <- data.frame(Well = paste0(LETTERS[1:8], rep((2*1:6)-1, each=8)),
                            putida = c(0, 1, 1, 1, 0, 0, 0, 0,
                                       1, 0, 0, 0, 1, 1, 1, 0,
                                       0, 0, 1, 1, 1, 0, 1, 0,
                                       0, 0, 1, 1, 1, 0, 1, 0,
                                       1, 1, 1, 0, 1, 0, 0, 0,
                                           0, rep(0,6), 1),
                            protegens = c(0, 0, 0, 1, 0, 0, 1, 0,
                                          0, 1, 0, 0, 1, 0, 0, 1,
                                          1, 0, 1, 1, 0, 1, 1, 0,
                                          0, 0, 1, 0, 0, 1, 0, 1,
                                          1, 1, 0, 1, 1, 0, 1, 0,
                                              1, rep(0,6), 0),
                            grimontii =  c(0, 0, 1, 0, 0, 1, 0, 0,
                                           0, 0, 1, 0, 0, 1, 0, 1,
                                           0, 1, 1, 0, 1, 1, 1, 0,
                                           1, 0, 0, 1, 0, 1, 0, 0,
                                           1, 0, 1, 1, 1, 0, 0, 1,
                                               1, rep(0,6), 0),
                            veronii =    c(0, 1, 0, 0, 1, 0, 0, 0,
                                           0, 0, 0, 1, 0, 0, 1, 0,
                                           1, 1, 0, 1, 1, 1, 1, 0,
                                           0, 1, 0, 0, 1, 0, 0, 0,
                                           0, 1, 1, 1, 1, 0, 1, 1,
                                               0, rep(0,6), 0))

### CommRich = 0 corresponds to blanks, mistakes made on Day0 are removed altogether,
###     and CommRich = NA is used to indicate contamination.
# modified layout of plate1 specific for 24-07-02
layout.plate1_2Jul <- layout.plate1
layout.plate1_2Jul$putida[c(1,8, 41:48)]    <- c(0, 1, 1, 1, 1, 0, 1, 0, 0, 0) 
layout.plate1_2Jul$protegens[c(1,8, 41:48)] <- c(1, 0, 1, 0, 0, 1, 0, 1, 0, 0)
layout.plate1_2Jul$grimontii[c(1,8, 41:48)] <- c(1, 0, 0, 1, 0, 1, 0, 0, 1, 0)
layout.plate1_2Jul$veronii[c(1,8, 41:48)]   <- c(0, 0, 0, 0, 1, 0, 0, 0, 0, 1)

# modified layout of plate1 specific for mistakes made on 24-07-08
  # column 4 of OD plate is swapped orientation
layout.plate1_8Jul <- layout.plate1
layout.plate1_8Jul$putida[25:32]    <- layout.plate1$putida[9:16]
layout.plate1_8Jul$protegens[25:32] <- layout.plate1$protegens[9:16]
layout.plate1_8Jul$grimontii[25:32] <- layout.plate1$grimontii[9:16]
layout.plate1_8Jul$veronii[25:32]   <- layout.plate1$veronii[9:16]

# add a column for community richness in all of the above df's
layout.plate1 <- layout.plate1 %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")
layout.plate1_2Jul <- layout.plate1_2Jul %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")
layout.plate1_8Jul <- layout.plate1_8Jul %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")


# the "Plate2" layout
layout.plate2 <- data.frame(Well = paste0(LETTERS[1:8], rep((2*1:6)-1, each=8)),
                            putida = c(1, 1, 1, 0, 1, 0, 0, 0,
                                       1, 0, 1, 0, 0, 0, 1, 1,
                                       1, 0, 1, 0, 0, 0, 1, 1,
                                       1, 0, 0, 0, 1, 1, 1, 0,
                                           rep(0,7), 0,
                                       0, 1, 0, 1, 1, 1, 0, 0),
                            protegens = c(1, 0, 0, 1, 0, 1, 0, 0,
                                          0, 1, 1, 0, 1, 0, 1, 1,
                                          0, 1, 0, 1, 0, 0, 1, 0,
                                          1, 0, 1, 0, 1, 1, 0, 1,
                                              rep(0,7), 1,
                                          0, 1, 1, 0, 1, 1, 0, 0),
                            grimontii = c(0, 1, 0, 1, 0, 0, 1, 0,
                                          1, 1, 1, 0, 0, 1, 1, 0,
                                          0, 1, 0, 0, 1, 0, 0, 1,
                                          1, 0, 0, 1, 1, 0, 1, 1,
                                             rep(0,7), 0,
                                          0, 1, 1, 1, 0, 1, 1, 0),
                            veronii = c(0, 0, 1, 0, 0, 0, 0, 1,
                                        1, 1, 1, 0, 1, 1, 0, 1,
                                        1, 0, 0, 0, 0, 1, 0, 0, 
                                        1, 0, 1, 1, 0, 1, 1, 1,
                                           rep(0,7), 1,
                                        0, 1, 1, 1, 1, 0, 1, 0))

# modified layout of plate2 specific for 24-07-02
layout.plate2_2Jul <- layout.plate2
layout.plate2_2Jul$putida[1:32]    <- layout.plate2$putida[c(9:32,41:47,40)]
layout.plate2_2Jul$protegens[1:32] <- layout.plate2$protegens[c(9:32,41:47,40)]
layout.plate2_2Jul$grimontii[1:32] <- layout.plate2$grimontii[c(9:32,41:47,40)]
layout.plate2_2Jul$veronii[1:32]   <- layout.plate2$veronii[c(9:32,41:47,40)]
layout.plate2_2Jul <- layout.plate2_2Jul[1:32,] # rest of flow plate 2 is empty

# modified layout of plate2 specific for mistakes made on 24-07-08 and 24-08-19
layout.plate2_8Jul19Aug <- layout.plate2[-(9:16),] # I screwed up column 8 of OD plate

# add a column for community richness in all of the above df's
layout.plate2 <- layout.plate2 %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")
layout.plate2_2Jul <- layout.plate2_2Jul %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")
layout.plate2_8Jul19Aug <- layout.plate2_8Jul19Aug %>% mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all")

# the "Inocula" layout
layout.inocula <- data.frame(Well = paste0(LETTERS[1:8], rep((2*1:6)-1, each=8)),
                               putida = rep(c(1, 0, 0, 0, 1, 1, 1, 0,
                                              0, 0, 1, 1, 1, 0, 1, NA), times=3),
                            protegens = rep(c(0, 1, 0, 0, 1, 0, 0, 1,
                                              1, 0, 1, 1, 0, 1, 1, NA), times=3),
                            grimontii = rep(c(0, 0, 1, 0, 0, 1, 0, 1,
                                              0, 1, 1, 0, 1, 1, 1, NA), times=3),
                              veronii = rep(c(0, 0, 0, 1, 0, 0, 1, 0,
                                              1, 1, 0, 1, 1, 1, 1, NA), times=3)) %>%
                mutate(CommRich = putida+protegens+grimontii+veronii , .keep="all") %>%
                    filter(!is.na(CommRich))


# a function to annotate each data set with the indicated layout
  # this will KEEP well blanks!
annotate_samples <- function(layout, select_date, select_plate) {
  relevant_data <- raw_data %>% filter(Date==select_date, Plate==select_plate)
  
  # for Innoc, use inner_join to combine the flow data with its annotation
  if(select_plate == 0){
    output_df <- inner_join(layout, relevant_data, by="Well")
  }
  if(select_plate != 0) {
    output_df <- left_join(merge(layout, relevant_data %>% select(Day, Incubator, Heat) %>% distinct()),
                         relevant_data, by=c("Well", "Day", "Incubator", "Heat"))
    output_df$Date <- select_date
    output_df$Plate <- select_plate
  }

  return(output_df)
  rm(relevant_data, output_df)#, blank_annot, blank_data)
}

# now we can add the sample names for each one.
annotated.rawdata <- rbind(annotate_samples(layout = layout.inocula, select_date = "24-07-02", select_plate=0),
                  annotate_samples(layout = layout.plate1_2Jul, select_date = "24-07-02", select_plate=1),
                  annotate_samples(layout = layout.plate2_2Jul, select_date = "24-07-02", select_plate=2),
                  annotate_samples(layout = layout.inocula, select_date = "24-07-08", select_plate=0),
                  annotate_samples(layout = layout.plate1_8Jul, select_date = "24-07-08", select_plate=1),
                  annotate_samples(layout = layout.plate2_8Jul19Aug, select_date = "24-07-08", select_plate=2), ##
                  annotate_samples(layout = layout.inocula, select_date = "24-08-05", select_plate=0),
                  annotate_samples(layout = layout.plate1, select_date = "24-08-05", select_plate=1),
                  annotate_samples(layout = layout.plate2, select_date = "24-08-05", select_plate=2),
                  annotate_samples(layout = layout.inocula, select_date = "24-08-19", select_plate=0),
                  annotate_samples(layout = layout.plate1, select_date = "24-08-19", select_plate=1),
                  annotate_samples(layout = layout.plate2_8Jul19Aug, select_date = "24-08-19", select_plate=2))

# fixing other small mistakes in annotation:
  # Day1 of 24-07-02: sample A1 from plate 2 was loaded into sample A1 plate 1.
annotated.rawdata$CommRich[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                   annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 3

annotated.rawdata$putida[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                 annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 1

annotated.rawdata$protegens[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                    annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 0

annotated.rawdata$grimontii[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                    annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 1

annotated.rawdata$veronii[which(annotated.rawdata$Date=="24-07-02" & annotated.rawdata$Day=="1" &
                                  annotated.rawdata$Well=="A1" & annotated.rawdata$Plate=="1")] <- 1


# Annotate the treatments
annotated.rawdata$Heat_Day <- as.numeric(NA)
annotated.rawdata$Heat_Day[which(annotated.rawdata$Heat!=0 & annotated.rawdata$Day==1)] <- 1
annotated.rawdata$Heat_Day[which(annotated.rawdata$Heat>6 & annotated.rawdata$Day==2)] <- 2
annotated.rawdata$Heat_Day[which(annotated.rawdata$Heat==48 & annotated.rawdata$Day==3)] <- 3

annotated.rawdata$Recov_Day <- as.numeric(NA)
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat==6 & annotated.rawdata$Day==2)] <- 1
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat==6 & annotated.rawdata$Day==3)] <- 2
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat %in% c(12,24) & annotated.rawdata$Day==3)] <- 1
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat %in% c(12,24) & annotated.rawdata$Day==4)] <- 2
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat==48 & annotated.rawdata$Day==4)] <- 1
annotated.rawdata$Recov_Day[which(annotated.rawdata$Heat==48 & annotated.rawdata$Day==5)] <- 2

# sanity check to make sure there are no redundant rows
stopifnot(!any(duplicated(annotated.rawdata %>% select(Date, Day, Incubator, Plate, Well))))

# change some of the values to more appropriate types
annotated.rawdata$Plate <- as.numeric(annotated.rawdata$Plate)
annotated.rawdata$Day <- as.numeric(annotated.rawdata$Day)

# clean up
rm(layout.inocula, layout.plate1, layout.plate1_2Jul, layout.plate1_8Jul, layout.plate2, layout.plate2_2Jul, layout.plate2_8Jul19Aug, list_rawdata, raw_data)
```

The annotated data contains information on the complete dataset, including blank wells and
excluded wells. Any mistakes there were made during inoculation on Day 0 have been removed
altogether. `CommRich == 0` indicates well blanks that should be empty (in this case, all
4 species columns will also be `0`). Finally, `CommRich == NA` indicates data rows that
were excluded; e.g., due to low total counts or contamination (in this case, the 4 species
columns will be kept to indicate what should have been in that excluded well).

For reproducibility and checking that the metadata is correctly associated with the data,
print the metadata out to file. Note that the location on the incubated plates
(corresponding to OD data) is different from the location on the flow cytometery plate. In
the code below I create a column for the OD_Well and assign unique identifiers for each
time series. The metadata file `annotation_for_alldata.csv` summarizes all of this info.

```{r, addODwell}
# annotation for Day 0 lists the plate as plate 0 but let's change that to Innoc
annotated.rawdata$Plate[which(annotated.rawdata$Plate==0)] <- "Innoc"

# copy the metadata to another variable and remove the data columns
metadata <- annotated.rawdata %>% select(-Volume,
                                         -Count_grimontii, -Count_protegens, -Count_putida, -Count_veronii)

# the columns currently labeled as "Well" and "plate" is actually only true for the location of the sample on the flow cytometer data
metadata$filler <- "plate"
metadata <- metadata %>% unite(col="plateNum", c(filler, Plate), sep="", remove = FALSE) %>% 
              unite(col="FLOWplateWell", c(plateNum, Well), sep="-", remove = FALSE) %>% select(-filler, -plateNum)

#####
# add true well sample location to metadata (i.e., as corresponding to OD data)
#####
# split up the Well into separate columns for the row and column location
metadata <- metadata %>% separate_wider_regex(Well, c(row="\\w", col="\\d+"))
metadata$REALcol <- 0

# for non-Innoc days after 2 July, the pattern is actually very simple and systematic
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==1)] <- 1
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==3)] <- 2
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==5)] <- 3
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==7)] <- 4
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==9)] <- 5
metadata$REALcol[which(metadata$Plate==1 & metadata$Date > "24-07-02" & metadata$col==11)] <- 6
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==1)] <- 7
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==3)] <- 8
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==5)] <- 9
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==7)] <- 10
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==9)] <- 11
metadata$REALcol[which(metadata$Plate==2 & metadata$Date > "24-07-02" & metadata$col==11)] <- 12

# for non-Innoc days on 2 July, the pattern is similar for plate 1 columns 1 to 9:
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==1)] <- 1
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==3)] <- 2
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==5)] <- 3
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==7)] <- 4
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==9)] <- 5
# the pattern changes from here:
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==11)] <- 7
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==1)] <- 8
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==3)] <- 9
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==5)] <- 10
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==7)] <- 12
# Note that plate 2 Well H7 on flow actually comes from H11
metadata$REALcol[which(metadata$Plate==2 & metadata$Date == "24-07-02" & metadata$col==7 & metadata$row=="H")] <- 11
# and for plate 1 column 1, Well H1 on flow actually comes from H6
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$col==1 & metadata$row=="H")] <- 6
# finally, plate 1 column1: Well A1 on flow actually comes from A6. But note the mistake on Day1
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$Day!=1 & metadata$col==1 & metadata$row=="A")] <- 6
# on 2 July Day 1, plate 1 well A1 on flow actually comes from A8
metadata$REALcol[which(metadata$Plate==1 & metadata$Date == "24-07-02" & metadata$Day==1 & metadata$col==1 & metadata$row=="A")] <- 8

# now we are finished with the NON-INNOC annotations
# we can put together the row and REALcol columns to get the location on the OD plate
data_meta <- metadata %>% filter(Plate != "Innoc") %>% unite("OD_well", c(row, REALcol), sep="") %>% select(-col) %>%
                    unite("uniqID", c(Date, Incubator, OD_well), sep=" ", remove = FALSE)

# last (and perhaps least), annotate the additional blank wells from 2 July,
july2_blanks <- data_meta %>% filter(Date=="24-07-02", CommRich==0) %>%
                    select(-FLOWplateWell, -Plate, -uniqID, -OD_well) %>% distinct()
missing_blanks <- data.frame(OD_well=c("A1", "H1", "H12", paste0(LETTERS[1:7],11), paste0(LETTERS[2:7],6)),
                             FLOWplateWell=NA, Plate=NA) %>%
                    mutate(uniqID=paste("24-07-02 Epoch", OD_well), .keep="all")
july2_missing <- merge(july2_blanks,missing_blanks)
data_meta <- rbind(data_meta, july2_missing)
rm(july2_blanks, missing_blanks, july2_missing)

#####
# Innoc data: add OD_well and uniqID columns
#####
# In order to annotate the most raw version of the data, I decided to create redundant rows for the Innoc data. This way each row from Innoc appears 5x with its associated OD_well and uniqID.
innoc_meta <- metadata %>% filter(Plate == "Innoc") %>% select(-row, -col, -REALcol, -Incubator, -Heat)
innoc_meta <- suppressWarnings( # we expect left_join to be upset about many-to-many relationship, no need to issue warning.
                    left_join(innoc_meta,
                              data_meta %>%
                                  select(-FLOWplateWell, -Day, -Plate, -Heat_Day, -Recov_Day) %>%
                                      distinct(), # remove the redundant rows from each day
                              by = c("CommRich", "putida", "protegens", "grimontii", "veronii", "Date"))
              )
# trash the now old df to avoid confusion
rm(metadata)

# save the complete metadata to file
write.csv(rbind(data_meta, innoc_meta), file="./annotation_for_alldata.csv", quote=FALSE, row.names=FALSE)

#####
# Save the fully annotated raw flow cytometry counts data
#####
# associate the metadata back with the raw counts data
# for Days > 0:
temp_metadata <- data_meta %>% separate_wider_regex(FLOWplateWell, c(FLOW="plate\\w+-", Well="\\w+"))
annot.days <- inner_join(temp_metadata, annotated.rawdata,
                         by=c("Well", "putida", "protegens", "grimontii", "veronii", "CommRich", "Date",
                              "Day", "Incubator", "Plate", "Heat", "Heat_Day", "Recov_Day")) %>%
                  unite("FLOWplateWell", c(FLOW, Well), sep="")
rm(temp_metadata, data_meta)

# for Innoc Days:
temp_metainnoc <- innoc_meta %>% separate_wider_regex(FLOWplateWell, c(FLOW="plate\\w+-", Well="\\w+"))
annot.innoc <- left_join(temp_metainnoc,
                         annotated.rawdata %>% select(-Incubator, -Heat),
                         by=c("Well", "putida", "protegens", "grimontii", "veronii", "CommRich", "Date",
                              "Day", "Plate", "Heat_Day", "Recov_Day")) %>%
                  unite("FLOWplateWell", c(FLOW, Well), sep="")

# save this to file as well
write.csv(rbind(annot.days, annot.innoc), file="./flow_rawdata.csv", quote=FALSE, row.names=FALSE)

# remove annotated.rawdata as it has been superseded by annot.days and annot.innoc
rm(annotated.rawdata, temp_metainnoc, innoc_meta)

########
# fix annotation mistake for flow cytometry acquisition of uniqID "24-07-02 Epoch A6"
# this well is missing on Day 1 bc A8 was pipetted there instead. But now we have 2 wells for "24-07-02 Epoch A8"...
########
wrong_row <- which(annot.days$uniqID == "24-07-02 Epoch A8" & annot.days$Day == 1 & is.na(annot.days$Volume))
annot.days$uniqID[wrong_row] <- "24-07-02 Epoch A6"
annot.days$OD_well[wrong_row] <- "A6"
annot.days$putida[wrong_row] <- 0
annot.days$protegens[wrong_row] <- 1
annot.days$grimontii[wrong_row] <- 1
annot.days$veronii[wrong_row] <- 0
annot.days$CommRich[wrong_row] <- 2
rm(wrong_row)
```

The data from Day 0 (`annot.innoc`) is 3x measurements of the innoculum that is used to
inoculate the 5 replicates. For the summary annotation file (aka
metadata above), each `FLOWplateWell` appears redundantly with the up to 5 associated
uniqID and OD_well replicates. I chose to do this so that the raw values from the flow
cytometry data are preserved with their relevant annotation.

Below, this Day 0 data is averaged across the 3 different `FLOWplateWell` values. This mean that below Day 0 is now joined with the rest of the data in the variable `annotated.rawdata`.

```{r, Day0}
# average the Day0 data actually across its redundant flow cytometery measurements...
mean.innoc <- annot.innoc %>% group_by(uniqID, OD_well, Incubator, Plate, Heat, Date,
                                 Day, Heat_Day, Recov_Day,
                                 CommRich, putida, protegens, grimontii, veronii) %>%
          summarise(Mean_putida = mean(Count_putida),
                    Mean_protegens = mean(Count_protegens),
                    Mean_grimontii = mean(Count_grimontii),
                    Mean_veronii = mean(Count_veronii),
                    SD_putida = sd(Count_putida),
                    SD_protegens = sd(Count_protegens),
                    SD_grimontii = sd(Count_grimontii),
                    SD_veronii = sd(Count_veronii),
                    Vol_mean = mean(Volume),
                    vol_sd = sd(Volume))

# here's some plots to summarize how much variation there is between measurements of the same inocula
plotting_mean.innoc <- mean.innoc %>% pivot_longer(cols = Mean_putida:SD_veronii,
                                                   names_to = c(".value", "species"),
                                                   names_sep = "_") %>%
                          filter(Incubator != "H1") # the same innoculum was used for 2 treatments on 24-07-08. Remove this redundancy for plotting
ggplot(plotting_mean.innoc,
       aes(x=Mean, y=SD, colour=species)) +
  geom_point(alpha=0.7) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  labs(title="3 measures of innoculum")

ggplot(plotting_mean.innoc,
       aes(x=Mean, y=SD, colour=Date)) +
  geom_point(alpha=0.7) +
  labs(title="3 measures of innoculum")

ggplot(plotting_mean.innoc %>% mutate(CV = SD/Mean),
       aes(x=Mean, y=CV, colour=species)) +
  geom_point(alpha=0.7) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  labs(title="3 measures of innoculum")

# due to false positive counts,
# the CV blows up when I am counting species that are not actually in that sample

ggplot(plotting_mean.innoc %>% filter((putida == 1 & species == "putida") |
                                      (protegens == 1 & species == "protegens") |
                                      (grimontii == 1 & species == "grimontii") |
                                      (veronii == 1 & species == "veronii")) %>%
            mutate(CV = SD/Mean),
       aes(x=Mean, y=CV, colour=species)) +
  geom_point(alpha=0.7) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  labs(title="3 measures of innoculum (remove absent sp)")

ggplot(plotting_mean.innoc %>% unite("community", putida:veronii),
       aes(x=community, y=Mean, colour=species)) +
  geom_point(alpha=0.7) +
  geom_errorbar(aes(ymin=Mean-SD, ymax=Mean+SD), width=.2) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(title="3 measures of innoculum")

ggplot(plotting_mean.innoc %>% filter((putida == 1 & species == "putida") |
                                      (protegens == 1 & species == "protegens") |
                                      (grimontii == 1 & species == "grimontii") |
                                      (veronii == 1 & species == "veronii")) %>%
            unite("community", putida:veronii),
       aes(x=community, y=Mean, colour=species)) +
  geom_point(alpha=0.7) +
  geom_errorbar(aes(ymin=Mean-SD, ymax=Mean+SD), width=.2) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(title="3 measures of innoculum (remove absent sp)")

ggplot(plotting_mean.innoc %>% ungroup() %>%
              select(-uniqID, -OD_well) %>% distinct() %>%
                              filter((putida == 1 & species == "putida") |
                                      (protegens == 1 & species == "protegens") |
                                      (grimontii == 1 & species == "grimontii") |
                                      (veronii == 1 & species == "veronii")) %>%
            unite("community", putida:veronii) %>% mutate(CV = SD/Mean),
       aes(x=community, y=CV, colour=species)) +
  geom_jitter(width=0.2, alpha=0.7) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  labs(title="3 measures of innoculum (remove absent sp)")

# I have no idea what I would do with this info about sample volume, but here it is:
  # There's no values for 24-07-02 because I accidentally forgot to save the .apx file (this is before I realized that volume is not saved in the .acs files)
ggplot(plotting_mean.innoc,
       aes(x=Vol_mean, y=vol_sd, colour=Date)) +
  geom_point(alpha=0.7) +
  labs(title="3 measures of innoculum")

# finally, we can add the mean counts for the Innoc to the whole data
annotated.rawdata <- mean.innoc %>% select(-SD_putida, -SD_protegens, -SD_grimontii, -SD_veronii, -vol_sd) %>%
                      rename(Count_putida=Mean_putida, Count_protegens=Mean_protegens, Count_grimontii=Mean_grimontii, Count_veronii=Mean_veronii, Volume=Vol_mean)
annotated.rawdata <- rbind(annotated.rawdata, annot.days)

# cleanup
rm(annot.days, annot.innoc, plotting_mean.innoc)
```

## Misclassification rate: estimated from innocula (Day 0)

I define the misclassification rate as
$\frac{\text{false positive events}}{\text{total events across all fluorescences}}$. In
other words, I am counting the number of events in the gate(s) where I know there should
be zero then dividing by the total number of fluorescent events in that well.
 To estimate the misclassification rate, I use the data from Day 0.

```{r, misclassification_est}
# use Day0 innoculum measurements for a first pass at estimating the misclassification rate
# i.e., the rate of falsely classifying as species A when I know for certain that species A is not present in my sample
misclass.innoc <- mean.innoc %>% mutate(Total_counts = Mean_putida + Mean_protegens + Mean_grimontii + Mean_veronii) %>% # get total for each sample
                    ungroup() %>% select(-uniqID, -OD_well) %>% distinct() %>%   # remove any redundant data
                        # put each species count in its own row in the column called mean (instead of having a column for each species)
                          pivot_longer(cols = Mean_putida:SD_veronii,
                                                   names_to = c(".value", "species"),
                                                   names_sep = "_") %>%
                            filter(Incubator != "H1") %>% # remove the redundant data
                              # keep just the instances where we know for sure that this species was NOT present
                                filter((putida == 0 & species == "putida") |
                                      (protegens == 0 & species == "protegens") |
                                      (grimontii == 0 & species == "grimontii") |
                                      (veronii == 0 & species == "veronii")) %>%
                                  # misclassification rate is the number of events / total counts
                                   mutate(mean_rate = Mean/Total_counts,
                                          sd_rate = SD/Total_counts)

# re-order the species from fast to slow for better plotting
misclass.innoc$species <- factor(misclass.innoc$species,
                                 levels = c("putida", "protegens", "grimontii", "veronii"))

ggplot(misclass.innoc, aes(x=species, y=mean_rate, colour=species)) +
  geom_beeswarm(alpha=0.5) +
  geom_errorbar(aes(ymin=mean_rate-sd_rate, ymax=mean_rate+sd_rate), width=.05, alpha=0.2) +
  scale_colour_manual(values=species_4pal_speed) +
  labs(title="misclassification rate in innoculum", y="mean +/- SD")

max(misclass.innoc$mean_rate)

ggplot(misclass.innoc %>% unite("community", putida:veronii),
       aes(x=species, y=mean_rate, colour=species)) +
  facet_wrap(vars(community)) +
  geom_point(alpha=0.5) +
  scale_y_continuous(breaks = c(0, 0.005, 0.01)) +
  scale_colour_manual(values=species_4pal_speed) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title="misclassification rate in innoculum")

# summarize the mean and max misclassification rates observed for each species
misclass.innoc %>% group_by(species) %>% summarise(mean_misclass = mean(mean_rate),
                                                   max_misclass = max(mean_rate))
# clean-up
rm(misclass.innoc, mean.innoc)
```


From here we can clearly see that the misclassification rate can be as bad as 1% and that
it depends on the species. Protegens is the most likely to be misclassified and, from the
plot of all possible community combinations, we see that the problem seems to be that
putida cells are being misclassified as belonging to protegens.

But I know that this rate of misclassification also depends on environmental conditions.
So I don't think it makes sense to correct the data using the exact values given above.
The more cautious approach would be to treat with caution any counts that are less than
1%.

# Data Processing

Here we make decisions about which data to keep and which to toss.

## Minimum Number of Events

I need to set a threshold for the minimum number of fluorescent events observed in a well
in order for me to decide that the well is not trustworthy.

At some point I did sample some wells that are true negatives. From this we learn that a
true negative can have as many as 20 total events.

Remember that I set the stopping conditions for 10 000 events in the cell gate OR until it
reaches the end of the sample (which seems to be 146uL). Let's rather arbitrarily set the
minimum total events in the well at 51 and see what happens with that.

```{r, identifyNAwells}
annotated.rawdata <- annotated.rawdata %>% mutate(Total_counts = Count_putida + Count_protegens + Count_grimontii + Count_veronii) %>%
                                           mutate(Total_density = Total_counts/Volume)

# plot the counts and volume for true negative wells
ggplot(annotated.rawdata %>% filter(CommRich==0, !is.na(Total_counts)),
       aes(x=Total_counts, y=Volume)) +
  geom_point() +
  labs(title="True negatives")

# plot the total counts as a histogram just to see what the dispersal is like
ggplot(annotated.rawdata, aes(x=Total_counts)) +
  geom_histogram(colour="black", fill="white") +
  labs(title="everything")

ggplot(annotated.rawdata, aes(x=Total_counts)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_counts in LOG SCALE!", title="everything")

ggplot(annotated.rawdata, aes(x=Total_counts)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_continuous(limits = c(-10,1010)) +
  labs(title="everything")


# then plot the total counts against the volume because we expect these very low counts should be associated with the highest volumes
ggplot(annotated.rawdata, aes(x=Total_counts, y=Volume, colour=as.factor(Heat_Day))) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  labs(colour="Day of heat") +
  labs(title="everything")


# okay, let's just see a histogram of the total cell density
ggplot(annotated.rawdata, aes(x=Total_density)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!") +
  labs(title="everything")


ggplot(annotated.rawdata %>% filter(!is.na(Heat_Day)),
       aes(x=Total_density)) +
  facet_grid(rows = vars(Heat_Day)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!", title="Day of Heat (everything)")

ggplot(annotated.rawdata %>% filter(!is.na(Recov_Day)),
       aes(x=Total_density)) +
  facet_grid(rows = vars(Recov_Day)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!", title="Day of Recovery (everything)")


### check what these graphs look like when I exclude wells where Total_counts < 51
ggplot(annotated.rawdata %>% filter(Total_counts > 50),
       aes(x=Total_counts, y=Volume, colour=as.factor(Heat_Day))) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  labs(colour="Day of heat") +
  labs(title="Total_counts > 50")

ggplot(annotated.rawdata %>% filter(Total_counts > 50),
       aes(x=Total_density)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!") +
  labs(title="Total_counts > 50")

ggplot(annotated.rawdata %>% filter(Total_counts > 50, !is.na(Heat_Day)),
       aes(x=Total_density)) +
  facet_grid(rows = vars(Heat_Day)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!", title="Day of Heat (Total_counts > 50)")

ggplot(annotated.rawdata %>% filter(Total_counts > 50, !is.na(Recov_Day)),
       aes(x=Total_density)) +
  facet_grid(rows = vars(Recov_Day)) +
  geom_histogram(colour="black", fill="white") +
  scale_x_log10() +
  labs(x="Total_density in LOG SCALE!", title="Day of Recovery (Total_counts > 50)")

#######
# set threshold of > 50 events in total
#######
# copy everything EXCEPT BLANK WELLS to new variable
the.data <- annotated.rawdata %>% filter(CommRich != 0)

# summarize some information about the data points that I'm about to exclude
the.data %>% filter(Total_counts < 51) %>% ungroup() %>% select(uniqID, Heat, Day, Heat_Day, Recov_Day, CommRich, Volume, Total_counts) %>% summary()

# exclude from analysis all non-blanks rows where Total_counts < 51
the.data$CommRich[which(the.data$Total_counts < 51)] <- NA

# put the blank data back with the whole dataset
the.data <- rbind(the.data,
                  annotated.rawdata %>% filter(CommRich == 0))

# replace the data with NA values for all rows where Total_counts < 51
    # this includes both the excluded unreliable data as well as the true blanks flow data
the.data$Count_putida[which(the.data$Total_counts < 51)] <- NA
the.data$Count_protegens[which(the.data$Total_counts < 51)] <- NA
the.data$Count_grimontii[which(the.data$Total_counts < 51)] <- NA
the.data$Count_veronii[which(the.data$Total_counts < 51)] <- NA
the.data$Total_density[which(the.data$Total_counts < 51)] <- NA
the.data$Total_counts[which(the.data$Total_counts < 51)] <- NA

# clean-up
rm(annotated.rawdata)
```

I have re-assigned all wells that had less than 51 total fluorescent events as NA values.
This was a total of 87 wells.

Note that I've also removed any flow cytometry data from the true negative wells. This was
17 wells.

```{r, relAbundances}
# calculate densities and relative abundances for each species
the.data <- the.data %>% mutate(Conc_putida = Count_putida/Volume,
                                Conc_protegens = Count_protegens/Volume,
                                Conc_grimontii = Count_grimontii/Volume,
                                Conc_veronii = Count_veronii/Volume,
                                relDen_putida = Count_putida/Total_counts,
                                relDen_protegens = Count_protegens/Total_counts,
                                relDen_grimontii = Count_grimontii/Total_counts,
                                relDen_veronii = Count_veronii/Total_counts) #%>%
              #select(-Total_counts)

# sanity check that the relative densities are always adding up to 1
check <- the.data %>% mutate(sum_relDen = relDen_putida + relDen_protegens + relDen_grimontii + relDen_veronii) %>%
            # for convenience, remove the 87 NA values
            drop_na(Total_counts)
all.equal(check$sum_relDen, rep(1, nrow(check))) %>% # use all.equal() as there seem values very close to 1 but not exactly equal to 1
  stopifnot()

rm(check)
```

I have calculated the relative densities and made sure that all relative densities add up
to 1.

## Plot preliminary time-series

Before diving deeper into the data, let's just see quickly what the time series look like:

```{r, prelimTimeSeries}
# check: is each replicated time series annotated appropriately so that it can be pieced together?
the.data <- the.data %>% unite("community", putida:veronii, remove=FALSE) %>% ungroup()
for(com in unique(the.data$community)) {
  plot( ggplot(the.data %>% filter(community==com) %>%
           select(uniqID, Heat, Day, relDen_putida, relDen_protegens, relDen_grimontii, relDen_veronii) %>%
              pivot_longer(cols=starts_with("relDen"), names_to="species", names_prefix="relDen_", values_to="relDensity"),
         aes(x=Day, y=relDensity, colour=species, group=paste(uniqID,Heat,species))) +
    facet_grid(~Heat) +
    geom_point(alpha=0.2) +
    geom_line(alpha=0.5) +
    scale_colour_manual(values=species_4pal_alphabetical) +
    labs(title=com))
}

# clean up
rm(com)
```

After staring at the above time series for long enough, two things become clear

1.  Protegens has contaminated several wells. This is unambiguous contamination when it is
    present in communities where it was not innoculated. For these contaminated
    replicates, the entire time-series will be excluded from the analysis.

2.  The misclassification rate varies over time: e.g., putida is misclassified in a
    protegens monoculture (0_1_0_0) on day 1 for 3 different heat treatments. It appears
    in that well with a density of \> 10% ! As well, protegens and veronii are
    misclassified in putida monoculture (1_0_0_0) on days 2 and 3 of 24 hrs heat.

## Identify wells driven to extinction

Of the 87 NA values identified above,

-   Some occurred as a result of flow cytometry issues. E.G., there was probably a bubble
    that I didn't notice. (When I noticed the bubble, I would re-run that well. But this
    is only after I began to understand that this was happening. So some wells were
    unfortunately lost because of this error.)\
    \
    IN THIS CASE: this is a true NA value. It only happens at one time point (which may or
    may not be a heat day). And there is data for this well during the recovery period.

-   Some occurred as a result of prolonged heat exposure that dropped the total density in
    that well below the threshold of detection. This only happened on day 3 of heat for
    the longest heat treatment. There is data for this well during the recovery period.\
    \
    IN THIS CASE: this is a true NA value.

-   Others occurred as a result of prolonged heat exposure that drove the well to complete
    extinction. There is no flow cytometery data for this well during the recovery period
    because it went extinct. Extinction needs to be confirmed against the OD data.\
    \
    IN THIS CASE: this is a true NA value during the heat treatment **but it should become a 0 value during the recovery period.**

The OD data is analyzed in the file called `main_expt--OD_analysis.Rmd`.
This script outputs a csv file indicating the extinct wells, which I will use below.

```{r, extinctWells}
# import extinct well data from file
extinct <- read.csv("./extinctOD_wells.csv")

# we know that there was no detectable growth on Recovery days. So replace the current values with true 0's here.
the.data[which(the.data$uniqID %in% extinct$uniqID & the.data$Recov_Day>0),] <- the.data[which(the.data$uniqID %in% extinct$uniqID & the.data$Recov_Day>0),] %>%
          mutate(Total_density=0, Conc_putida=0, Conc_protegens=0, Conc_grimontii=0, Conc_veronii=0,
                 relDen_putida=0, relDen_protegens=0, relDen_grimontii=0, relDen_veronii=0,
                 CommRich=putida+protegens+grimontii+veronii)
# during the heat days, we know that there was no OD-detectable growth for (extinct$Day + 1).
# This means any flow cytometry data we have is unreliable and should be replaced with NA.
  # wells where Day 2 is unreliable
tmp <- extinct %>% filter(Day == 1)
the.data[which(the.data$uniqID %in% tmp$uniqID & the.data$Day==2),] <- the.data[which(the.data$uniqID %in% tmp$uniqID & the.data$Day==2),] %>%
          mutate(Total_density=NA, Conc_putida=NA, Conc_protegens=NA, Conc_grimontii=NA, Conc_veronii=NA,
                 relDen_putida=NA, relDen_protegens=NA, relDen_grimontii=NA, relDen_veronii=NA,
                 CommRich=NA)
rm(tmp)
  # wells where Day 3 is unreliable (and Day 3 is a heat day!)
extinct <- extinct[-which(extinct$uniqID %in% c("24-08-19 Epoch B4", "24-08-19 Epoch D2")),]
the.data[which(the.data$uniqID %in% extinct$uniqID & the.data$Day==3),] <- the.data[which(the.data$uniqID %in% extinct$uniqID & the.data$Day==3),] %>%
          mutate(Total_density=NA, Conc_putida=NA, Conc_protegens=NA, Conc_grimontii=NA, Conc_veronii=NA,
                 relDen_putida=NA, relDen_protegens=NA, relDen_grimontii=NA, relDen_veronii=NA,
                 CommRich=NA)
rm(extinct)
```

## Distinguish between contamination & misclassification

To address both the problem of contamination & the problem of the misclassification rate
varying over time, I had to closely re-examine the flow cytometry raw data (which I did by eye, *insert crying emoji*).

From the Day 0 data, I hypothesized that the misclassification rate is ~1%. So let's pull up the identity of all the flow cytometry data where >1% of the relative density is attributed to a species that was not inoculated in that well (i.e., and therefore it should not be there). I then manually examined the flow cytometry raw data files in FCS Express for all the wells listed below:

```{r, contaminationANDmisclassification}
# identify contamination at 1%
contamin.df <- the.data %>%  filter((putida == 0 & relDen_putida > 0.01) |
                                    (protegens == 0 & relDen_protegens > 0.01) |
                                    (grimontii == 0 & relDen_grimontii > 0.01) |
                                    (veronii == 0 & relDen_veronii > 0.01))

contamin.df %>% filter(Date %in% c("24-08-05", "24-08-19")) %>% select(Date, FLOWplateWell, Day, community,
                                                                       relDen_putida, relDen_protegens, relDen_grimontii, relDen_veronii)
```

The gating for all the wells listed above (and more) was double-checked by eye in FCS Express and new cell counts were outputted. It seemed to me that there is a correlation between the heat environment, or at least the day of the serial transfer, and how clean or messy the gating looked. In particular it seemed to me that it was more difficult to classify species during heat days.

We know from the OD data (see `main_expt--OD_analysis`) that the 24h of heat treatment had no instance of contamination detected (i.e., at least for the blank wells). Since I suspect that the misclassification rate changes with the heat day, let's assume that the 24h heat treatment does not contain any contamination events then look at the occurence of species that were never inoculated in those wells as an estimate of the misclassification rate on different days of the serial transfer.

(In the future: it should also be possible to get a covariance matrix to estimate which species are being misclassified as which.)

```{r, misclass_24hHeat}
misclass24 <- the.data %>% filter(Day > 0, Heat == 24) %>%
                filter((putida == 0 & relDen_putida > 0) |
                       (protegens == 0 & relDen_protegens > 0) |
                       (grimontii == 0 & relDen_grimontii > 0) |
                       (veronii == 0 & relDen_veronii > 0))
# separate the correctly called species from the species that are absent
misclass24_REAL <- misclass24 %>% mutate(relDen_putida = putida * relDen_putida,
                                         relDen_protegens = protegens * relDen_protegens,
                                         relDen_grimontii = grimontii * relDen_grimontii,
                                         relDen_veronii = veronii * relDen_veronii)
misclass24 <- misclass24 %>% mutate(relDen_putida = abs(putida-1) * relDen_putida,
                                    relDen_protegens = abs(protegens-1) * relDen_protegens,
                                    relDen_grimontii = abs(grimontii-1) * relDen_grimontii,
                                    relDen_veronii = abs(veronii-1) * relDen_veronii)
# pivot longer so there's a column for species
misclass24_REAL <- misclass24_REAL %>% pivot_longer(cols = relDen_putida:relDen_veronii,
                                                    values_to = "relDen",
                                                    names_to = "species",
                                                    names_prefix = "relDen_") %>%
                    select(uniqID, Day, community, putida, protegens, grimontii, veronii,
                           Total_density, relDen, species)
misclass24 <- misclass24 %>% pivot_longer(cols = relDen_putida:relDen_veronii,
                                                    values_to = "relDen",
                                                    names_to = "species",
                                                    names_prefix = "relDen_") %>%
                select(uniqID, Day, community, putida, protegens, grimontii, veronii,
                       Total_density, relDen, species)
# remove the true species from the misclass data because these are now fake 0's
misclass24 <- misclass24[-which(misclass24$putida == 1 & misclass24$species == "putida"),]
misclass24 <- misclass24[-which(misclass24$protegens == 1 & misclass24$species == "protegens"),]
misclass24 <- misclass24[-which(misclass24$grimontii == 1 & misclass24$species == "grimontii"),]
misclass24 <- misclass24[-which(misclass24$veronii == 1 & misclass24$species == "veronii"),]
# remove the single contaminated sample
misclass24 <- misclass24[-which(misclass24$protegens == 0 & misclass24$species == "protegens" & misclass24$relDen > 0.75),]

ggplot(misclass24,
       aes(x=species, y=relDen, colour=species)) +
  facet_wrap(vars(Day)) +
  geom_beeswarm(alpha=0.5) +
  scale_colour_manual(values=species_4pal_alphabetical) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(y="relative density of misclassified",
       title="misclassification in 24h heat for different days")

# clean up
rm(misclass24, misclass24_REAL)
```
Recall that for 24h duration, Day 1 of serial transfer had 6h of extreme heat at the end, Day 2 was all extreme heat then returned to the "ambient" warm temperature only in the last few hours, and Days 3 & 4 were the recovery days with constant "ambient" warm temperature.

What we see from the plot above is that the misclassification rate can get as high as 20% (and that it does depend on the day but it seems that the first day of recovery is actually worse than the heat days themselves), although most replicates & days seem to be well behaved.

Therefore let's set 25% as the threshold for contamination. This means that any replicates that show >25% relative density for a species that was not inoculated there are defined as contaminated. All time-points from these contaminated replicates are completely removed from the downstream analysis.

```{r, plot_finalized_timeseries}
rm(contamin.df) # remove anything we may have had above.

# for now let's define contamination as >25% for something that should not be there.
contamin.df <- the.data %>%  filter((putida == 0 & relDen_putida > 0.25) |
                                    (protegens == 0 & relDen_protegens > 0.25) |
                                    (grimontii == 0 & relDen_grimontii > 0.25) |
                                    (veronii == 0 & relDen_veronii > 0.25))
tmp <- the.data[-which(the.data$uniqID %in% unique(contamin.df$uniqID)),]

###############
# output absolute density data for analysis
###############

# Day 0 would need to be the pre-dilution absolute densities
ggplot(tmp %>% filter(Day==0) %>% select(-uniqID, -OD_well, -Heat, -Incubator) %>% distinct(),
       aes(x=Date, y=Total_density)) +
  geom_beeswarm() +
  labs(y="")
ggplot(tmp %>% filter(Day==0) %>% select(-uniqID, -OD_well, -Heat, -Incubator) %>% distinct(),
       aes(x=Date, y=Volume)) +
  geom_beeswarm() +
  labs(y="")
# I lost the data on flow volume for Date 24-07-02.
# But we see from the plot that there's not *that* much variation between batches.

## IMPORTANT NOTE: the Day 0 data is not featured in any of the downstream analyses for this manuscript. Therefore this interpolation doesn't actually matter.

# let's interpolate the well volumes on Day 0 of 24-07-02 by using the median well volumes for all other dates on Day 0
tmp.Day0 <- tmp[which(tmp$Day==0 & tmp$Incubator=="Epoch"),] %>% select(-uniqID, -OD_well) %>% distinct()
  # get the median volume for Day 0
medianVol <- median(tmp.Day0$Volume, na.rm=TRUE)
  # apply the median volume to Day 0 values from 24-07-02
tmp.Day0 <- tmp.Day0 %>% filter(Date=="24-07-02")
tmp.Day0$Volume <- medianVol
  # recalculate the absolute densities for Day0
tmp.Day0 <- tmp.Day0 %>% mutate(Total_density = Total_counts/Volume,
                                Conc_putida = Count_putida/Volume,
                                Conc_protegens = Count_protegens/Volume,
                                Conc_grimontii = Count_grimontii/Volume,
                                Conc_veronii = Count_veronii/Volume)
# finally, join the estimated absolute densities for Day0 back in with the whole data
tmp.Day0.0702 <- left_join(tmp %>% filter(Day==0, Date=="24-07-02") %>% select(-Volume, -Total_density, -Conc_putida, -Conc_protegens, -Conc_grimontii, -Conc_veronii),
                           tmp.Day0)
tmp <- rbind(tmp %>% filter(Date != "24-07-02"),
             tmp %>% filter(Date == "24-07-02") %>% filter(Day > 0),
             tmp.Day0.0702)

rm(medianVol, tmp.Day0, tmp.Day0.0702)

# finally, remove known miscalled estimates from the data
tmp <- tmp %>% mutate(Conc_putida = putida * Conc_putida,
                      Conc_protegens = protegens * Conc_protegens,
                      Conc_grimontii = grimontii * Conc_grimontii,
                      Conc_veronii = veronii * Conc_veronii) %>%
        mutate(Total_density = Conc_putida + Conc_protegens + Conc_grimontii + Conc_veronii)

# output this data to file
absDensity <- tmp %>% filter(community != "0_0_0_0") %>%
                select(uniqID, Heat, Day, Heat_Day, Recov_Day, CommRich:veronii, Total_density:Conc_veronii)
save(absDensity, file="./absolute_density_data.RData")

rm(tmp, com, contamin.df)
```

## Plot final time-series

```{r, plot_finalized_timeseries}
for(com in unique(absDensity$community)) {
  plot(ggplot(absDensity %>% filter(community==com) %>%
           select(uniqID, Heat, Day, Conc_putida, Conc_protegens, Conc_grimontii, Conc_veronii) %>%
              pivot_longer(cols=starts_with("Conc"), names_to="species", names_prefix="Conc_", values_to="absDensity"),
         aes(x=Day, y=absDensity, colour=species, group=paste(uniqID,Heat,species))) +
    facet_grid(~Heat) +
    geom_point(alpha=0.2) +
    geom_line(alpha=0.5) +
    scale_colour_manual(values=species_4pal_alphabetical) +
    labs(title=com))
}

# in the analyses below we will be interested in shannon diversity so let's already make a column for that
absDensity$Diversity <- diversity(absDensity[,c("Conc_putida", "Conc_protegens", "Conc_grimontii", "Conc_veronii")])

# first let's remove the empty wells as we won't need them anymore
absDensity <- absDensity %>% filter(community != "0_0_0_0")
# Note that there are many 0 and NA values for Total_density
summary(absDensity$Total_density)
# 0's are communities that went extinct altogether and never recovered
absDensity[which(absDensity$Total_density == 0),]
# most NA's are communities below the threshold of detection during heat that later perhaps recovered
absDensity[which(is.na(absDensity$Total_density) & absDensity$Heat>12),]
# other NA's are just missing data (e.g., due to flow cytometry clogs or just plain pipetting mistakes)
absDensity[which(is.na(absDensity$Total_density) & absDensity$Heat<12),]

# the total density data will have to be slightly adjusted for fitting to the models
absDen_forFit <- absDensity %>% filter(Day > 0)
# for the "raw" total density data that will be fitted via negative binomial GLM,
  # keep the 0's in the data
  # but convert NA's into epsilon values (where epsilon is just below the threshold of detection)
below_threshold_rows <- which(is.na(absDen_forFit$Total_density) & absDen_forFit$Heat>12)
absDen_forFit$Total_density[below_threshold_rows] <- (0.25*50/146)
# for the transformed total density data, apply (x + epsilon) transformation to all values EXCEPT those that used to be NA's
absDen_forFit$TotDen_plusEpsilon <- absDen_forFit$Total_density
absDen_forFit$TotDen_plusEpsilon[-below_threshold_rows] <- absDen_forFit$TotDen_plusEpsilon[-below_threshold_rows] + (0.25*50/146)
rm(below_threshold_rows)

# re-arrange the levels of Heat so that emmeans can be run:
absDen_forFit$Heat <- as.character(absDen_forFit$Heat)
absDen_forFit$Heat[which(absDen_forFit$Heat == 0)] <- "control"
# !!! emmeans expects the control to be the very *last* level !!!
absDen_forFit$Heat <- factor(absDen_forFit$Heat,
                             levels = c("6", "12", "24", "48", "control"))

# clean up
rm(com)
```

After plotting, I also created another version of the full data that will be used for fitting the data to models: ``. It excludes the Day 0 data (as this will not be analyzed hereafter).

Below, I will analyze the diversity and the productivity (AKA total density) to understand how
they change relative to the no heat control during the resistance and the recovery period.
The diversity is easier to deal with because we can use the Shannon diversity calculation
as implemented by `vegan`.

For the total abundances, there are extinction (AKA 0's) and NA events in this data -- the extinction events in particular are important and meaningful parts of our data!! To deal with this issue, I will distinguish between 0's and NA's by using a $x + \epsilon$ transformation, where $\epsilon$ indicates samples that are below the threshold of detection. I will use $\epsilon$ as 0.25 \* the threshold of detection for
the flow cytometer (which is 50 total fluorescent events in $146\mu L$).

## Load community growth rates expectations

We define "average growth rate of the community" either as the expectation from the inoculated communities (e.g., the quadruplet community has expected growth rate = mean of the 4 species). In other words, this assumes that all species that were inoculated in the community remain at equal ratios and therefore the average growth rate of a community is just the mean of the growth rates of the species that were inoculated there. This is called the `community_expected_mu`.

We also define it as the mean of the realized communities by using the species mean relative densities in the no heat control condition. In other words, this takes into account the actual relative densities of the species that can hang out together across serial transfers and uses that as an expectation of the community's growth rate. This is called the `community_averaged_mu`.

```{r, load_growth_rates30C}
# load in the stationary phase growth rate estimates from Expt1
load("expt1--all_growthcurve_data.RData")
rm(ALL_data.df, derivs.df, TTD.df) # keep just the dataframe with the growth rate estimates (mu)
Dil_growthrates.df <- Dil_growthrates.df %>% filter(Inoculum == "Stationary",
                                                    Temp == 30,
                                                    Sample %in% c("BSC001", "BSC005", "BSC019", "CK101")) %>%
                        arrange(desc(mu))

# calculate the average growth rate for the inoculated communities
absDen_forFit <- absDen_forFit %>% mutate(community_expected_mu = (Dil_growthrates.df$mu[1]*putida + Dil_growthrates.df$mu[2]*protegens + Dil_growthrates.df$mu[3]*grimontii + Dil_growthrates.df$mu[4]*veronii)/(putida + protegens + grimontii + veronii))

# calculate the average growth rate for the realized communities
temp <- absDensity %>% filter(Heat == 0) %>% group_by(community) %>%
          mutate(relDen_putida = Conc_putida/Total_density,
                 relDen_protegens = Conc_protegens/Total_density,
                 relDen_grimontii = Conc_grimontii/Total_density,
                 relDen_veronii = Conc_veronii/Total_density) %>%
            summarise(relDen_putida = median(relDen_putida, na.rm = TRUE),
                      relDen_protegens = median(relDen_protegens, na.rm = TRUE),
                      relDen_grimontii = median(relDen_grimontii, na.rm = TRUE),
                      relDen_veronii = median(relDen_veronii, na.rm = TRUE)) %>%
              mutate(community_averaged_mu = Dil_growthrates.df$mu[1]*relDen_putida + Dil_growthrates.df$mu[2]*relDen_protegens + Dil_growthrates.df$mu[3]*relDen_grimontii + Dil_growthrates.df$mu[4]*relDen_veronii)

absDen_forFit <- inner_join(absDen_forFit, temp %>% select(community, community_averaged_mu))

# remember to also add the average community growth rates to the other data frame (this one is used for the extinction analysis because Heat is numeric here)
absDensity <- inner_join(absDensity,
                         absDen_forFit %>%
                           select(community, community_expected_mu, community_averaged_mu) %>%
                             distinct())

# clean up
rm(temp, Dil_growthrates.df)
```

# Ordination of communities over time

After backing up a bit and thinking about what the main story of the paper could be, I
think the main message that I would like to tell with the paper is that heat duration has
a threshold effect. So while shorter and intermediate heat durations have some effect
during heat that is different from control, communities return to a similar state after
recovery. On the other hand, long duration heat events lead to extinction (i.e., either of
the entire community or of vulnerable species within the community) so the communities
cannot recover anymore. In other words, there's a threshold effect where the amount of
heat (or bacterial) induced killing has gone on for so long than it passes a critical
point and the communities recover to a different state. I don't want to use the term
"tipping point" but for sure the design of our experiment allows us to use phrases like
"threshold effect" and "critical transition" *sensu stricto* (e.g., as explained in
[Munson et al., 2018](https://doi.org/10.1111/nph.15145)).

I think it would be fantastic if I could produce a figure that summarizes the entire data
in a way that builds an argument for the quintessential ball-landscape schematic that
people keep showing when they talk about ecosystem stability to perturbation (e.g., see
schematic in Fig. 2 of [Shade et al., 2012](https://doi.org/10.3389/fmicb.2012.00417) or
the empirical figure in Fig. 3 of [Jurburg et al.,
2017](https://doi.org/10.1038/srep45691)).

Here are some tutorials on ordination:
<https://eddatascienceees.github.io/tutorial-rayrr13/>
<https://ourcodingclub.github.io/tutorials/ordination/>
<https://uw.pressbooks.pub/appliedmultivariatestatistics/chapter/anosim/>
<https://uw.pressbooks.pub/appliedmultivariatestatistics/chapter/visualizing-and-interpreting-ordinations/>

To calculate the Bray-Curtis dissimilarity, we are forced to choose how to deal with NA values (most of
which are found in the resistance time points and so it doesn't really make sense to outright drop them). NA
values exist for two reasons:

1.  "true" missing data where the well was not acquired at all due to technical
    difficulties/mistakes (only a few of this type). I used interpolation to deal with these: use the median value from other replicates at that same community:day:treatment.

2.  below threshold of detection missing data where the total density was too low to
    reliably estimate the cell counts. I replace the NA with the limit of detection of the cytometer ("epsilon", as above) and assume equal frequencies of the species that were inoculated in that community.
    
Then, if we just follow the example tutorials directly, with columns = four species and rows =
different communities on different days for different heat treatments, then the data
simply gets split up by species. But that's not what we want to understand in this case.

We want to understand how the communities are changing over time (and as a function of different heat durations) so let's give it the data
as species x time. This can be achieved by widening the data so that we have abundances of
the 4 species during resistance, during early recovery, and during late recovery.

Note that I also had to keep just 3 time points from the control treatment. I chose to keep day 1 (coded as "resistance"), day 3 (coded as "early recovery"), and day 5 (coded as "late recovery") because this way the ordination plot will show the control treatment early, middle, and late in the time series... 

```{r, NMDS_create_wide_matrix}
# go back to the complete data that includes NA values for all 4 species on some days
absDen_forOrd <- absDen_forFit %>% select(-Total_density, -Diversity, -community_expected_mu, -community_averaged_mu)
# NA values with Total_density == NA are "true" missing data where I failed to record the flow cytometry measurements on that day due to technical difficulties/mistakes. These can be interpolated by using the median values from the remaining community replicates
  ## get the median values for all communities, days, and heat treatments
median_vals <- absDen_forOrd %>% group_by(Heat, Day, community) %>%
                  summarise(Med_putida = median(Conc_putida, na.rm=TRUE),
                            Med_protegens = median(Conc_protegens, na.rm=TRUE),
                            Med_grimontii = median(Conc_grimontii, na.rm=TRUE),
                            Med_veronii = median(Conc_veronii, na.rm=TRUE))
  ## get the index for the rows with "true" missing values
missing_rows <- which(is.na(absDen_forOrd$TotDen_plusEpsilon))
  ## loop through the missing values
for(i in missing_rows){
  # find the interpolation value in the table of median values
  temp_med_val <- median_vals[median_vals$Heat == absDen_forOrd$Heat[i] &
                                median_vals$Day == absDen_forOrd$Day[i] &
                                median_vals$community == absDen_forOrd$community[i],]
  # replace the NA values with the median values
  absDen_forOrd$Conc_putida[i] <- temp_med_val$Med_putida
  absDen_forOrd$Conc_protegens[i] <- temp_med_val$Med_protegens
  absDen_forOrd$Conc_grimontii[i] <- temp_med_val$Med_grimontii
  absDen_forOrd$Conc_veronii[i] <- temp_med_val$Med_veronii
  # clean up
  rm(temp_med_val)
}
# clean up
rm(median_vals, missing_rows, i)

# on the other hand, NA values where Total_density is epsilon represent flow cytometry counts that were below the threshold of detection. In this case let's assume 1:1 ratios of inoculated strains at a total density equal to epsilon.
epsilon <- min(absDen_forOrd$TotDen_plusEpsilon, na.rm=TRUE)
  ## get the index for the missing value rows below the threshold of detection
missing_rows <- which(is.na(absDen_forOrd$Conc_putida))
  ## CommRich NA values were supposed to indicate some differences but that doesn't really matter for us anymore
absDen_forOrd$CommRich <- absDen_forOrd$putida + absDen_forOrd$protegens + absDen_forOrd$grimontii + absDen_forOrd$veronii
for(i in missing_rows){
  # replace the NA values with epsilon divided by the inoculated species richness
  absDen_forOrd$Conc_putida[i] <- absDen_forOrd$putida[i] * epsilon / absDen_forOrd$CommRich[i]
  absDen_forOrd$Conc_protegens[i] <- absDen_forOrd$protegens[i] * epsilon / absDen_forOrd$CommRich[i]
  absDen_forOrd$Conc_grimontii[i] <- absDen_forOrd$grimontii[i] * epsilon / absDen_forOrd$CommRich[i]
  absDen_forOrd$Conc_veronii[i] <- absDen_forOrd$veronii[i] * epsilon / absDen_forOrd$CommRich[i]
}
# re-order the levels of Heat for better plotting
absDen_forOrd$Heat <- factor(absDen_forOrd$Heat, levels=c("control", "6", "12", "24", "48"))
# finally we can drop the TotDen_plusEpsilon column
absDen_forOrd <- absDen_forOrd %>% select(-TotDen_plusEpsilon)
rm(epsilon, missing_rows, i)

# first we have to widen the data:
# create a column that indicates the treatment day as resistance, early recovery, or late recovery
absDen_forOrd$trtmt_day <- "resist"
absDen_forOrd$trtmt_day[absDen_forOrd$Recov_Day == 1] <- "early_recov"
absDen_forOrd$trtmt_day[absDen_forOrd$Recov_Day == 2] <- "late_recov"
# ENTIRELY ARBITARARILY: I will keep days 1, 3, and 5 for control
absDen_forOrd$trtmt_day[absDen_forOrd$Heat == "control" & absDen_forOrd$Day == 3] <- "early_recov"
absDen_forOrd$trtmt_day[absDen_forOrd$Heat == "control" & absDen_forOrd$Day == 5] <- "late_recov"
# remove day 1 for 12h, 24h, 48h AND day 2 for 48h.
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == 12 & absDen_forOrd$Day == 1), ]
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == 24 & absDen_forOrd$Day == 1), ]
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == 48 & absDen_forOrd$Day == 1), ]
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == 48 & absDen_forOrd$Day == 2), ]
# also remove day 2 and day 4 for control.
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == "control" & absDen_forOrd$Day == 2), ]
absDen_forOrd <- absDen_forOrd[!(absDen_forOrd$Heat == "control" & absDen_forOrd$Day == 4), ]

# pivot wider to create a column for each of the 4 species on each of the 3 days
absDen_wide_forOrd <- absDen_forOrd %>% select(-Day, -Heat_Day, -Recov_Day) %>%
                          pivot_wider(names_from = trtmt_day,
                                      values_from = c(Conc_putida, Conc_protegens, Conc_grimontii, Conc_veronii))

# re-name the species abundance over time columns so they are shorter (again for better plotting)
colnames(absDen_wide_forOrd)[9:20] <- c("Pu_Resist", "Pu_earlyR", "Pu_lateR",
                                        "Pt_Resist", "Pt_earlyR", "Pt_lateR",
                                        "Gi_Resist", "Gi_earlyR", "Gi_lateR",
                                        "Vn_Resist", "Vn_earlyR", "Vn_lateR")

```
```{r, NMDS_analysis_plots_signif}
# keep just the species abundances
abundance_matrix <- as.matrix(absDen_wide_forOrd[,9:20])

# a function to automatically run the NMDS for k = 1 to 10 so we can choose appropriately small number of dimensions for ordination
NMDS.scree <- function(mat) { #where x is the abundance matrix
  data.frame(k = 1:10,
            # autotransform the data before calculating the bray-curtis dissimilarity
            stress = sapply(1:10, function(x) metaMDS(mat, distance = "bray", k = x, autotransform = TRUE)$stress))
}
scree_out <- NMDS.scree(abundance_matrix)
plot(scree_out)
# k=3 looks great
try.NMDS <- metaMDS(abundance_matrix, distance = "bray", k = 3, autotransform = TRUE, trymax=100)

# check the stress value. It should be < 0.2, ideally even < 0.05. (But too low stress values can indicate too many 0 values)
try.NMDS$stress

# let's get a general idea of what this NMDS is separating...

# plot the results for axis 1 & 2
ordiplot(try.NMDS, type = "n") # create blank ordination plot
orditorp(try.NMDS, display = "sites", cex = 0.5, air = 0.1) # add row numbers in black
orditorp(try.NMDS, display = "species", col="red", air = 0.1) # add species names in red

# plot the results for axis 1 & 3
ordiplot(try.NMDS, choices = c(1,3), type = "n") # create blank ordination plot
orditorp(try.NMDS, choices = c(1,3), display = "sites", cex = 0.5, air = 0.1) # add row numbers in black
orditorp(try.NMDS, choices = c(1,3), display = "species", col="red", air = 0.1) # add species names in red

# plot the results for axis 2 & 3
ordiplot(try.NMDS, choices = c(2,3), type = "n") # create blank ordination plot
orditorp(try.NMDS, choices = c(2,3), display = "sites", cex = 0.5, air = 0.1) # add row numbers in black
orditorp(try.NMDS, choices = c(2,3), display = "species", col="red", air = 0.1) # add species names in red


# we already know that presense/absence of protegens is consistently the most important thing for all communities so let's see if that shows up here.
# Let's switch over to ggplot to be certain that everything is labelled correctly.

# define a function (related to vegan) that finds coordinates for drawing a covariance ellipse
  # CREDIT: THIS COMES FROM ONE OF THE TUTORIALS ABOVE!!!
veganCovEllipse <- function (cov, center = c(0, 0), scale = 1, npoints = 100) {
  theta <- (0:npoints) * 2 * pi/npoints
  Circle <- cbind(cos(theta), sin(theta))
  t(center + scale * t(Circle %*% chol(cov)))
  # finds the centroids and dispersion of the different ellipses based on a grouping factor of your choice
}

nmds_for_ggplot <- cbind(absDen_wide_forOrd[,1:8],
                         as.data.frame(scores(try.NMDS)$sites))
# create a new factor that defines the combination of heat and protegens
nmds_for_ggplot <- nmds_for_ggplot %>% unite("HeatxProtegens", c(Heat, protegens), remove = FALSE)
nmds_for_ggplot$HeatxProtegens <- factor(nmds_for_ggplot$HeatxProtegens,
                                         levels = c("6_0", "6_1", "12_0", "12_1", "24_0", "24_1", "48_0", "48_1", "control_0", "control_1"))



# create empty dataframes to combine NMDS data with ellipse data
ellipse12_df <- ellipse13_df <- ellipse23_df <- data.frame() # numbers indicate the ordination axes
# adding data for ellipses, using HeatxProtegens as a grouping factor
for(g in levels(nmds_for_ggplot$HeatxProtegens)){
  ellipse12_df <- rbind(ellipse12_df, cbind(as.data.frame(with(nmds_for_ggplot[nmds_for_ggplot$HeatxProtegens==g,],
                                                     veganCovEllipse(cov.wt(cbind(NMDS1, NMDS2),
                                                                            wt=rep(1/length(NMDS1),length(NMDS1)))$cov,
                                                                     center=c(mean(NMDS1),mean(NMDS2)))))
                                  , HeatxProtegens=g))
  ellipse13_df <- rbind(ellipse13_df, cbind(as.data.frame(with(nmds_for_ggplot[nmds_for_ggplot$HeatxProtegens==g,],
                                                     veganCovEllipse(cov.wt(cbind(NMDS1, NMDS3),
                                                                            wt=rep(1/length(NMDS1),length(NMDS1)))$cov,
                                                                     center=c(mean(NMDS1),mean(NMDS3)))))
                                  , HeatxProtegens=g))
  ellipse23_df <- rbind(ellipse23_df, cbind(as.data.frame(with(nmds_for_ggplot[nmds_for_ggplot$HeatxProtegens==g,],
                                                     veganCovEllipse(cov.wt(cbind(NMDS2, NMDS3),
                                                                            wt=rep(1/length(NMDS2),length(NMDS2)))$cov,
                                                                     center=c(mean(NMDS2),mean(NMDS3)))))
                                  , HeatxProtegens=g))
}

# now we separate the HeatxProtegens columns:
ellipse12_df <- ellipse12_df %>% separate(HeatxProtegens, c("Heat", "protegens"))
ellipse12_df$Heat <- factor(ellipse12_df$Heat, levels = levels(nmds_for_ggplot$Heat))
ellipse13_df <- ellipse13_df %>% separate(HeatxProtegens, c("Heat", "protegens"))
ellipse13_df$Heat <- factor(ellipse13_df$Heat, levels = levels(nmds_for_ggplot$Heat))
ellipse23_df <- ellipse23_df %>% separate(HeatxProtegens, c("Heat", "protegens"))
ellipse23_df$Heat <- factor(ellipse23_df$Heat, levels = levels(nmds_for_ggplot$Heat))

nmds_for_ggplot$protegens <- as.character(nmds_for_ggplot$protegens) # this needs to be discrete (could also be a factor)

# and finally we can make the plots:
ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS2)) +
    geom_point(aes(color = Heat, shape = protegens), alpha=0.4) + # adding different colours and shapes for points at different distances
    geom_path(data=ellipse12_df, aes(x=NMDS1, y=NMDS2, colour=Heat, linetype=protegens), linewidth=1) + # adding covariance ellipses according to distance # use size argument if ggplot2 < v. 3.4.0
    guides(color = guide_legend(override.aes = list(linetype=rep(NA,5)))) + # removes lines from colour part of the legend
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + # not sure why I need this but I do to over-write the default grey theme
    labs(title="NMDS of all data (4sp & 3 time-points)")
# axes 1 & 2 again showing just the ellipses (bc it's hard to see protegens effects as it's so overlapped)
ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS2)) +
    geom_path(data=ellipse12_df, aes(x=NMDS1, y=NMDS2, colour=Heat, linetype=protegens), linewidth=1) + # plot just the ellipses
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + # not sure why I need this but I do to over-write the default grey theme
    labs(title="NMDS of all data (4sp & 3 time-points)")


ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS3)) +
    geom_point(aes(color = Heat, shape = protegens), alpha=0.4) + 
    geom_path(data=ellipse13_df, aes(x=NMDS1, y=NMDS3, colour=Heat, linetype=protegens), linewidth=1) + 
    guides(color = guide_legend(override.aes = list(linetype=rep(NA,5)))) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + 
    labs(title="NMDS of all data (4sp & 3 time-points)")
# axes 1 & 3 again showing just the ellipses
ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS3)) +
    geom_path(data=ellipse13_df, aes(x=NMDS1, y=NMDS3, colour=Heat, linetype=protegens), linewidth=1) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + 
    labs(title="NMDS of all data (4sp & 3 time-points)")

ggplot(data = nmds_for_ggplot, aes(NMDS2, NMDS3)) +
    geom_point(aes(color = Heat, shape = protegens), alpha=0.4) + 
    geom_path(data=ellipse23_df, aes(x=NMDS2, y=NMDS3, colour=Heat, linetype=protegens), linewidth=1) + 
    guides(color = guide_legend(override.aes = list(linetype=rep(NA,5)))) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + 
    labs(title="NMDS of all data (4sp & 3 time-points)")
# axes 2 & 3 again showing just the ellipses (bc it's hard to see protegens effects as it's so overlapped)
ggplot(data = nmds_for_ggplot, aes(NMDS2, NMDS3)) +
    geom_path(data=ellipse23_df, aes(x=NMDS2, y=NMDS3, colour=Heat, linetype=protegens), linewidth=1) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    fave_theme + 
    labs(title="NMDS of all data (4sp & 3 time-points)")

################
# check significance:
# using a PERMANOVA to test the differences in community composition
# This is a PERmutational Multivariate ANalysis Of VAriance and tests the differences between groups, like an ANOVA, but with lots of variables.
# it is essentially a multivariate analysis of variance used to compare groups of objects
nmdsdata_test_Heat <- adonis2(abundance_matrix ~ Heat, absDen_wide_forOrd,
                              permutations = 999, method = "bray")
print(nmdsdata_test_Heat)

nmdsdata_test_Prot <- adonis2(abundance_matrix ~ protegens, absDen_wide_forOrd,
                              permutations = 999, method = "bray")
print(nmdsdata_test_Prot)

nmdsdata_test_HeatxProt <- adonis2(abundance_matrix ~ Heat * protegens, absDen_wide_forOrd,
                                   permutations = 999, method = "bray")
print(nmdsdata_test_HeatxProt)

# so these are all significant but is that spurious because the dispersion is different btw groups? (e.g., much smaller for protegens)

##############
# check PERMANOVA assumption of homogeneous group variances
# Bray-curtis distance matrix
dist_mat <- vegdist(abundance_matrix, method = "bray")

# use betadisper test to check for multivariate homogeneity of group variances
dispersion <- betadisper(dist_mat, group = paste(absDen_wide_forOrd$Heat, absDen_wide_forOrd$protegens))
permutest(dispersion)

# yeap! We need to try a different test that is robust to heterogenous group variances...

################
# check significance:
# let's test for significance again using ANOSIM (which is another non-parametric test but this time only considering the ranks)
nmdsdata_test2_HeatxProt <- anosim(dist_mat,
                                   grouping = paste(absDen_wide_forOrd$Heat, absDen_wide_forOrd$protegens),
                                   permutations = 999)
plot(nmdsdata_test2_HeatxProt)
summary(nmdsdata_test2_HeatxProt)

# okay, let's say that we are satisfied with this significance testing...

test <- absDen_wide_forOrd
test$Heat <- as.character(levels(test$Heat))[test$Heat]
test$Heat[test$Heat == "control"] <- 0
test$Heat <- as.numeric(test$Heat)

# let's see what the heat gradient looks like
gg_ordiplot(try.NMDS, groups = absDen_wide_forOrd$protegens, plot = TRUE)
gg_envfit(try.NMDS, env = test$Heat, groups = absDen_wide_forOrd$protegens, plot = TRUE, alpha=0.5) # notice this gradient is not significant!!!
gg_envfit(try.NMDS, env = test$Heat, groups = absDen_wide_forOrd$protegens, plot = TRUE, alpha=0.5, choices=c(1,3))
gg_envfit(try.NMDS, env = test$Heat, groups = absDen_wide_forOrd$protegens, plot = TRUE, alpha=0.5, choices=c(2,3))
```

Great!, This summarizes the same result that I found with the other indices: presence of P.protegens is the most important thing. Communities where this species was present look quite similar across different heat treatments. Longer heat durations push the communities toward different direction, until a threshold is reached at the longest heat treatment (48h).

The NMDS ordination results are significant by PERMANOVA but the assumptions of that test might be violated because the dispersal is heterogeneous between groups. I think ANOSIM should be somewhat more robust to this problem because it uses ranks. The NMDS ordination results are significant by ANOSIM.

```{r, NMDS_plot_for_pub}
################################
# Plot figure for main text: Figure 3b
################################
# change protegens values for better plotting
nmds_for_ggplot$P_protegens <- "absent"
nmds_for_ggplot$P_protegens[nmds_for_ggplot$protegens == 1] <- "present"

ellipse12_df$P_protegens <- "absent"
ellipse12_df$P_protegens[ellipse12_df$protegens == 1] <- "present"

ellipse13_df$P_protegens <- "absent"
ellipse13_df$P_protegens[ellipse13_df$protegens == 1] <- "present"

# change Heat values for better plotting
levels(nmds_for_ggplot$Heat)[2:5] <- paste(levels(nmds_for_ggplot$Heat)[2:5], "hrs")
levels(ellipse12_df$Heat)[2:5] <- paste(levels(ellipse12_df$Heat)[2:5], "hrs")
levels(ellipse13_df$Heat)[2:5] <- paste(levels(ellipse13_df$Heat)[2:5], "hrs")

# create the plot of 1 vs 2:
plot1_2 <- ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS2)) +
            geom_point(aes(color = Heat, shape = P_protegens), size=2, alpha=0.4) + # adding different colours and shapes for points at different distances
            geom_path(data=ellipse12_df, aes(x=NMDS1, y=NMDS2, colour=Heat, linetype=P_protegens), linewidth=1) + # adding covariance ellipses according to distance # use size argument if ggplot2 < v. 3.4.0
            guides(color = guide_legend(override.aes = list(linetype=rep(NA,5),# removes lines from colour part of the legend
                                                  alpha=1, size=3)), # make the points opaque and bigger in the colour part of the legend
            shape = guide_legend(override.aes = list(size=3))) + # make the points bigger in the greyscale part of the legend
            scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9)

# plot 1 vs 2 with the legend ... I will extract the legend from here
png(filename="Fig3_A_legend.png", width = 3.48, height = 3.41, units = "in", res=300)
print(plot1_2)
dev.off()


# plot 1 vs 2 without the legend
png(filename="Fig3_A_axis1vs2.png", width = 5.35, height = 3.78, units = "in", res=300)
print(plot1_2 + theme(legend.position="none"))
dev.off()

# plot 1 vs 3 without the legend
png(filename="Fig3_A_axis1vs3.png", width = 5.35, height = 3.78, units = "in", res=300)

ggplot(data = nmds_for_ggplot, aes(NMDS1, NMDS3)) +
    geom_point(aes(color = Heat, shape = P_protegens), size=2, alpha=0.4) + 
    geom_path(data=ellipse13_df, aes(x=NMDS1, y=NMDS3, colour=Heat, linetype=P_protegens), linewidth=1) + 
    scale_colour_viridis_d(option = "plasma", begin=0.05, end = 0.9) +
    theme(legend.position="none")

dev.off()

# clean up
rm(abundance_matrix, scree_out, try.NMDS, nmds_for_ggplot, ellipse12_df, ellipse13_df, ellipse23_df, nmdsdata_test_Heat, nmdsdata_test_Prot, nmdsdata_test_HeatxProt, dist_mat, dispersion, nmdsdata_test2_HeatxProt, test, absDen_forOrd, absDen_wide_forOrd, g)
```

# Richness

Let's summarize the main result that P. protegens dominates all communities where it was inoculated. The species richness is conceptually a good metric for this ... but recall that the flow cytometry data has a some rate of misclassification (in some cases as much as 20% !!!). So we need to use richness estimates that take into account the proportion of species and are more likely to ignore rare species.

## Plot

```{r, richness}
species_div.df <- absDen_forFit %>% mutate(relden_putida = Conc_putida/Total_density,
                                 relden_protegens = Conc_protegens/Total_density,
                                 relden_grimontii = Conc_grimontii/Total_density,
                                 relden_veronii = Conc_veronii/Total_density)
species_div.df <- species_div.df %>% mutate(HillEven_q0 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillEven",
        q=0)),
                              HillEven_q1 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillEven",
        q=1)),
        HillEven_q2 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillEven",
        q=2)),
        HillDiv_q1 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillDiv",
        q=1)),
        HillDiv_q2 = unlist(calcDiv(sampleData = species_div.df[,c("relden_putida","relden_protegens","relden_grimontii","relden_veronii")],
        type = "HillDiv",
        q=2)))

ggplot(species_div.df,
       aes(y=HillEven_q0, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis", begin=0.1)

ggplot(species_div.df,
       aes(y=HillEven_q1, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis", begin=0.1)

ggplot(species_div.df,
       aes(y=HillEven_q2, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis", begin=0.1)

ggplot(species_div.df,
       aes(y=HillDiv_q1, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis")

ggplot(species_div.df,
       aes(y=HillDiv_q2, x=Day, colour=as.factor(CommRich))) +
  facet_grid(protegens~as.factor(Heat)) +
  geom_point(alpha=0.5) +
  scale_colour_viridis_d(option = "viridis")
# this one gives infinite values. That's not useful.


################################
# Plot figure for main text: Figure 3b
################################

# The Hill Diversity with q=1 seems useful!! Let's include this plot in the final manuscript:
# re-order  the levels of heat so that the control appears first
species_div.df$Heat <- factor(species_div.df$Heat,
                              levels = levels(species_div.df$Heat)[c(5,1:4)])
# change variable names for nicer plotting
levels(species_div.df$Heat)[2:5] <- paste(levels(species_div.df$Heat)[2:5], "hrs")
species_div.df$Pprot_facet <- ifelse(species_div.df$protegens == 0, "P. protegens absent", "P. protegens present")

# create a data.frame for plotting red rectangles in the background
bckgrd <- data.frame(Heat=levels(species_div.df$Heat),
                     HillDiv_q1 = c(0, rep(2.4, 4)),
                     Day = c(0, rep(0.8, 4))) # all heat treatments start at the same time
test <- rbind(bckgrd,
              data.frame(Heat=levels(species_div.df$Heat),
                         HillDiv_q1 = c(0, rep(2.4, 4)),
                         Day = c(0, 1.1, 1.5, 1.9, 2.9))) # choose end points that look good even if not perfectly accurate

png(filename = "Fig3_B.png", width = 6.98, height = 4.52, units = "in", res = 300)

ggplot(species_div.df %>% filter(CommRich > 1),
        # exclude monocultures bc richness is not informative for these: their richness will always be equal to 1 even when they go extinct. 
       aes(y=HillDiv_q1, x=Day)) +
  facet_grid(Pprot_facet ~ as.factor(Heat),
             scales = "free_x", # allow x-axis of facets to freely choose their own max values
             space = "free_x") + # allow facet columns to differ in their sizes
  # add red rectangles in the background to indicate heat treatment as in Fig 1.
  geom_ribbon(data=test, aes(ymin=0.95, ymax=2.4), # add a bit of padding above & below the points to look nice
              position = "identity", # not sure this is needed now that I've switched away from geom_area? But it works so I don't care
              fill="#C43131", alpha=0.25) + # use same colour and alpha as in fill for Fig. 1
  # use beeswarm to jitter the points properly (alpha must be set to 0 bc of red rectangles)
  geom_quasirandom(aes(fill=as.factor(CommRich)), # fill as a function of CommRich *must* be inside of this function otherwise it leads to a lot of problems with the geom_ribbon layer
                   pch=21) + # use points with fill and border bc they look nicer here
  scale_fill_viridis_d(option = "viridis", begin=0.85, end=0) +
  scale_x_continuous(breaks=1:5, # tick marks & tick labels only at integers
                     limits=c(0.5,NA), # add a little extra padding on the left side of x-axis bc I think it looks better
                     expand = c(0, 0.2)) + # for some reason this prevents points in the 6h facet from getting squished up against the right border of the facet
  scale_y_continuous(expand = c(0, 0)) + # this prevents ggplot from adding any extra padding in the y-axis; we already added the defauly +/- 0.05 padding manually when we specified geom_ribbon
  labs(y = "Observed Richness", # 1st order Hill Diversity
       fill = "Inoculated\nRichness") +
  guides(fill = guide_legend(override.aes = list(size=3), # make the points bigger in the legend
                             title.hjust = 0.5)) + # justify the 2nd line of legend title & points so they sit nicely under the 1st line
  theme(strip.background = element_rect(fill = "white", colour="black"),
        strip.text = element_text(color = "black"),
        legend.box.spacing = margin(0.5), # stretch plot rightwards & closer to its legend
        legend.title = element_text(size=14)) # make legend title a little smaller

dev.off()

# clean up
rm(species_div.df, bckgrd, test)
```

Notice that for the control condition in the absence of protegens, there is a trend of decreasing species richness over time (e.g., as the communities reach equilibrium).

( ## Analysis?? )

I have effect sizes on richness for resistance and recovery but I'm not sure if this is going to make it into the manuscript... (see the `analyze_temp_serial_transfer_expt--28Oct24.html` if you're interested).

# Extinction

Which communities were most likely to go extinct? How long did the heat duration have to be in order to drive those communities to extinction?

The simplest hypothesis is that heat duration alone explains whether a community happens to go extinct.

A slightly more complex hypothesis from the thermal performance curve data (Fig. 2) would
be that any species that is resistant to heat should be less likely to go extinct, even long duration heat. Therefore we would expect that the presence/absence of the heat resistant species, P. putida, should explain whether a community goes extinct.

Maddy's hypothesis in setting up this experiment was that a higher inoculated species richness would make a community more resistant to heat. So we are going to check whether the inoculated species richness has any effect.

Another hypothesis that emerges from looking at the time series data itself (e.g., the ordination data) is that protegens has a unique effect on all communities where it is present. So let's check that model as well.

## Analysis

```{r, extinction_InnocRichness}
# keep just the data on the last day of each time series
extinct.df <- absDensity %>% filter(Recov_Day == 2, CommRich > 0)
extinct.df <- rbind(extinct.df,
                    absDensity %>% filter(Heat == 0, Day == 5, CommRich > 0))
# binary vector of survival or extinction
extinct.df <- extinct.df %>% mutate(survived = ifelse(Total_density > 0, 1, 0))
### note that sample "24-07-08 Epoch G1" has missing data on Day 5 even though we know from the OD data that it survived.
extinct.df$survived[extinct.df$uniqID == "24-07-08 Epoch G1"] <- 1

# make protegens into a factor
extinct.df$protegens <- factor(extinct.df$protegens)

# fit the models
ext_mod.heat <- glmmTMB(survived ~ Heat,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat, plot = TRUE)

ext_mod.heat_plus_rich <- glmmTMB(survived ~ CommRich + Heat,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_plus_rich, plot = TRUE)

ext_mod.heat_plus_prot <- glmmTMB(survived ~ Heat + protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = ext_mod.heat_plus_prot, plot = TRUE)

ext_mod.heat_plus_putida <- glmmTMB(survived ~ Heat + putida,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_plus_putida, plot = TRUE)


ext_mod.heat_by_rich <- glmmTMB(survived ~ CommRich*Heat,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_by_rich, plot = TRUE)

ext_mod.heat_by_prot <- glmmTMB(survived ~ Heat*protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_by_prot, plot = TRUE)


ext_mod.heat_rich_prot <- glmmTMB(survived ~ CommRich + Heat + protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.heat_rich_prot, plot = TRUE)

ext_mod.rich_heatXprot <- glmmTMB(survived ~ CommRich + Heat*protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.rich_heatXprot, plot = TRUE)

#anova(ext_mod.heat, ext_mod.heat_plus_rich)
#anova(ext_mod.heat, ext_mod.heat_by_rich)
#anova(ext_mod.heat_plus_rich, ext_mod.heat_by_rich)

AIC(ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_putida) %>% arrange(AIC)
BIC(ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_putida) %>% arrange(BIC)

summary(ext_mod.heat_rich_prot)
summary(ext_mod.heat_plus_prot)

anova(ext_mod.heat, ext_mod.heat_plus_prot)
anova(ext_mod.heat_plus_prot, ext_mod.heat_rich_prot)

# and let's report the R-squared for this 
efronRSquared(residual = residuals(ext_mod.heat_plus_prot, type="response"), 
              predicted = predict(ext_mod.heat_plus_prot, type="response"), 
              statistic = "EfronRSquared")
```

This tells us that the most important predictors are: 1. the duration of the heat event and 2. the presence/absence of protegens in the inoculated community. This model explains about 50% of the variation in the data. We have little power to detect an effect of inoculated community richness on the extinction.

A final possibility is that the growth rates of the different communities can explain whether they go extinct. Let's check if the average growth rate of the community at 30C can predict its extinction...

```{r, extinction_InnocRichness}
ext_mod.expect_mu <- glmmTMB(survived ~ Heat + community_expected_mu,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.expect_mu, plot = TRUE)

ext_mod.exptmu_prot <- glmmTMB(survived ~ Heat + community_expected_mu + protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.exptmu_prot, plot = TRUE)

ext_mod.averaged_mu <- glmmTMB(survived ~ Heat + community_averaged_mu,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
#simulateResiduals(fittedModel = ext_mod.averaged_mu, plot = TRUE)

ext_mod.avemu_prot <- glmmTMB(survived ~ Heat + community_averaged_mu + protegens,
                          data = extinct.df,
                          family = binomial,
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = ext_mod.avemu_prot, plot = TRUE)

summary(ext_mod.exptmu_prot)
summary(ext_mod.avemu_prot)

AIC(ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_putida, ext_mod.expect_mu, ext_mod.averaged_mu, ext_mod.exptmu_prot, ext_mod.avemu_prot) %>% arrange(AIC)
BIC(ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.heat_plus_putida, ext_mod.expect_mu, ext_mod.averaged_mu, ext_mod.exptmu_prot, ext_mod.avemu_prot) %>% arrange(BIC)

# wow, I'm shocked that this growth rate model is actually better. Let's double check that...
anova(ext_mod.heat_plus_prot, ext_mod.exptmu_prot)
anova(ext_mod.heat_plus_prot, ext_mod.avemu_prot)

# and let's report the R-squared
efronRSquared(residual = residuals(ext_mod.avemu_prot, type="response"), 
              predicted = predict(ext_mod.avemu_prot, type="response"), 
              statistic = "EfronRSquared")
```

Ok, so neither the community_expected_mu nor the community_averaged_mu are as good predictors as just heat duration and presence/absence of protegens. But when we add the presence/absence of protegens to the community_averaged_mu, we get a very good model that's significantly better than the one reported above. It explains almost 75% of the variation.

## Plot

Plot the best model against the data:

```{r, extinction_plot}
# create data.frame for plotting
extinct.df <- cbind(extinct.df,
                    predicted = predict(ext_mod.avemu_prot, type="response"))
# plot the predictions against the data
plot(ggplot(extinct.df,
        aes(x=as.factor(Heat),
            y=survived,
            colour=community_averaged_mu, 
            group=as.factor(community_averaged_mu))) +
    facet_wrap(. ~ protegens,
               labeller = as_labeller(c(`0`="P. protegens absent",
                                        `1`="P. protegens present"))) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_line(aes(y = predicted)) +
    geom_jitter(alpha=0.6, width=0.1, height = 0.25) +
    scale_y_continuous(breaks = c(0, 1)) +
    scale_colour_viridis_c(option = "inferno", end=0.85) +
    labs(x="Heat duration (hrs)",
         y="Growth in well on last day?", colour="Community\nAveraged\nGrowth Rate"))

# plot the effect sizes of the preferred model
extinct_forplot <- data.frame(confint(ext_mod.avemu_prot))
colnames(extinct_forplot)[1:2] <- c("loCI", "hiCI")
extinct_forplot$predictor <- as.factor(rownames(extinct_forplot))
# protegens effect size is not significant and has giant CI that obscure other estimates
ggplot(extinct_forplot,
       aes(x = Estimate, y = predictor)) +
  geom_vline(xintercept = 0, colour="grey") +
  geom_point() +
  geom_errorbarh(aes(xmin = loCI, xmax = hiCI), height=0)

# plot the effect sizes again without protegens
ggplot(extinct_forplot %>% filter(predictor != "protegens1"),
       aes(x = Estimate, y = predictor)) +
  geom_vline(xintercept = 0, colour="grey") +
  geom_point() +
  geom_errorbarh(aes(xmin = loCI, xmax = hiCI), height=0)

# clean up
rm(extinct.df, ext_mod.heat, ext_mod.heat_plus_rich, ext_mod.heat_plus_putida, ext_mod.heat_plus_prot, ext_mod.heat_by_rich, ext_mod.heat_by_prot, ext_mod.heat_rich_prot, ext_mod.rich_heatXprot, ext_mod.expect_mu, ext_mod.averaged_mu, ext_mod.exptmu_prot, ext_mod.avemu_prot, extinct_forplot)
```

# Shannon diversity

How is community diversity impacted during and after heat? Here we will have to be mindful to control for inoculated community richness as a nuisance variable (i.e., because we will always expect to see (less) more diversity in communities that were inoculated with more (less) species. But this is just part of our experimental design; we're not interested in this effect per se).

## Plot

Let's first plot the Shannon diversity directly because we 

## Analysis

# Productivity

## Plot

## Analysis

# IGNORE EVERYTHING BELOW

# Diversity stats

The simplest hypothesis is that heat duration, inoculated richness, and their interaction
explain the diversity during recovery and resistance.

A slightly more complex hypothesis from the thermal performance curve data (Fig. 1) would
be that any species that can withstand heat should have higher productivity during heat,
even long duration heat. Therefore, I created another model where I replaced only `putida`
with the predictor `withstands_heat` (because this is the information we know from Fig. 1)
and also kept heat duration and inoculated richness as predictors, as well as their
interaction.

Finally, a hypothesis that emerges from looking at the time series data itself is that
putida is a strong competitor. This is a similar type of model as "withstands heat" model
above, but for the presence of `protegens`.

## Simple effect size

```{r, diversity}
#### EVERYTHING BELOW THIS LINE WAS ALREADY COPIED UP ABOVE
absDensity$Diversity <- diversity(absDensity[,c("Conc_putida", "Conc_protegens", "Conc_grimontii", "Conc_veronii")])

# first let's remove the empty wells as we won't need them anymore
absDensity <- absDensity %>% filter(community != "0_0_0_0")
# Note that there are many 0 and NA values for Total_density
summary(absDensity$Total_density)
# 0's are communities that went extinct altogether and never recovered
absDensity[which(absDensity$Total_density == 0),]
# most NA's are communities below the threshold of detection during heat that later perhaps recovered
absDensity[which(is.na(absDensity$Total_density) & absDensity$Heat>12),]
# other NA's are just missing data (e.g., due to flow cytometry clogs or just plain pipetting mistakes)
absDensity[which(is.na(absDensity$Total_density) & absDensity$Heat<12),]

# the total density data will have to be slightly adjusted for fitting to the models
absDen_forFit <- absDensity %>% filter(Day > 0)
# for the "raw" total density data that will be fitted via negative binomial GLM,
  # keep the 0's in the data
  # but convert NA's into epsilon values (where epsilon is just below the threshold of detection)
below_threshold_rows <- which(is.na(absDen_forFit$Total_density) & absDen_forFit$Heat>12)
absDen_forFit$Total_density[below_threshold_rows] <- (0.25*50/146)
# for the transformed total density data, apply (x + epsilon) transformation to all values EXCEPT those that used to be NA's
absDen_forFit$TotDen_plusEpsilon <- absDen_forFit$Total_density
absDen_forFit$TotDen_plusEpsilon[-below_threshold_rows] <- absDen_forFit$TotDen_plusEpsilon[-below_threshold_rows] + (0.25*50/146)
rm(below_threshold_rows)
#### EVERYTHING ABOVE THIS LINE WAS ALREADY COPIED UP ABOVE

plot(ggplot(absDensity %>% filter(Day!=0,
                                  !is.na(CommRich)),
     aes(x=Day,
         y=Diversity,
         colour=as.factor(CommRich), 
         group=paste(uniqID,Heat))) +
    facet_grid(~Heat) +
    geom_point(alpha=0.2) +
    geom_line(alpha=0.9) +
      scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="All data", y="Shannon Diversity",
         colour="Inoculated\nRichness"))

# this is not very useful to look at. Let's convert to effect size
control_trtmt <- absDen_forFit %>% filter(Heat==0) %>%
                   select(-uniqID, -Heat, -putida, -protegens, -grimontii, -veronii) %>%
                      rename(ctrl_Total_density = Total_density,
                             ctrl_TotDen_plusEpsilon = TotDen_plusEpsilon,
                             ctrl_Conc_putida = Conc_putida,
                             ctrl_Conc_protegens = Conc_protegens,
                             ctrl_Conc_grimontii = Conc_grimontii,
                             ctrl_Conc_veronii = Conc_veronii,
                             ctrl_Diversity = Diversity)
effectSize <- left_join(absDen_forFit %>% filter(Heat!=0),
                        control_trtmt,
                        by=c("Day", "community")) %>%
                mutate(Total_density_frac = Total_density / ctrl_Total_density,
                       TotDen_plusEpsilon_frac = TotDen_plusEpsilon / ctrl_TotDen_plusEpsilon,
                       Conc_putida_frac = Conc_putida / ctrl_Conc_putida,
                       Conc_protegens_frac = Conc_protegens / ctrl_Conc_protegens,
                       Conc_grimontii_frac = Conc_grimontii / ctrl_Conc_grimontii,
                       Conc_veronii_frac = Conc_veronii / ctrl_Conc_veronii,
                       Diversity_diff = Diversity - ctrl_Diversity) %>%
                    group_by(Day, Heat, community, uniqID) %>%
                      summarise(Total_density_mean = mean(Total_density_frac, na.rm = TRUE),
                                Total_density_sd = sd(Total_density_frac, na.rm = TRUE),
                                TotDen_plusEpsilon_mean = mean(TotDen_plusEpsilon_frac, na.rm = TRUE),
                                TotDen_plusEpsilon_sd = sd(TotDen_plusEpsilon_frac, na.rm = TRUE),
                                Conc_putida_mean = mean(Conc_putida_frac, na.rm = TRUE),
                                Conc_putida_sd = sd(Conc_putida_frac, na.rm = TRUE),
                                Conc_protegens_mean = mean(Conc_protegens_frac, na.rm = TRUE),
                                Conc_protegens_sd = sd(Conc_protegens_frac, na.rm = TRUE),
                                Conc_grimontii_mean = mean(Conc_grimontii_frac, na.rm = TRUE),
                                Conc_grimontii_sd = sd(Conc_grimontii_frac, na.rm = TRUE),
                                Conc_veronii_mean = mean(Conc_veronii_frac, na.rm = TRUE),
                                Conc_veronii_sd = sd(Conc_veronii_frac, na.rm = TRUE),
                                Diversity_mean = mean(Diversity_diff, na.rm = TRUE),
                                Diversity_sd = sd(Diversity_diff, na.rm = TRUE))
rm(control_trtmt)
# add the annotation back to the data
effectSize <- full_join(effectSize,
                        annotated.rawdata %>%
                          unite("community", putida:veronii, remove = FALSE) %>%
                            ungroup() %>% select(Heat, Day:veronii) %>%
                              filter(Day!=0) %>% distinct(),
                        by=c("community", "Heat", "Day")) %>%
                          filter(community != "0_0_0_0", Heat > 0)

# adjust the variable types for plotting and analysis
effectSize$Heat <- factor(effectSize$Heat, ordered = TRUE)
effectSize$putida <- as.logical(effectSize$putida)
effectSize$protegens <- as.logical(effectSize$protegens)
effectSize$grimontii <- as.logical(effectSize$grimontii)
effectSize$veronii <- as.logical(effectSize$veronii)
# create a column indicating the last heat day for each heat treatment
effectSize$Last_Heat_Day <- FALSE
effectSize$Last_Heat_Day[which(effectSize$Heat==6 & effectSize$Day==1)] <- TRUE
effectSize$Last_Heat_Day[which(effectSize$Heat==12 & effectSize$Day==2)] <- TRUE
effectSize$Last_Heat_Day[which(effectSize$Heat==24 & effectSize$Day==2)] <- TRUE
effectSize$Last_Heat_Day[which(effectSize$Heat==48 & effectSize$Day==3)] <- TRUE
# consider heat as an unordered factor
effectSize$Heat <- factor(effectSize$Heat, ordered = FALSE)
# create a column indicating presence of species that don't die during heat
effectSize$withstands_heat <- FALSE
effectSize$withstands_heat[which(effectSize$putida == TRUE)] <- TRUE

## now we can plot effect size of diversity
plot(ggplot(effectSize,
        aes(x=Day,
            y=Diversity_mean,
            colour=as.factor(CommRich), 
            group=community)) +
    geom_hline(yintercept = 0) +
    facet_grid(~Heat) +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    stat_summary(fun=mean, geom="line", alpha=0.9) +
      scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="All data", colour="Inoculated\nRichness",
         y="Effect size on Shannon Diversity"))

plot(ggplot(effectSize %>%
              filter(protegens==1),
        aes(x=Day,
            y=Diversity_mean,
            colour=as.factor(CommRich),
            group=community)) +
    geom_hline(yintercept = 0) +
    facet_grid(~Heat) +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    stat_summary(fun=mean, geom="line", alpha=0.9) +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Including protegens", colour="Inoculated\nRichness",
         y="Effect size on Shannon Diversity"))

plot(ggplot(effectSize %>%
              filter(protegens==0),
        aes(x=Day,
            y=Diversity_mean,
            colour=as.factor(CommRich),
            group=community)) +
    geom_hline(yintercept = 0) +
    facet_grid(~Heat) +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    stat_summary(fun=mean, geom="line", alpha=0.9) +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Excluding protegens", colour="Inoculated\nRichness",
         y="Effect size on Shannon Diversity"))

print("Which communities correspond to the outlier with increased diversity in the absense of protegens?")
effectSize[which(effectSize$CommRich > 1 & effectSize$protegens==0 & effectSize$Diversity_mean > 0),] %>% select(Day, Heat, community) %>% distinct %>% arrange(Heat, Day)

# RESISTANCE: let's check for statistically significant effect
effectSize$CommRich <- factor(effectSize$CommRich)
resist_divers0 <- with(effectSize %>% filter(Last_Heat_Day == TRUE),
                      lm(Diversity_mean ~ CommRich + Heat + CommRich:Heat))
# Maddy's preferred model
print("RESISTANCE DIVERSITY DATA. SUMMARY OF THE SIMPLEST MODEL:")
summary(resist_divers0)

resist_divers1 <- with(effectSize %>% filter(Last_Heat_Day == TRUE),
                      lm(Diversity_mean ~ CommRich + withstands_heat + Heat + CommRich:withstands_heat + Heat:withstands_heat + CommRich:Heat))

resist_divers2 <- with(effectSize %>% filter(Last_Heat_Day == TRUE),
                      lm(Diversity_mean ~ CommRich + protegens + Heat + CommRich:protegens + Heat:protegens + CommRich:Heat))

# let's compare the simpler model with the more complex one
anova(resist_divers0, resist_divers1)
anova(resist_divers0, resist_divers2)
AIC(resist_divers0, resist_divers1, resist_divers2) %>% arrange(AIC)
BIC(resist_divers0, resist_divers1, resist_divers2) %>% arrange(BIC)

# the statistically preferred model
print("")
print("RESISTANCE DIVERSITY DATA. SUMMARY OF THE LOWEST AIC & BIC MODEL:")
print("")
summary(resist_divers2)
plot(resist_divers2)

# plot Maddy's preferred model
plot(ggplot(effectSize %>%
              filter(Last_Heat_Day == TRUE),
        aes(x=Heat,
            y=Diversity_mean,
            colour=CommRich,
            group=CommRich)) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_jitter(alpha=0.4, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Resistance", x="Inoculated Richness",
         y="Effect size on Shannon Diversity", colour="Inoculated\nRichness"))

# plot the statistically preferred model
plot(ggplot(effectSize %>%
              filter(Last_Heat_Day == TRUE),
        aes(x=Heat,
            y=Diversity_mean,
            colour=CommRich, 
            group=CommRich)) +
    facet_grid(~putida) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_jitter(alpha=0.4, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Resistance (protegens present?)", x="Inoculated Richness",
         y="Effect size on Shannon Diversity", colour="Inoculated\nRichness"))

print("Which communities have missing values on the last day of heat?")
effectSize[which(is.na(effectSize$Diversity_mean) & effectSize$CommRich > 1),] %>% select(Day, Heat, community) %>% distinct %>% arrange(Heat, Day)

# clean up
rm(resist_divers0, resist_divers1, resist_divers2)
     
# RECOVERY: let's check for statistically significant effect
recov_divers0 <- with(effectSize %>% filter(Recov_Day == 2),
                      lm(Diversity_mean ~ CommRich + Heat + CommRich:Heat))
# summarize Maddy's preferred model
print("RECOVERY DIVERSITY DATA. SUMMARY OF THE SIMPLEST MODEL:")
summary(recov_divers0)

recov_divers1 <- with(effectSize %>% filter(Recov_Day == 2),
                      lm(Diversity_mean ~ CommRich + withstands_heat + Heat + CommRich:withstands_heat + Heat:withstands_heat + CommRich:Heat))

recov_divers2 <- with(effectSize %>% filter(Recov_Day == 2),
                      lm(Diversity_mean ~ CommRich + protegens + Heat + CommRich:protegens + Heat:protegens + CommRich:Heat))

# let's compare the simpler model with the more complex one
anova(recov_divers0, recov_divers1)
anova(recov_divers0, recov_divers2)
AIC(recov_divers0, recov_divers1, recov_divers2) %>% arrange(AIC)
BIC(recov_divers0, recov_divers1, recov_divers2) %>% arrange(BIC)

# summarize the statistically preferred model
print("")
print("RECOVERY DIVERSITY DATA. SUMMARY OF THE LOWEST AIC & BIC MODEL:")
print("")
summary(recov_divers2)
plot(recov_divers2)

# plot Maddy's preferred model
plot(ggplot(effectSize %>%
              filter(Recov_Day == 2),
        aes(x=Heat,
            y=Diversity_mean,
            colour=CommRich,
            group=CommRich)) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_jitter(alpha=0.4, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Recovery", colour="Inoculated\nRichness",
         y="Effect size on Shannon Diversity", x="Heat Duration (hrs)"))

# plot the preferred model
plot(ggplot(effectSize %>%
              filter(Recov_Day == 2),
        aes(x=Heat,
            y=Diversity_mean,
            colour=CommRich,
            group=CommRich)) +
    facet_grid(~putida) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_jitter(alpha=0.4, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm) +
    scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
    labs(title="Recovery (protegens present?)", colour="Inoculated\nRichness",
         y="Effect size on Shannon Diversity", x="Heat Duration (hrs)"))

# clean up
rm(recov_divers0, recov_divers1, recov_divers2)
```

This summarizes diversity at the end of resistance and diversity at the end of recovery.

When *protegens* is **present**, heat has no effect on diversity. This is because
protegens gets fixed in those communities.

When *protegens* is **absent**: heat has a negative effect on diversity. EXCEPT for the
community composed of the 2 slower growing species (grimontii & veronii) for heat events
that are not yet long enough to drive them to extinction.

During resistance, inoculated community richness has an overall negative effect on
diversity. This is because the presence of protegens -- like the presence of putida -- has
an overall negative effect on diversity. However, when we take into account this protegens
effect, inoculated community richness in fact has a positive effect on diversity. I'm
surprised that heat duration on its own does not have a significant effect: this is
probably because there are a lot of missing values for 48h of heat. (This model is
teetering on the edge of reasonable: 22 estimated parameters on 162 observations.)

Recovery is consistent with resistance. The only difference is that we finally see
significant effects of heat duration. Heat duration on its own has a negative effect. This
becomes slightly positive when the protegens effect is taken into account. (This model is
teetering on the edge of reasonable: 22 estimated parameters on 184 observations.)

**BUT!!!!** It seems that the linear model is not a good fit to this data. Consider using
a GLM with a distribution that can have many 0 values.

## Estimated marginal means (emmeans)

Maddy and Gerard suggest that I use the full model to estimate effect size then emmeans to
estimate the effect size post-hoc. I'm tailouring this analysis on the example script that
Nico sent me (`Script_simplified for Hermina.R`).

One unique attribute of my experimental design is that the Day used as control differs
with heat duration (e.g., last day of recovery for 6h heat duration is Day 3 but last day
of recovery for 48h is Day 5). One possible solution for this is to run separate models
for each heat treatment with its respective controls (i.e., 4 models in total). To make
sure that the effect sizes will be directly comparable across the models (especially with
respect to the standard deviation), Gerard suggested that I scale the whole data prior to
splitting it up into 4 (but not centering it as that will give me negative values that I
can't really use a ). Finally, if/when testing for significance it will then be necessary
to control for multiple testing (e.g., using a Bonferroni correction).

Note that for diversity I am considering CommRich as a numeric (instead of as a factor).
Initially I tried playing around with CommRich as an ordered & unordered factor. But I
found that `glmmTMB` was choosing to drop different predictors because it was upset that
my model was overparameterized. This was particularly annoying as it was dropping the
estimates for the control treatments...

```{r, diversity_glmFamily}
# remove the monocultures from the data
diversity_forFit <- absDen_forFit %>% filter(CommRich > 1) %>% # diversity is nonsense for monocultures
                        select(-Total_density, -TotDen_plusEpsilon,
                                      -Conc_putida, -Conc_protegens, -Conc_grimontii, -Conc_veronii)
# make heat into a factor with 0 as control
diversity_forFit$Heat <- factor(diversity_forFit$Heat,
                                levels = c("0", "6", "12", "24", "48"))
levels(diversity_forFit$Heat)[1] <- "control"

# add a column indicating whether the replicate survived
diversity_forFit <- inner_join(diversity_forFit,
                               extinct.df %>% select(uniqID, Heat, survived),
                               by = c("uniqID", "Heat"))
# keep only the data where wells did NOT go extinct (in this case diversity is 0 but this is trivial)
#diversity_forFit <- diversity_forFit %>% filter(survived == 1)

# scale the data by its standard deviation
diversity_forFit$Diversity_scale <- scale(diversity_forFit$Diversity,
                                          scale = sd(diversity_forFit$Diversity, na.rm =  TRUE),
                                          center = FALSE)

# the max re-scaled value is 5.38 and 38% of the data is 0's
# so try gamma and lognormal distributions (maybe also Gaussian just to check that it's a bad fit?)
summary(diversity_forFit$Diversity_scale)
sum(diversity_forFit$Diversity_scale == 0, na.rm = TRUE) / length(diversity_forFit$Diversity_scale)

# set CommRich to unordered factors
#diversity_forFit$CommRich <- factor(diversity_forFit$CommRich, ordered = FALSE)
# setting Day as a factor (either ordered or unordered) leads to overfitting
#diversity_forFit$Day <- factor(diversity_forFit$Day, ordered = FALSE)
# !!! emmeans expects the control to be the very *last* level !!!
diversity_forFit$Heat <- factor(diversity_forFit$Heat,
                                levels = c("6", "12", "24", "48", "control"))

# let's compare different GLM families
try_gaussian <- glmmTMB(Diversity_scale ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                        data = diversity_forFit,
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_gaussian, plot = TRUE)

try_gamma <- glmmTMB(Diversity_scale ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                     data = diversity_forFit,
                     family = ziGamma,
                     ziformula = ~1, # this needs to be added because there are 0 values in the data
                     control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_gamma, plot = TRUE)

try_lognorm <- glmmTMB(Diversity_scale ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                       data = diversity_forFit,
                       family = lognormal,
                       ziformula = ~1, # this needs to be added because there are 0 values in the data
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_lognorm, plot = TRUE)

try_LOGlognorm <- glmmTMB(log(Diversity_scale+1) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                          data = diversity_forFit,
                          family = lognormal,
                          ziformula = ~1, # I'm keeping this as 0-inflated lognormal alone was already over-dispersed. So I want to see if the log(x+1) transformation sufficiently brings in the long tail.
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_LOGlognorm, plot = TRUE)

try_negbinom <- glmmTMB(as.integer(Diversity_scale*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                       data = diversity_forFit,
                       family = nbinom2,
                       control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom, plot = TRUE)

try_negbinom0 <- glmmTMB(as.integer(Diversity_scale*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                         data = diversity_forFit,
                         family = nbinom2,
                         ziformula = ~1, # try zero inflated distribution
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom0, plot = TRUE)

try_poisson <- glmmTMB(as.integer(Diversity_scale*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                         data = diversity_forFit,
                         family = genpois,
                         #ziformula = ~1, # try zero inflated distribution
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson, plot = TRUE)

try_poisson0 <- glmmTMB(as.integer(Diversity_scale*1000) ~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                         data = diversity_forFit,
                         family = genpois,
                         ziformula = ~1, # try zero inflated distribution
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson0, plot = TRUE)

# let's check this with AIC and BIC
AIC(try_gaussian, try_gamma, try_lognorm, try_LOGlognorm,
    try_negbinom, try_negbinom0, try_poisson, try_poisson0) %>% arrange(AIC)
BIC(try_gaussian, try_gamma, try_lognorm, try_LOGlognorm,
    try_negbinom, try_negbinom0, try_poisson, try_poisson0) %>% arrange(BIC)

# clean up
rm(try_gamma, try_lognorm, try_LOGlognorm, try_negbinom, try_negbinom0, try_poisson, try_poisson0)
```

According to the residuals, the zero-inflated negative binomial and the zero-inflated
lognormal are about equally okay-ish. We could also take the AIC & BIC values in
consideration for our decision but that is far less important. At the end of the day my
decision is to go with the zero-inflated lognormal. The reason for this is because my
understanding is that the most important thing to consider when selecting a GLM family is
which family would *a priori* be the most natural choice. For diversity data, the Gamma or
lognormal distributions are the most natural choices *a priori* because, for 4 species,
the Shannon diversity data is a continuous variable between 0 and 1.386294. Therefore I
think it makes sense to choose the lognormal (even if its residuals are not perfect).

Note that in the model fitting above I consider Day as a numeric predictor. This is
because I want to decide on the GLM family by considering the complete data. (I was having
problems with Day as an un/ordered factor for reasons that are trouble-shooted in the
unevaluated code below.) For the rest of the analysis below, I consider the effect of day
(which is called `Trtmt_Day`) as a factor representing either resistance (i.e., on the
last day of heat) or recovery.

```{r, diagnose_biodiversity_glm, eval=FALSE}
##
## below is some old trouble shooting I had to do while glmmTMB was complaining about "dropping columns from rank-deficient conditional model"
## I found the following stack overflow string that was very helpful in diagnosing this problem
## https://stackoverflow.com/questions/78183492/using-glmmtmb-getting-warning-message-dropping-columns-from-rank-deficient-cond
## but in the end it is no longer relevant for later versions of the code because I am using both Day and CommRich as numeric instead of un/ordered factors.
##


# create all possible combinations for the 3 different predictors and display how many replicates there are for each combo
data_combinations <- with(diversity_forFit, 
                          as.data.frame(table(Day, CommRich, Heat)))
print(data_combinations)
# from here we see that there's no data for Day 5 at Heat durations of 24 and 12 hours

# check to see which cases get dropped (even if data does exist for these combinations)
model_combinations <- with(model.frame(try_gaussian), 
                           as.data.frame(table(Day, CommRich, Heat)))
print(model_combinations)
# these data.frames seem very similar but let's take advantage of same row order to check identity of Freq values
all(data_combinations$Freq == model_combinations$Freq)
# indeed, it seems that cases are NOT getting dropped for having NA values in the predictors
# (or, well, at least these cases were covered simply when we looked at the data_combinations above)

# see which combinations of variables were aliased,
X <- model.matrix(~ CommRich:Day + Heat:Day + CommRich:Heat:Day,
                  data = diversity_forFit)
caret::findLinearCombos(X)

rm(data_combinations, model_combinations, try_gaussian)
```

In order to actually analyze the data, I need to split it up into 4 separate data-sets
according to treatment and with its associated control. Remember that we need to correct
for the fact that we are doing 4 independent tests. So instead of using the typical
$\alpha=0.05$ we use the Bonferroni corrected $\alpha / n = 0.05/4 = 0.0125$ (or simply
just 0.01).

```{r, diversity_emmeans}
# clean up from above
rm(try_gaussian)

####################
# 6h heat duration
####################
# grab just the treatment with its associated control data
diversity_6h <- rbind(diversity_forFit %>% filter(Heat == "6"),
                      diversity_forFit %>% filter(Heat == "control", Day < 4))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_6h$Trtmt_Day <- "resist"
diversity_6h$Trtmt_Day[diversity_6h$Day == 2] <- "recov_1"
diversity_6h$Trtmt_Day[diversity_6h$Day == 3] <- "recov_2"

divers6h_H0 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day,
                       data = diversity_6h,
                       family = lognormal,
                       ziformula = ~Heat, # this needs to be added because there are 0 values in the data
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H0, plot = TRUE)

# note that putida is both in the zero inflation & fixed effect
divers6h_H1 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + putida,
                       data = diversity_6h,
                       family = lognormal,
                       ziformula = ~putida + Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H1, plot = TRUE)

divers6h_H2 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + protegens,
                       data = diversity_6h,
                       family = lognormal,
                       ziformula = ~protegens + Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H2, plot = TRUE)

# unfortunately this also open up the possibility of interactions in fixed effects
divers6h_H2_1 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + protegens*CommRich,
                       data = diversity_6h,
                       family = lognormal,
                       ziformula = ~protegens + Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

divers6h_H3 <- glmmTMB(Diversity_scale ~ CommRich + Heat*Trtmt_Day*protegens,
                       data = diversity_6h,
                       family = lognormal,
                       ziformula = ~protegens*Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H3, plot = TRUE)


# check preferred models
anova(divers6h_H0, divers6h_H1)
anova(divers6h_H0, divers6h_H2)
anova(divers6h_H2, divers6h_H2_1)

AIC(divers6h_H0, divers6h_H1, divers6h_H2, divers6h_H2_1, divers6h_H3) %>% arrange(AIC)
BIC(divers6h_H0, divers6h_H1, divers6h_H2, divers6h_H2_1, divers6h_H3) %>% arrange(BIC)
# H3 is the preferred model
summary(divers6h_H3)


# create data.frame for plotting
divers_predict <- cbind(divers6h_H3$frame,
                        predict(divers6h_H3, type="response"))
colnames(divers_predict)[c(1,6)] <- c("observed", "predicted")
# plot the model predictions against the data
ggplot(divers_predict, 
       aes(x=Trtmt_Day, y=observed, colour=as.factor(CommRich))) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich")
# cleanup
rm(divers_predict)

# here's another way to plot this in case I'm interested later
#ggplot(diversity_6h %>%
#           mutate(predictions = predict(divers6h_H2, type="response")), 
#       aes(x=Day, y=Diversity_scale, colour=as.factor(CommRich))) +
#    facet_grid(~Heat) +
#    geom_jitter(alpha=0.4) +
#    geom_line(aes(y=predictions,
#                  group=paste(CommRich, protegens),
#                  linetype=as.factor(protegens))) +
#    labs(y="Shannon diversity (rescaled)",
#         colour="CommRich",
#         linetype="protegens?")

####################
# 12h heat duration
####################
# grab just the treatment with its associated control data
diversity_12h <- rbind(diversity_forFit %>% filter(Heat == "12", Day > 1),
                       diversity_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_12h$Trtmt_Day <- "resist"
diversity_12h$Trtmt_Day[diversity_12h$Day == 3] <- "recov_1"
diversity_12h$Trtmt_Day[diversity_12h$Day == 4] <- "recov_2"

divers12h_H0 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day,
                       data = diversity_12h,
                       family = lognormal,
                       ziformula = ~ Heat, # this needs to be added because there are 0 values in the data
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers12h_H0, plot = TRUE)

divers12h_H1 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + putida,
                       data = diversity_12h,
                       family = lognormal,
                       ziformula = ~putida + Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers12h_H1, plot = TRUE)

divers12h_H2 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + protegens,
                       data = diversity_12h,
                       family = lognormal,
                       ziformula = ~protegens + Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers12h_H2, plot = TRUE)

divers12h_H2_1 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + protegens*CommRich,
                       data = diversity_12h,
                       family = lognormal,
                       ziformula = ~protegens + Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

divers12h_H3 <- glmmTMB(Diversity_scale ~ CommRich + Heat*Trtmt_Day*protegens,
                       data = diversity_12h,
                       family = lognormal,
                       ziformula = ~protegens*Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers12h_H3, plot = TRUE)


# check preferred models
anova(divers12h_H0, divers12h_H1)
anova(divers12h_H0, divers12h_H2)
anova(divers12h_H2, divers12h_H2_1)

AIC(divers12h_H0, divers12h_H1, divers12h_H2, divers12h_H2_1, divers12h_H3) %>% arrange(AIC)
BIC(divers12h_H0, divers12h_H1, divers12h_H2, divers12h_H2_1, divers12h_H3) %>% arrange(BIC)

# H2 is the preferred model (even if its residuals are the worst)
summary(divers12h_H2)

# plot the model predictions against the data
ggplot(diversity_12h %>%
         mutate(predictions = predict(divers12h_H2, type="response")), 
       aes(x=Day, y=Diversity_scale, colour=as.factor(CommRich))) + 
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predictions, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich")

####################
# 24h heat duration
####################
# grab just the treatment with its associated control data
diversity_24h <- rbind(diversity_forFit %>% filter(Heat == "24", Day > 1),
                       diversity_forFit %>% filter(Heat == "control", Day > 1, Day != 5))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_24h$Trtmt_Day <- "resist"
diversity_24h$Trtmt_Day[diversity_24h$Day == 3] <- "recov_1"
diversity_24h$Trtmt_Day[diversity_24h$Day == 4] <- "recov_2"

divers24h_H0 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day,
                       data = diversity_24h,
                       family = lognormal,
                       ziformula = ~Heat, # this needs to be added because there are 0 values in the data
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers24h_H0, plot = TRUE)

divers24h_H1 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + putida,
                       data = diversity_24h,
                       family = lognormal,
                       ziformula = ~putida + Heat, 
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers24h_H1, plot = TRUE)

divers24h_H2 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + protegens,
                       data = diversity_24h,
                       family = lognormal,
                       ziformula = ~protegens + Heat, 
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers24h_H2, plot = TRUE)

divers24h_H2_1 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + protegens*CommRich,
                       data = diversity_24h,
                       family = lognormal,
                       ziformula = ~protegens + Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

divers24h_H3 <- glmmTMB(Diversity_scale ~ CommRich + Heat*Trtmt_Day*protegens,
                       data = diversity_24h,
                       family = lognormal,
                       ziformula = ~protegens*Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers24h_H3, plot = TRUE)


# check preferred models
anova(divers24h_H0, divers24h_H1)
anova(divers24h_H0, divers24h_H2)
anova(divers24h_H2, divers24h_H2_1)

AIC(divers24h_H0, divers24h_H1, divers24h_H2, divers24h_H2_1, divers24h_H3) %>% arrange(AIC)
BIC(divers24h_H0, divers24h_H1, divers24h_H2, divers24h_H2_1, divers24h_H3) %>% arrange(BIC)

# H2 is the preferred model (even if its residuals are the worst)
summary(divers24h_H2)

# plot the model predictions against the data
ggplot(diversity_24h %>%
         mutate(predictions = predict(divers24h_H2, type="response")), 
       aes(x=Day, y=Diversity_scale, colour=as.factor(CommRich))) + 
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predictions, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich")

####################
# 48h heat duration
####################
# grab just the treatment with its associated control data
diversity_48h <- rbind(diversity_forFit %>% filter(Heat == "48", Day > 2),
                       diversity_forFit %>% filter(Heat == "control", Day > 2))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_48h$Trtmt_Day <- "resist"
diversity_48h$Trtmt_Day[diversity_48h$Day == 4] <- "recov_1"
diversity_48h$Trtmt_Day[diversity_48h$Day == 5] <- "recov_2"
# drop the resistance data altogether from 48h treatment because glmmTMB fails to converge, saying that there's a Non-positive definite (NPD) Hessian
# Running diagnose(<model>) tells us that the likelihood surface is flat near the MLE and that this is happening for parameters Trtmt_Dayresist, CommRich:Trtmt_Dayresist, Heatcontrol:Trtmt_Dayresist, and CommRich:Heatcontrol:Trtmt_Dayresist
# This is likely because of all the NA values during the last day of heat for this longest duration
diversity_48h <- diversity_48h %>% filter(Trtmt_Day != "resist")

divers48h_H0 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day,
                       data = diversity_48h,
                       family = lognormal,
                       ziformula = ~Heat, # this needs to be added because there are 0 values in the data
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers48h_H0, plot = TRUE)

divers48h_H1 <- glmmTMB(Diversity_scale ~ CommRich*Heat + putida,
                       data = diversity_48h,
                       family = lognormal,
                       ziformula = ~putida + Heat, 
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers48h_H1, plot = TRUE)

divers48h_H2 <- glmmTMB(Diversity_scale ~ CommRich*Heat*Trtmt_Day + protegens,
                       data = diversity_48h,
                       family = lognormal,
                       ziformula = ~protegens + Heat, # this is the difference!!!
                       control = glmmTMBControl(optCtrl = list(iter.max = 1e6, eval.max = 1e6)))
simulateResiduals(fittedModel = divers48h_H2, plot = TRUE)

divers48h_H2_1 <- glmmTMB(Diversity_scale ~ protegens + Heat + CommRich,
                       data = diversity_48h,
                       family = lognormal,
                       ziformula = ~protegens + Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

divers48h_H3 <- glmmTMB(Diversity_scale ~ CommRich + Heat*Trtmt_Day*protegens,
                       data = diversity_48h,
                       family = lognormal,
                       ziformula = ~protegens*Heat,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers48h_H3, plot = TRUE)

# check preferred models
anova(divers48h_H0, divers48h_H1)
anova(divers48h_H0, divers48h_H2)
anova(divers48h_H2, divers48h_H2_1)

AIC(divers48h_H0, divers48h_H1, divers48h_H2, divers48h_H2_1, divers48h_H3) %>% arrange(AIC)
BIC(divers48h_H0, divers48h_H1, divers48h_H2, divers48h_H2_1, divers48h_H3) %>% arrange(BIC)

# H2_1 is the preferred model because it has just as much explanatory power as H2
summary(divers48h_H2_1)

# plot the model predictions against the data
ggplot(diversity_48h %>%
         mutate(predictions = predict(divers48h_H2_1, type="response")), 
       aes(x=Day, y=Diversity_scale, colour=as.factor(CommRich))) + 
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predictions, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich",
       title="H2_1: diversity ~ protegens + Heat + CommRich")

# but I will use H2 below because it is consistent with the rest of the data
summary(divers48h_H2)
ggplot(diversity_48h %>%
         mutate(predictions = predict(divers48h_H2, type="response")), 
       aes(x=Day, y=Diversity_scale, colour=as.factor(CommRich))) + 
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predictions, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich",
       title="H2: diversity ~ CommRich*Heat*Trtmt_Day + protegens")

############################
# effect sizes
############################

# get the effect size & post-hoc p-values
## remember that we need to correct for multiple comparisons that were generated by subsetting the data into 4 parts
## THEREFORE CONSIDER ALPHA/N = 0.05/4 = 0.0125 as the threshold for significance
emm_6h <- emmeans(divers6h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = diversity_6h, type = "response")
effect_6h <- eff_size(emm_6h, sigma(divers6h_H2), edf = df.residual(divers6h_H2))

emm_12h <- emmeans(divers12h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = diversity_12h, type = "response")
effect_12h <- eff_size(emm_12h, sigma(divers12h_H2), edf = df.residual(divers12h_H2))

emm_24h <- emmeans(divers24h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = diversity_24h, type = "response")
effect_24h <- eff_size(emm_24h, sigma(divers24h_H2), edf = df.residual(divers24h_H2))

emm_48h <- emmeans(divers48h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = diversity_48h, type = "response")
effect_48h <- eff_size(emm_48h, sigma(divers48h_H2), edf = df.residual(divers48h_H2))


# a function that extracts the confidence intervals from eff_size *** contingent on protegens ***
get_effsize_CIs <- function(eff_size_object, heat_trtmt) {
  data.frame(Heat = heat_trtmt,
             CommRich = confint(eff_size_object)[[2]],
             Trtmt_Day = confint(eff_size_object)[[3]],
             protegens = confint(eff_size_object)[[4]],
             est = confint(eff_size_object)[[5]],
             loCI = confint(eff_size_object)[[8]],
             hiCI = confint(eff_size_object)[[9]])
}
# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
div_effects_protegens <- data.frame()
div_effects_protegens <- rbind(div_effects_protegens,
                              get_effsize_CIs(effect_6h, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h, heat_trtmt = 48),
                              c(48, NA, "resist", 0, rep(NA, 3)),
                              c(48, NA, "resist", 1, rep(NA, 3)))
# several columns were coerced to character. They need to be put back as numeric values
div_effects_protegens[,-3] <- sapply(div_effects_protegens[,-3], as.numeric) # change all but 3rd column into numeric
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects_protegens$Trtmt_Day <- factor(div_effects_protegens$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(div_effects_protegens$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

# plot conditional part of the model
ggplot(div_effects_protegens,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day, shape = as.logical(protegens))) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(title = "Effect size of conditional model",
       x = "Effect Size on Shannon Diversity",
       shape = "protegens\npresent?",
       y="Heat duration")

# okay so we have confirmed that the presence of protegens doesn't actually interact with heat
# so let's average across the effect of protegens.
posthoc_6h <- emmeans(effect_6h, pairwise ~ Trtmt_Day*CommRich, data = diversity_6h)
posthoc_12h <- emmeans(effect_12h, pairwise ~ Trtmt_Day*CommRich, data = diversity_12h)
posthoc_24h <- emmeans(effect_24h, pairwise ~ Trtmt_Day*CommRich, data = diversity_24h)
posthoc_48h <- emmeans(effect_48h, pairwise ~ Trtmt_Day*CommRich, data = diversity_48h)

# a function that extracts the confidence intervals from a post-hoc object
get_posthoc <- function(posthoc_object, heat_trtmt) {
  output <- multcomp::cld(posthoc_object, alpha=0.05/4, Letters = letters) %>%
              data.frame() %>%
                select(-df)
  colnames(output)[3:7] <- c("est", "SE", "loCI", "hiCI", "groups")
  output$Heat <- heat_trtmt
  return(output)
}

div_effects <- data.frame()
div_effects <- rbind(div_effects,
                     get_posthoc(posthoc_6h, heat_trtmt = 6),
                     get_posthoc(posthoc_12h, heat_trtmt = 12),
                     get_posthoc(posthoc_24h, heat_trtmt = 24),
                     get_posthoc(posthoc_48h, heat_trtmt = 48),
                     c("resis", rep(NA, 6), 48))
# several columns were coerced to character. They need to be put back as numeric values
div_effects[,c(-1, -7)] <- sapply(div_effects[,c(-1, -7)], as.numeric)
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects$Trtmt_Day <- factor(div_effects$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(div_effects$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

# plot with group labels
ggplot(div_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-1.3, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Shannon Diversity",
       y="Heat duration")

# plot without group labels
ggplot(div_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Shannon Diversity",
       y="Heat duration")

#######
# finally, we will do a series of pairwise two-tailed t-tests to compare between heat durations
#######
# a function that approximates the sample size from each data subset
estimate_n <- function(data_subset, CommRich = FALSE) {
  if(CommRich == 0) {
    # get the number of unique ID's present on recovery day 2 for the heat treatment
    # then divide this by 4 as we want to know the average sample size across CommRich
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2",]$uniqID))/4
  }
  if(CommRich == 1){ # do the same thing for specified values of CommRich
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2" & data_subset$CommRich == 1,]$uniqID))/4
  }
  if(CommRich == 2){
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2" & data_subset$CommRich == 2,]$uniqID))/4
  }
  if(CommRich == 3){
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2" & data_subset$CommRich == 3,]$uniqID))/4
  }
  if(CommRich == 4){
    output <- length(unique(data_subset[data_subset$Heat != "control" & data_subset$Trtmt_Day == "recov_2" & data_subset$CommRich == 4,]$uniqID))/4
  }
  return(output)
}
# a function that runs two-tailed t-test between row numbers of diversity_effects df
run_ttest <- function(row_x, row_y,
                      summary_stats_df){
  ttest_object <- tsum.test(mean.x = summary_stats_df$est[row_x],
                            s.x = summary_stats_df$SE[row_x],
                            n.x = summary_stats_df$n[row_x],
                            mean.y = summary_stats_df$est[row_y],
                            s.y = summary_stats_df$SE[row_y],
                            n.y = summary_stats_df$n[row_y],
                            alternative="two.sided")
  return(data.frame(t_statistic = ttest_object$statistic,
                    df = ttest_object$parameters,
                    pvalue = ttest_object$p.value))
}

# estimate the sample sizes
temp <- div_effects # copy the effects to temp
div_effects <- rbind(temp %>% filter(Heat == 6) %>% mutate(n = estimate_n(diversity_6h)),
                     temp %>% filter(Heat == 12) %>% mutate(n = estimate_n(diversity_12h)),
                     temp %>% filter(Heat == 24) %>% mutate(n = estimate_n(diversity_24h)),
                     temp %>% filter(Heat == 48) %>% mutate(n = estimate_n(diversity_48h)))
rm(temp)
# estimate the SD from the SE
div_effects <- div_effects %>% mutate(SD = SE * sqrt(n)) %>%
    # re-order by Heat and Trtmt_Day
                          arrange(Heat, Trtmt_Day)

# all pairwise combinations of comparisons between the same treatment day for different durations
temp <- t(combn(c(1,4,7,10), 2))
combos <- rbind(temp, temp+1, temp+2)
rm(temp)
# loop through all the combinations and do the t-tests
divEffects_ttests <- data.frame()
for(i in 1:nrow(combos)){
  divEffects_ttests <- rbind(divEffects_ttests,
                             run_ttest(row_x = combos[i,1],
                                       row_y = combos[i,2],
                                       summary_stats_df = div_effects))
}
divEffects_ttests$adjusted_p <- p.adjust(divEffects_ttests$pvalue, method = "bonferroni")
divEffects_ttests$Trtmt_Day <- div_effects$Trtmt_Day[combos[,1]]
divEffects_ttests$Heat_1 <- div_effects$Heat[combos[,1]]
divEffects_ttests$Heat_2 <- div_effects$Heat[combos[,2]]

print(divEffects_ttests)

# and we can also plot the zero inflated model effect sizes:
effect_6h_zi <- eff_size(emmeans(divers6h_H2, ~ Heat, data = diversity_6h, component = "zi"),
                         sigma(divers6h_H2),
                         edf = df.residual(divers6h_H2))
effect_12h_zi <- eff_size(emmeans(divers12h_H2, ~ Heat, data = diversity_12h, component = "zi"),
                         sigma(divers12h_H2),
                         edf = df.residual(divers12h_H2))
effect_24h_zi <- eff_size(emmeans(divers24h_H2, ~ Heat, data = diversity_24h, component = "zi"),
                         sigma(divers24h_H2),
                         edf = df.residual(divers24h_H2))
effect_48h_zi <- eff_size(emmeans(divers48h_H2_1, ~ Heat, data = diversity_48h, component = "zi"),
                         sigma(divers48h_H2_1),
                         edf = df.residual(divers48h_H2_1))
# a function to extract CIs from eff_size on ZERO INFLATED part *** averaging across protegens ***
get_ZI_effsize_CIs <- function(eff_size_object, heat_trtmt) {
  data.frame(Heat = heat_trtmt,
             ZIest = confint(eff_size_object)[[2]],
             ZIloCI = confint(eff_size_object)[[5]],
             ZIhiCI = confint(eff_size_object)[[6]])
}
# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
div_ZIeff <- data.frame()
div_ZIeff <- rbind(div_ZIeff,
                   get_ZI_effsize_CIs(effect_6h_zi, heat_trtmt = 6),
                   get_ZI_effsize_CIs(effect_12h_zi, heat_trtmt = 12),
                   get_ZI_effsize_CIs(effect_24h_zi, heat_trtmt = 24),
                   get_ZI_effsize_CIs(effect_48h_zi, heat_trtmt = 48))
ggplot(div_ZIeff,
       aes(x = ZIest, y = as.factor(Heat))) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = ZIloCI, xmax = ZIhiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(title = "Effect size of zero-inflated model",
       x = "Effect Size on Shannon Diversity",
       y="Heat duration")

# a function to get CI's
hack_CIs <- function(cond_loCI, cond_hiCI, ZI_loCI, ZI_hiCI){
  one <- cond_loCI * (1 - ZI_loCI)
  two <- cond_loCI * (1 - ZI_hiCI)
  three <- cond_hiCI * (1 - ZI_loCI)
  four <- cond_hiCI * (1 - ZI_hiCI)
  array <- cbind(one, two, three, four)
  return(list(apply(array, 1, min),
              apply(array, 1, max)))
}
# we can also be super hacky and estimate an overall effect size that combines the conditional and zero-inflated parts of the model:
div_overall_effects <- inner_join(div_effects, div_ZIeff, by = "Heat") %>%
                          mutate(overall_est = est*(1 - ZIest),
                                 overall_loCI = hack_CIs(loCI, hiCI, ZIloCI, ZIhiCI)[[1]],
                                 overall_hiCI = hack_CIs(loCI, hiCI, ZIloCI, ZIhiCI)[[2]]) %>%
                            select(Trtmt_Day, Heat, overall_est, overall_loCI, overall_hiCI)
# plot
ggplot(div_overall_effects,
       aes(x = overall_est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = overall_loCI, xmax = overall_hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Shannon Diversity",
       y = "Heat duration",
       title = "hacky combination of cond & ZI effects")

# cleanup
rm(divers6h_H0, divers6h_H1, divers6h_H2, emm_6h, effect_6h,
   divers12h_H0, divers12h_H1, divers12h_H2, emm_12h, effect_12h,
   divers24h_H0, divers24h_H1, divers24h_H2, emm_24h, effect_24h,
   divers48h_H0, divers48h_H1, divers48h_H2, emm_48h, effect_48h,
   divers6h_H2_1, divers12h_H2_1, divers24h_H2_1, divers48h_H2_1,
   div_effects_protegens, divEffects_ttests,
   effect_6h_zi, effect_12h_zi, effect_24h_zi, effect_48h_zi,
   posthoc_6h, posthoc_12h, posthoc_24h, posthoc_48h,
   get_ZI_effsize_CIs, div_ZIeff,
   div_effects, div_overall_effects)
```

This is a bit confusing because the effect sizes are split up over the conditional and the
zero-inflated parts of the model. The overall effect size is: (1 - zi)\*(cond mean). See
[this stackover flow
thread](https://stackoverflow.com/questions/62351158/contrasts-with-zero-inflated-glmmtmb).

Let's try the analysis again but now adding a small value to all diversity estimates such
that we remove the zero's. This way we will no longer need to use a zero-inflated part in
the model and the effect sizes will be simpler to explain. (Especially because I think I
am losing a lot of power for the 48h treatment as a result of everything literally being
0.)

```{r, diversity_plus1}
# find the smallest non-zero value in the rescaled diversity estimate
smallest_diversity <- min(diversity_forFit$Diversity_scale[diversity_forFit$Diversity_scale != 0], na.rm=TRUE)
# now add 1/100th of that value to all the diversity estimates and re-do all the analyses I did above...

####################
# 6h heat duration
####################
# add small value to diversity
diversity_6h$Diversity_scalePLUSepsilon <- diversity_6h$Diversity_scale + smallest_diversity/100

divers6h_H0 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day,
                       data = diversity_6h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H0, plot = TRUE)

divers6h_H1 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day + putida,
                       data = diversity_6h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H1, plot = TRUE)

divers6h_H2 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day + protegens,
                       data = diversity_6h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H2, plot = TRUE)

# let's try categorical CommRich
divers6h_H2_1 <- glmmTMB(Diversity_scalePLUSepsilon ~ as.factor(CommRich)*Heat*Trtmt_Day + protegens,
                       data = diversity_6h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H2, plot = TRUE)

divers6h_H3 <- glmmTMB(Diversity_scalePLUSepsilon ~ Heat*Trtmt_Day*protegens + CommRich,
                       data = diversity_6h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers6h_H3, plot = TRUE)

# check preferred models
anova(divers6h_H0, divers6h_H1)
anova(divers6h_H0, divers6h_H2)

AIC(divers6h_H0, divers6h_H1, divers6h_H2, divers6h_H2_1, divers6h_H3) %>% arrange(AIC)
BIC(divers6h_H0, divers6h_H1, divers6h_H2, divers6h_H2_1, divers6h_H3) %>% arrange(BIC)
# H3 is the preferred model
summary(divers6h_H3)

# create data.frame for plotting
divers_predict <- cbind(divers6h_H3$frame,
                        predict(divers6h_H3, type="response"))
colnames(divers_predict)[c(1,6)] <- c("observed", "predicted")
# plot the model predictions against the data
ggplot(divers_predict, 
       aes(x=Trtmt_Day, y=observed, colour=as.factor(CommRich))) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich")
# cleanup
rm(divers_predict)

####################
# 12h heat duration
####################
# add small value to diversity
diversity_12h$Diversity_scalePLUSepsilon <- diversity_12h$Diversity_scale + smallest_diversity/100

divers12h_H0 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day,
                       data = diversity_12h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers12h_H0, plot = TRUE)

divers12h_H1 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day + putida,
                       data = diversity_12h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers12h_H1, plot = TRUE)

divers12h_H2 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day + protegens,
                       data = diversity_12h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers12h_H2, plot = TRUE)

divers12h_H2_1 <- glmmTMB(Diversity_scalePLUSepsilon ~ as.factor(CommRich)*Heat*Trtmt_Day + protegens,
                       data = diversity_12h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

divers12h_H3 <- glmmTMB(Diversity_scalePLUSepsilon ~ Heat*Trtmt_Day*protegens + CommRich,
                       data = diversity_12h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers12h_H3, plot = TRUE)



# check preferred models
anova(divers12h_H0, divers12h_H1)
anova(divers12h_H0, divers12h_H2)
anova(divers12h_H2, divers12h_H2_1)

AIC(divers12h_H0, divers12h_H1, divers12h_H2, divers12h_H2_1, divers12h_H3) %>% arrange(AIC)
BIC(divers12h_H0, divers12h_H1, divers12h_H2, divers12h_H2_1, divers12h_H3) %>% arrange(BIC)

# H3 is the preferred model (even if its residuals are the worst)
summary(divers12h_H3)

# create data.frame for plotting
divers_predict <- cbind(divers12h_H3$frame,
                        predict(divers12h_H3, type="response"))
colnames(divers_predict)[c(1,6)] <- c("observed", "predicted")
# plot the model predictions against the data
ggplot(divers_predict, 
       aes(x=Trtmt_Day, y=observed, colour=as.factor(CommRich))) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich")
# cleanup
rm(divers_predict)

####################
# 24h heat duration
####################
# add small value to diversity
diversity_24h$Diversity_scalePLUSepsilon <- diversity_24h$Diversity_scale + smallest_diversity/100

divers24h_H0 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day,
                       data = diversity_24h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers24h_H0, plot = TRUE)

divers24h_H1 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day + putida,
                       data = diversity_24h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers24h_H1, plot = TRUE)

divers24h_H2 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day + protegens,
                       data = diversity_24h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers24h_H2, plot = TRUE)

divers24h_H2_1 <- glmmTMB(Diversity_scalePLUSepsilon ~ as.factor(CommRich)*Heat*Trtmt_Day + protegens,
                       data = diversity_24h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

divers24h_H3 <- glmmTMB(Diversity_scalePLUSepsilon ~ Heat*Trtmt_Day*protegens + CommRich,
                       data = diversity_24h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers24h_H3, plot = TRUE)


# check preferred models
anova(divers24h_H0, divers24h_H1)
anova(divers24h_H0, divers24h_H2)
anova(divers24h_H0, divers24h_H3)
anova(divers24h_H2, divers24h_H2_1)

AIC(divers24h_H0, divers24h_H1, divers24h_H2, divers24h_H2_1, divers24h_H3) %>% arrange(AIC)
BIC(divers24h_H0, divers24h_H1, divers24h_H2, divers24h_H2_1, divers24h_H3) %>% arrange(BIC)

# H3 is the preferred model
summary(divers24h_H3)

# create data.frame for plotting
divers_predict <- cbind(divers24h_H3$frame,
                        predict(divers24h_H3, type="response"))
colnames(divers_predict)[c(1,6)] <- c("observed", "predicted")
# plot the model predictions against the data
ggplot(divers_predict, 
       aes(x=Trtmt_Day, y=observed, colour=as.factor(CommRich))) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich")
# cleanup
rm(divers_predict)

####################
# 48h heat duration
####################
# let's see if we can now look at the resistance too?
diversity_48h <- rbind(diversity_forFit %>% filter(Heat == "48", Day > 2),
                       diversity_forFit %>% filter(Heat == "control", Day > 2))
# create a column for last day of heat, first day of recovery, and last day of recovery
diversity_48h$Trtmt_Day <- "resist"
diversity_48h$Trtmt_Day[diversity_48h$Day == 4] <- "recov_1"
diversity_48h$Trtmt_Day[diversity_48h$Day == 5] <- "recov_2"

# add small value to diversity
diversity_48h$Diversity_scalePLUSepsilon <- diversity_48h$Diversity_scale + smallest_diversity/100


divers48h_H0 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day,
                       data = diversity_48h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers48h_H0, plot = TRUE)

divers48h_H1 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat + putida,
                       data = diversity_48h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers48h_H1, plot = TRUE)

divers48h_H2 <- glmmTMB(Diversity_scalePLUSepsilon ~ CommRich*Heat*Trtmt_Day + protegens,
                       data = diversity_48h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 1e6, eval.max = 1e6)))
simulateResiduals(fittedModel = divers48h_H2, plot = TRUE)

divers48h_H2_1 <- glmmTMB(Diversity_scalePLUSepsilon ~ protegens + Heat + CommRich,
                       data = diversity_48h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

divers48h_H3 <- glmmTMB(Diversity_scalePLUSepsilon ~ Heat*Trtmt_Day*protegens + CommRich,
                       data = diversity_48h,
                       family = lognormal,
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = divers48h_H3, plot = TRUE)

# check preferred models
anova(divers48h_H0, divers48h_H1)
anova(divers48h_H0, divers48h_H2)
anova(divers48h_H2, divers48h_H2_1)
anova(divers48h_H0, divers48h_H3)

AIC(divers48h_H0, divers48h_H1, divers48h_H2, divers48h_H2_1, divers48h_H3) %>% arrange(AIC)
BIC(divers48h_H0, divers48h_H1, divers48h_H2, divers48h_H2_1, divers48h_H3) %>% arrange(BIC)

# create data.frame for plotting
divers_predict <- cbind(divers48h_H3$frame,
                        predict(divers48h_H3, type="response"))
colnames(divers_predict)[c(1,6)] <- c("observed", "predicted")
# plot the model predictions against the data
ggplot(divers_predict, 
       aes(x=Trtmt_Day, y=observed, colour=as.factor(CommRich))) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=as.factor(CommRich))) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Shannon diversity (rescaled)",
       colour="CommRich")
# cleanup
rm(divers_predict)

############################
# effect sizes
############################
## remember that we need to correct for multiple comparisons that were generated by subsetting the data into 4 parts
## THEREFORE CONSIDER ALPHA/N = 0.05/4 = 0.0125 as the threshold for significance
emm_6h <- emmeans(divers6h_H3, ~ Heat | CommRich + Trtmt_Day*protegens, data = diversity_6h, type = "response")
effect_6h <- eff_size(emm_6h, sigma(divers6h_H3), edf = df.residual(divers6h_H3))

emm_12h <- emmeans(divers12h_H3, ~ Heat | CommRich + Trtmt_Day*protegens, data = diversity_12h, type = "response")
effect_12h <- eff_size(emm_12h, sigma(divers12h_H3), edf = df.residual(divers12h_H3))

emm_24h <- emmeans(divers24h_H3, ~ Heat | CommRich + Trtmt_Day*protegens, data = diversity_24h, type = "response")
effect_24h <- eff_size(emm_24h, sigma(divers24h_H3), edf = df.residual(divers24h_H3))

emm_48h <- emmeans(divers48h_H3, ~ Heat | CommRich + Trtmt_Day*protegens, data = diversity_48h, type = "response")
effect_48h <- eff_size(emm_48h, sigma(divers48h_H3), edf = df.residual(divers48h_H3))

# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
div_effects_protegens <- data.frame()
div_effects_protegens <- rbind(div_effects_protegens,
                              get_effsize_CIs(effect_6h, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects_protegens$Trtmt_Day <- factor(div_effects_protegens$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(div_effects_protegens$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

# plot conditional part of the model
ggplot(div_effects_protegens,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day, shape = as.logical(protegens))) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(title = "(diversity + x)",
       x = "Effect Size on Shannon Diversity",
       shape = "protegens\npresent?",
       y="Heat duration")

#oh, wow this is why the 3rd model is highly preferred:
# there's a strong interaction between protegens & heat

# we can do a posthoc on this to illustrate statistically significant effects
posthoc_6h <- emmeans(effect_6h, pairwise ~ Trtmt_Day*protegens, data = diversity_6h)
posthoc_12h <- emmeans(effect_12h, pairwise ~ Trtmt_Day*protegens, data = diversity_12h)
posthoc_24h <- emmeans(effect_24h, pairwise ~ Trtmt_Day*protegens, data = diversity_24h)
posthoc_48h <- emmeans(effect_48h, pairwise ~ Trtmt_Day*protegens, data = diversity_48h)
# create a data.frame for plotting
div_effects <- data.frame()
div_effects <- rbind(div_effects,
                     get_posthoc(posthoc_6h, heat_trtmt = 6),
                     get_posthoc(posthoc_12h, heat_trtmt = 12),
                     get_posthoc(posthoc_24h, heat_trtmt = 24),
                     get_posthoc(posthoc_48h, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects$Trtmt_Day <- factor(div_effects$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(div_effects$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

# plot with group labels
ggplot(div_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day, shape=as.logical(protegens))) +
  facet_grid(~protegens) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-2.5, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Shannon Diversity",
       y="Heat duration",
       title = "(diversity + x)")

# anyway we can still average over the effect of protegens
# we can do a posthoc on this to illustrate statistically significant effects
posthoc_6h <- emmeans(effect_6h, pairwise ~ Trtmt_Day, data = diversity_6h)
posthoc_12h <- emmeans(effect_12h, pairwise ~ Trtmt_Day, data = diversity_12h)
posthoc_24h <- emmeans(effect_24h, pairwise ~ Trtmt_Day, data = diversity_24h)
posthoc_48h <- emmeans(effect_48h, pairwise ~ Trtmt_Day, data = diversity_48h)

# a function that extracts the confidence intervals from a post-hoc object
get_posthocTEMP <- function(posthoc_object, heat_trtmt) {
  output <- multcomp::cld(posthoc_object, alpha=0.05/4, Letters = letters) %>%
              data.frame() %>%
                select(-df)
  colnames(output)[2:6] <- c("est", "SE", "loCI", "hiCI", "groups")
  output$Heat <- heat_trtmt
  return(output)
}
# create a data.frame for plotting
div_effects <- data.frame()
div_effects <- rbind(div_effects,
                     get_posthocTEMP(posthoc_6h, heat_trtmt = 6),
                     get_posthocTEMP(posthoc_12h, heat_trtmt = 12),
                     get_posthocTEMP(posthoc_24h, heat_trtmt = 24),
                     get_posthocTEMP(posthoc_48h, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
div_effects$Trtmt_Day <- factor(div_effects$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(div_effects$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

# plot with group labels
ggplot(div_effects,
       aes(x = est, y = as.factor(Heat), colour = Trtmt_Day)) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-2.5, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Shannon Diversity",
       y="Heat duration",
       title = "averaged over protegens (diversity + x)")


# cleanup
rm(divers6h_H0, divers6h_H1, divers6h_H2, emm_6h, effect_6h,
   divers12h_H0, divers12h_H1, divers12h_H2, emm_12h, effect_12h,
   divers24h_H0, divers24h_H1, divers24h_H2, emm_24h, effect_24h,
   divers48h_H0, divers48h_H1, divers48h_H2, emm_48h, effect_48h,
   divers6h_H2_1, divers12h_H2_1, divers24h_H2_1, divers48h_H2_1,
   diversity_6h, diversity_12h, diversity_24h, diversity_48h,
   div_effects_protegens, divEffects_ttests,
   effect_6h_zi, effect_12h_zi, effect_24h_zi, effect_48h_zi,
   posthoc_6h, posthoc_12h, posthoc_24h, posthoc_48h,
   get_ZI_effsize_CIs, div_ZIeff,
   div_effects, div_overall_effects, diversity_forFit)
```


# Productivity stats

## Simple effect size

```{r, total_productivity}
## plot effect size of productivity (aka Total absolute density)
plot(ggplot(effectSize,
        aes(x=Day,
            y=TotDen_plusEpsilon_mean,
            colour=CommRich, 
            group=community)) +
    facet_grid(~Heat) +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    stat_summary(fun=mean, geom="line", alpha=0.9) +
    scale_y_log10() +
    scale_colour_viridis_d(option = "viridis", end=0.65) +
    labs(title="All data", colour="Inoculated\nRichness",
         y="Effect size on Total Density+epsilon"))

plot(ggplot(effectSize %>%
              filter(protegens==1),
        aes(x=Day,
            y=TotDen_plusEpsilon_mean,
            colour=CommRich, 
            group=community)) +
    facet_grid(~Heat) +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    stat_summary(fun=mean, geom="line", alpha=0.9) +
    scale_y_log10() +
    scale_colour_viridis_d(option = "viridis", end=0.65) +
    labs(title="Protegens included", colour="Inoculated\nRichness",
         y="Effect size on Total Density+epsilon"))

plot(ggplot(effectSize %>%
              filter(protegens==0),
        aes(x=Day,
            y=TotDen_plusEpsilon_mean,
            colour=CommRich, 
            group=community)) +
    facet_grid(~Heat) +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    stat_summary(fun=mean, geom="line", alpha=0.9) +
    scale_y_log10() +
    scale_colour_viridis_d(option = "viridis", end=0.65) +
    labs(title="Protegens excluded", colour="Inoculated\nRichness",
         y="Effect size on Total Density+epsilon"))
```

Let's fit this data to two types of linear models. 1) Log transform the Total Density +
Epsilon data then fit a (Gaussian) linear model. And 2) use the Total Density data
directly but fit a Negative Binomial linear model.

### Resistance

As above, we compare the fit of 3 different models.

```{r, total_productivity_RESIST}
#####################
# log10(Total Density + epsilon) transformed data
#####################
resist_prod0 <- with(effectSize %>% filter(Last_Heat_Day == TRUE) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                      lm(log_TotalDensity_plusE ~ CommRich + Heat + CommRich:Heat))
# Maddy's preferred model
print("RESISTANCE PRODUCTIVITY DATA. SUMMARY OF THE SIMPLEST MODEL:")
summary(resist_prod0)

resist_prod1 <- with(effectSize %>% filter(Last_Heat_Day == TRUE) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                      lm(log_TotalDensity_plusE ~ CommRich + withstands_heat + Heat + CommRich:withstands_heat + Heat:withstands_heat + CommRich:Heat))
print("RESISTANCE PRODUCTIVITY DATA. SUMMARY OF THE Putida MODEL:")
summary(resist_prod1)

resist_prod2 <- with(effectSize %>% filter(Last_Heat_Day == TRUE) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                      lm(log_TotalDensity_plusE ~ CommRich + protegens + Heat + CommRich:protegens + Heat:protegens + CommRich:Heat))

# compare the nested models
anova(resist_prod0, resist_prod1)
anova(resist_prod0, resist_prod2)
# compare all 3 models
AIC(resist_prod0, resist_prod1, resist_prod2) %>% arrange(AIC)
BIC(resist_prod0, resist_prod1, resist_prod2) %>% arrange(BIC)

# the statistically preferred model
print("")
print("RESISTANCE PRODUCTIVITY DATA. SUMMARY OF THE LOWEST AIC & BIC MODEL:")
print("")
summary(resist_prod2)
plot(resist_prod2)

# clean up
rm (resist_prod0, resist_prod1, resist_prod2)

# plot Maddy's preferred model
plot(ggplot(effectSize %>%
              filter(Last_Heat_Day == TRUE),
        aes(x=Heat,
            y=TotDen_plusEpsilon_mean,
            colour=CommRich, 
            group=CommRich)) +
    geom_hline(yintercept = 1, colour="grey") +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_y_log10() +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Resistance", colour="Inoculated\nRichness",
         y="Effect size on Total Density+epsilon", x="Heat Duration (hrs)"))

# plot the preferred model
plot(ggplot(effectSize %>%
              filter(Last_Heat_Day == TRUE),
        aes(x=Heat,
            y=TotDen_plusEpsilon_mean,
            colour=CommRich, 
            group=CommRich)) +
    facet_grid(~protegens) +
    geom_hline(yintercept = 1, colour="grey") +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_y_log10() +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Resistance (protegens present?)", colour="Inoculated\nRichness",
         y="Effect size on Total Density+epsilon", x="Heat Duration (hrs)"))

# clean up
rm(resist_prod0, resist_prod1, resist_prod2)
```

During resistance, the presence of protegens has a strong positive effect on productivity.
This makes sense because it can grow with heat. I expected to see a similar effect of
putida but for some reason we see a negative effect?!? Meanwhile, the strongest effect
overall is the negative effect of heat duration. The presence of protegens in the culture
reverses this effect.

### Recovery

```{r, total_productivity_RECOVER}
#####################
# log10(Total Density + epsilon) transformed data
#####################
recov_prod0 <- with(effectSize %>% filter(Recov_Day == 2) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                    lm(log_TotalDensity_plusE ~ CommRich + Heat + CommRich:Heat))
# Maddy's preferred model
print("RECOVERY PRODUCTIVITY DATA. SUMMARY OF THE SIMPLEST MODEL:")
summary(recov_prod0)

recov_prod1 <- with(effectSize %>% filter(Recov_Day == 2) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                      lm(log_TotalDensity_plusE ~ CommRich + withstands_heat + Heat + CommRich:withstands_heat + Heat:withstands_heat + CommRich:Heat))

recov_prod2 <- with(effectSize %>% filter(Recov_Day == 2) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                      lm(log_TotalDensity_plusE ~ CommRich + protegens + Heat + CommRich:protegens + Heat:protegens + CommRich:Heat))


# compare the nested models
anova(recov_prod0, recov_prod1)
anova(recov_prod0, recov_prod2)
# compare all 3 models
AIC(recov_prod0, recov_prod1, recov_prod2) %>% arrange(AIC)
BIC(recov_prod0, recov_prod1, recov_prod2) %>% arrange(BIC)

# the statistically preferred model
print("")
print("RECOVERY PRODUCTIVITY DATA. SUMMARY OF THE LOWEST AIC & BIC MODEL:")
print("")
summary(recov_prod2)
plot(recov_prod2)

# clean up
rm(recov_prod0, recov_prod1, recov_prod2)

# plot Maddy's preferred model
plot(ggplot(effectSize %>%
              filter(Recov_Day == 2),
        aes(x=Heat,
            y=Total_density_mean,
            colour=CommRich, 
            group=CommRich)) +
    geom_hline(yintercept = 1, colour="grey") +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_y_log10() +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Recovery", colour="Inoculated\nRichness",
         y="Effect size on Total density", x="Heat Duration (hrs)"))

# plot a preferred model
plot(ggplot(effectSize %>%
              filter(Recov_Day == 2),
        aes(x=Heat,
            y=Total_density_mean,
            colour=CommRich, 
            group=CommRich)) +
    facet_grid(~protegens) +
    geom_hline(yintercept = 1, colour="grey") +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_y_log10() +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Recovery (protegens present?)", colour="Inoculated\nRichness",
         y="Effect size on Total density", x="Heat Duration (hrs)"))

# plot a preferred model
plot(ggplot(effectSize %>%
              filter(Recov_Day == 2),
        aes(x=Heat,
            y=Total_density_mean,
            colour=CommRich, 
            group=CommRich)) +
    facet_grid(~withstands_heat) +
    geom_hline(yintercept = 1, colour="grey") +
    geom_jitter(alpha=0.2, size=0.8, width=0.05) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_y_log10() +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Recovery (withstands heat? AKA putida present?)", colour="Inoculated\nRichness",
         y="Effect size on Total density", x="Heat Duration (hrs)"))
```

```{r, over_recovery}
#####################
# log10(Total Density + epsilon) transformed data
#####################
overrecov0 <- with(effectSize %>% filter(Recov_Day == 1) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                    lm(log_TotalDensity_plusE ~ CommRich + Heat + CommRich:Heat))
# Maddy's preferred model
print("DAY1 RECOVERY PRODUCTIVITY DATA. SUMMARY OF THE SIMPLEST MODEL:")
summary(overrecov0)

overrecov1 <- with(effectSize %>% filter(Recov_Day == 1) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                      lm(log_TotalDensity_plusE ~ CommRich + withstands_heat + Heat + CommRich:withstands_heat + Heat:withstands_heat + CommRich:Heat))

overrecov2 <- with(effectSize %>% filter(Recov_Day == 1) %>%
                        mutate(log_TotalDensity_plusE = log(TotDen_plusEpsilon_mean)),
                      lm(log_TotalDensity_plusE ~ CommRich + protegens + Heat + CommRich:protegens + Heat:protegens + CommRich:Heat))

# compare nested models
anova(overrecov0, overrecov1)
anova(overrecov0, overrecov2)
# which is the best one?
AIC(overrecov0, overrecov1, overrecov2) %>% arrange(AIC)
BIC(overrecov0, overrecov1, overrecov2) %>% arrange(BIC)

# the statistically preferred model
print("")
print("DAY1 RECOVERY PRODUCTIVITY DATA. SUMMARY OF THE LOWEST AIC & BIC MODEL:")
print("")
summary(overrecov2)
plot(overrecov2)

# clean up
rm(overrecov0, overrecov1, overrecov2)

# plot Maddy's preferred model
plot(ggplot(effectSize %>% filter(Recov_Day == 1),
        aes(x=Heat, y=Total_density_mean,
            colour=CommRich, group=CommRich)) + 
    scale_y_log10() +
    geom_hline(yintercept = 1, colour="grey") +
    geom_jitter(alpha=0.2, size=0.8, width=0.1) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Day 1 of Recovery", colour="Inoculated\nRichness",
         y="Effect size on Total density"))

# plot the preferred model
plot(ggplot(effectSize %>% filter(Recov_Day == 1),
        aes(x=Heat, y=Total_density_mean,
            colour=CommRich, group=CommRich)) +
    facet_grid(~protegens) +
    scale_y_log10() +
    geom_hline(yintercept = 1, colour="grey") +
    geom_jitter(alpha=0.2, size=0.8, width=0.1) +
    geom_line(stat="smooth", method=lm, alpha=0.9) +
    scale_colour_viridis_d(option = "viridis", end=0.85) +
    labs(title="Day 1 of Recovery (protegens present?)", colour="Inoculated\nRichness",
         y="Effect size on Total density"))

# clean up
rm(effectSize)
```

One trend that I noticed as I scoured the data when I was trying to fit it to the gLV
model is that the biomass seems systematically higher on Day 1 of recovery. ...???

## Estimated marginal means (emmeans)

Now we repeat the same type of emmeans analysis as we did for diversity but using the
total density (aka a proxy of productivity). In this case I am *a priori* more comfortable
with using Poisson or negative binomial family because the total density is more like
counts data.

Remember that total densities below the threshold of detection from wells that *DID*
recover during the recovery phase (i.e., those that did *not* go extinct) have values of
epsilon corresponding to the threshold of detection. (Remaining NA values represent
missing data due to pipetting mistakes or clogs during flow cytometry.) Below threshold of
detection total density values (i.e., epsilons) make up the majority of observations
during resistance for the longest heat duration. See a further discussion in the section
below.

```{r, productivity_glmFamily}
# scale the data by its standard deviation
absDen_forFit$TotDensity_scale <- scale(absDen_forFit$Total_density,
                                        scale = sd(absDen_forFit$Total_density, na.rm = TRUE),
                                        center = FALSE)
# the max scaled value is ~7.9 and almost 3% of the data is 0 values
summary(absDen_forFit$TotDensity_scale)
sum(absDen_forFit$TotDensity_scale == 0) / length(absDen_forFit$TotDensity_scale)
# in fact, the total density data is even more long-tailed than the diversity data. I guess that makes sense as there is a max value for the possible diversity with 4 species.
hist(absDen_forFit$TotDensity_scale)

# re-arrange the levels so that emmeans can be run:
absDen_forFit$Heat <- as.character(absDen_forFit$Heat)
absDen_forFit$Heat[which(absDen_forFit$Heat == 0)] <- "control"
# !!! emmeans expects the control to be the very *last* level !!!
absDen_forFit$Heat <- factor(absDen_forFit$Heat,
                             levels = c("6", "12", "24", "48", "control"))
# let's keep CommRich and Day as numeric for now while we look for the best fitting GLM family

# let's compare different GLM families
try_gaussian <- glmmTMB(TotDensity_scale ~ CommRich*Heat*Day*protegens,
                        data = absDen_forFit,
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_gaussian, plot = TRUE)

try_gamma <- glmmTMB(TotDensity_scale ~ CommRich*Heat*Day*protegens,
                     data = absDen_forFit,
                     family = ziGamma,
                     ziformula = ~1, # this needs to be added because there are 0 values in the data
                     control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_gamma, plot = TRUE)


try_lognorm <- glmmTMB(TotDensity_scale ~ CommRich*Heat*Day*protegens,
                       data = absDen_forFit,
                       family = lognormal,
                       ziformula = ~1, # this needs to be added because there are 0 values in the data
                       control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_lognorm, plot = TRUE)

try_LOGlognorm <- glmmTMB(log(TotDensity_scale + 1) ~ CommRich*Heat*Day*protegens,
                          data = absDen_forFit,
                          family = lognormal,
                          ziformula = ~1, # I'm keeping this as 0-inflated lognormal alone was already over-dispersed. So I want to see if the log(x+1) transformation sufficiently brings in the long tail.
                          control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = try_LOGlognorm, plot = TRUE)

try_negbinom <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Day*protegens,
                        data = absDen_forFit,
                        family = nbinom2,
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom, plot = TRUE)

try_negbinom0 <- glmmTMB(as.integer(Total_density * 1000) ~ CommRich*Heat*Day*protegens,
                         data = absDen_forFit,
                         family = nbinom2,
                         ziformula = ~1, # try zero inflated distribution
                         control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_negbinom0, plot = TRUE)

try_poisson <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Day*protegens,
                       data = absDen_forFit,
                       family = genpois,
                       control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson, plot = TRUE)

try_poisson0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Day*protegens,
                        data = absDen_forFit,
                        family = genpois,
                        ziformula = ~1, # try zero inflated distribution
                        control = glmmTMBControl(optCtrl = list(iter.max = 10000,eval.max = 10000)))
simulateResiduals(fittedModel = try_poisson0, plot = TRUE)

# let's check this with AIC and BIC
AIC(try_gaussian, try_gamma, try_lognorm, try_LOGlognorm,
    try_negbinom, try_negbinom0, try_poisson, try_poisson0) %>% arrange(AIC)
BIC(try_gaussian, try_gamma, try_lognorm, try_LOGlognorm,
    try_negbinom, try_negbinom0, try_poisson, try_poisson0) %>% arrange(BIC)

# clean up
rm(try_gaussian, try_gamma, try_lognorm, try_LOGlognorm, try_negbinom, try_negbinom0, try_poisson, try_poisson0)
```

Okay, so let's go for the Poisson family. Its residuals look a little worse than the
log(x+1) transformed lognormal... But I feel really sketched out by the latter model.
Whereas the Poisson is the type of family that I might expect to see for count-style data
like the Total density.

### Including extinctions

First let's analyze productivity with the whole dataset, including the replicates for 24h
and 48h durations that were below the threshold of detection during resistance (NA values
that have been replaced with $\approx 0.086$) and true 0's that never recovered.

The resistance effect estimated for 24h duration should be treated with some skepticism as
12.1% of the data (8/66) is NA values, with all reps missing from P. grimontii
monocultures, 2 reps missing from P. veronii monocultures, and 1 rep missing from P.
putida monocultures.

And the resistance effect at 48h should be treated with extreme caution because 78.1% of
the data (50/64 replicates) is NA values. There are no monocultures from either slow
growing species, 1 monoculture from P. protegens, 1 monoculture from P. putida,

```{r, productivity_emmeans}
####################
# 6h heat duration
####################
# grab just the treatment with its associated control data
absDen_6h <- rbind(absDen_forFit %>% filter(Heat == "6"),
                   absDen_forFit %>% filter(Heat == "control", Day < 4))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_6h$Trtmt_Day <- "resist"
absDen_6h$Trtmt_Day[absDen_6h$Day == 2] <- "recov_1"
absDen_6h$Trtmt_Day[absDen_6h$Day == 3] <- "recov_2"

# try changing CommRich to unordered factor
absDen_6h$CommRich <- factor(absDen_6h$CommRich, ordered = FALSE)

# save the data to storage for later
productivitySubsettedData <- list(h6 = absDen_6h)

# try dropping inoculated community richness = 4 because it's unitary (i.e., it get dropped by glmmTMB then eff_size complains that it can't do anything with the resultant NA estimate values)
absDen_6h <- absDen_6h %>% filter(CommRich != 4)
absDen_6h$CommRich <- droplevels(absDen_6h$CommRich)

productivity6h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_6h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity6h_H0, plot = TRUE)

productivity6h_H1 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + putida*CommRich + putida*Heat,
                             data = absDen_6h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity6h_H1, plot = TRUE)

productivity6h_H2 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + protegens*CommRich + protegens*Heat,
                             data = absDen_6h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity6h_H2, plot = TRUE)

# check preferred models
anova(productivity6h_H0, productivity6h_H1)
anova(productivity6h_H0, productivity6h_H2)

AIC(productivity6h_H0, productivity6h_H1, productivity6h_H2) %>% arrange(AIC)
BIC(productivity6h_H0, productivity6h_H1, productivity6h_H2) %>% arrange(BIC)
# H2 is the preferred model
summary(productivity6h_H2)

# create data.frame for plotting
prod_predict <- cbind(productivity6h_H2$frame,
                      predict(productivity6h_H2, type="response"))
colnames(prod_predict)[c(1,6)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,6)] <- prod_predict[,c(1,6)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

####################
# 12h heat duration
####################
# grab just the treatment with its associated control data
absDen_12h <- rbind(absDen_forFit %>% filter(Heat == "12", Day > 1),
                    absDen_forFit %>% filter(Heat == "control", Day > 1, Day !=5))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_12h$Trtmt_Day <- "resist"
absDen_12h$Trtmt_Day[absDen_12h$Day == 3] <- "recov_1"
absDen_12h$Trtmt_Day[absDen_12h$Day == 4] <- "recov_2"

# change CommRich to unordered factor
absDen_12h$CommRich <- factor(absDen_12h$CommRich, ordered = FALSE)

# save the data to storage for later
productivitySubsettedData[["h12"]] <- absDen_12h

# drop inoculated community richness = 4 because it is unitary for models H1 and H2
absDen_12h <- absDen_12h %>% filter(CommRich != 4)
absDen_12h$CommRich <- droplevels(absDen_12h$CommRich)

productivity12h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_12h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity12h_H0, plot = TRUE)

productivity12h_H1 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + putida*CommRich + putida*Heat,
                             data = absDen_12h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity12h_H1, plot = TRUE)

productivity12h_H2 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + protegens*CommRich + protegens*Heat,
                             data = absDen_12h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity12h_H2, plot = TRUE)

# check preferred models
anova(productivity12h_H0, productivity12h_H1)
anova(productivity12h_H0, productivity12h_H2)

AIC(productivity12h_H0, productivity12h_H1, productivity12h_H2) %>% arrange(AIC)
BIC(productivity12h_H0, productivity12h_H1, productivity12h_H2) %>% arrange(BIC)
# H2 is the preferred model
summary(productivity12h_H2)

# create data.frame for plotting
prod_predict <- cbind(productivity12h_H2$frame,
                      predict(productivity12h_H2, type="response"))
colnames(prod_predict)[c(1,6)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,6)] <- prod_predict[,c(1,6)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

####################
# 24h heat duration
####################
# grab just the treatment with its associated control data
absDen_24h <- rbind(absDen_forFit %>% filter(Heat == "24", Day > 1),
                    absDen_forFit %>% filter(Heat == "control", Day > 1, Day !=5))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_24h$Trtmt_Day <- "resist"
absDen_24h$Trtmt_Day[absDen_24h$Day == 3] <- "recov_1"
absDen_24h$Trtmt_Day[absDen_24h$Day == 4] <- "recov_2"

# change CommRich to unordered factor
absDen_24h$CommRich <- factor(absDen_24h$CommRich, ordered = FALSE)

# save the data to storage for later
productivitySubsettedData[["h24"]] <- absDen_24h

# drop inoculated community richness = 4 because it is unitary for models H1 and H2
absDen_24h <- absDen_24h %>% filter(CommRich != 4)
absDen_24h$CommRich <- droplevels(absDen_24h$CommRich)

productivity24h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_24h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity24h_H0, plot = TRUE)

productivity24h_H1 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + putida*CommRich + putida*Heat,
                             data = absDen_24h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity24h_H1, plot = TRUE)

productivity24h_H2 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + protegens*CommRich + protegens*Heat,
                             data = absDen_24h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity24h_H2, plot = TRUE)

# check preferred models
anova(productivity24h_H0, productivity24h_H1)
anova(productivity24h_H0, productivity24h_H2)

AIC(productivity24h_H0, productivity24h_H1, productivity24h_H2) %>% arrange(AIC)
BIC(productivity24h_H0, productivity24h_H1, productivity24h_H2) %>% arrange(BIC)
# H2 is the preferred model
summary(productivity24h_H2)

# create data.frame for plotting
prod_predict <- cbind(productivity24h_H2$frame,
                      predict(productivity24h_H2, type="response"))
colnames(prod_predict)[c(1,6)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,6)] <- prod_predict[,c(1,6)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

####################
# 48h heat duration
####################
# grab just the treatment with its associated control data
absDen_48h <- rbind(absDen_forFit %>% filter(Heat == "48", Day > 2),
                    absDen_forFit %>% filter(Heat == "control", Day > 2))
# create a column for last day of heat, first day of recovery, and last day of recovery
absDen_48h$Trtmt_Day <- "resist"
absDen_48h$Trtmt_Day[absDen_48h$Day == 4] <- "recov_1"
absDen_48h$Trtmt_Day[absDen_48h$Day == 5] <- "recov_2"

# change CommRich to unordered factor
absDen_48h$CommRich <- factor(absDen_48h$CommRich, ordered = FALSE)

# save the data to storage for later
productivitySubsettedData[["h48"]] <- absDen_48h

# drop inoculated community richness = 4 because it is unitary for models H1 and H2
absDen_48h <- absDen_48h %>% filter(CommRich != 4)
absDen_48h$CommRich <- droplevels(absDen_48h$CommRich)

productivity48h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_48h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity48h_H0, plot = TRUE)

productivity48h_H1 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + putida*CommRich + putida*Heat,
                             data = absDen_48h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity48h_H1, plot = TRUE)

productivity48h_H2 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + protegens*CommRich + protegens*Heat,
                             data = absDen_48h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity48h_H2, plot = TRUE)

# check preferred models
anova(productivity48h_H0, productivity48h_H1)
anova(productivity48h_H0, productivity48h_H2)

AIC(productivity48h_H0, productivity48h_H1, productivity48h_H2) %>% arrange(AIC)
BIC(productivity48h_H0, productivity48h_H1, productivity48h_H2) %>% arrange(BIC)
# H2 is the preferred model
summary(productivity48h_H2)

# create data.frame for plotting
prod_predict <- cbind(productivity48h_H2$frame,
                      predict(productivity48h_H2, type="response"))
colnames(prod_predict)[c(1,6)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,6)] <- prod_predict[,c(1,6)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

#######################
# effect sizes
#######################

# plot the effect size contingent on protegens
effect_6h_protegens <- eff_size(emmeans(productivity6h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = absDen_6h),
                                sigma(productivity6h_H2),
                                edf = df.residual(productivity6h_H2))
effect_12h_protegens <- eff_size(emmeans(productivity12h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = absDen_12h),
                                sigma(productivity12h_H2),
                                edf = df.residual(productivity12h_H2))
effect_24h_protegens <- eff_size(emmeans(productivity24h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = absDen_24h),
                                sigma(productivity24h_H2),
                                edf = df.residual(productivity24h_H2))
effect_48h_protegens <- eff_size(emmeans(productivity48h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = absDen_48h),
                                sigma(productivity48h_H2),
                                edf = df.residual(productivity48h_H2))

# a function that extracts the confidence intervals from eff_size contingent on protegens
get_effsize_CIs <- function(eff_size_object, heat_trtmt) {
  data.frame(Heat = heat_trtmt,
             CommRich = confint(eff_size_object)[[2]],
             Trtmt_Day = confint(eff_size_object)[[3]],
             protegens = confint(eff_size_object)[[4]],
             effect_est = confint(eff_size_object)[[5]],
             effect_loCI = confint(eff_size_object)[[8]],
             effect_hiCI = confint(eff_size_object)[[9]])
}
# create a data.frame for plotting marginal effect sizes using a forest plot
productivity_protegens <- data.frame()
productivity_protegens <- rbind(productivity_protegens,
                              get_effsize_CIs(effect_6h_protegens, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h_protegens, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h_protegens, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h_protegens, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_protegens$Trtmt_Day <- factor(productivity_protegens$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(productivity_protegens$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

ggplot(productivity_protegens,
       aes(x = effect_est, y = CommRich, colour = Trtmt_Day, shape = as.logical(protegens))) +
  facet_grid(. ~ Heat) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = effect_loCI, xmax = effect_hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Innoculated Community Richness",
       shape = "protegens\npresent?",
       title = "(with extinct reps)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
# only 48h has any noticable effect with protegens
# let's look at the post-hoc just for 48h with protegens
posthoc_48h_protegens <- emmeans(effect_48h_protegens,
                                 pairwise ~ CommRich + Trtmt_Day + protegens,
                                 data = absDen_48h)
print("Post-hoc for productivity at 48h (extinct replicates INCLUDED) conditional on protegens:")
multcomp::cld(posthoc_48h_protegens, alpha=0.05/4, Letters = letters)
# okay, this makes sense. Only 48h of heat is long enough to depress total density even after 2 days of recovery
# But, in the presence of protegens, total density can bounce back. Meanwhile, in the absence of protegens, total density does not bounce back even after 2 days of recovery.


# But we are not interested in the details of protegens. Let's do the post-hoc again now averaging across the effects of protegens.

posthoc_6h <- emmeans(effect_6h_protegens,
                      pairwise ~ CommRich + Trtmt_Day,
                      data = absDen_6h)
posthoc_12h <- emmeans(effect_12h_protegens,
                       pairwise ~ CommRich + Trtmt_Day,
                       data = absDen_12h)
posthoc_24h <- emmeans(effect_24h_protegens,
                       pairwise ~ CommRich + Trtmt_Day,
                       data = absDen_24h)
posthoc_48h <- emmeans(effect_48h_protegens,
                       pairwise ~ CommRich + Trtmt_Day,
                       data = absDen_48h)

# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
productivity_effects <- data.frame()
productivity_effects <- rbind(productivity_effects,
                              get_posthoc(posthoc_6h, heat_trtmt = 6),
                              get_posthoc(posthoc_12h, heat_trtmt = 12),
                              get_posthoc(posthoc_24h, heat_trtmt = 24),
                              get_posthoc(posthoc_48h, heat_trtmt = 48))

# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_effects$Trtmt_Day <- factor(productivity_effects$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(productivity_effects$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

# plot
ggplot(productivity_effects,
       aes(x = est, y = CommRich, colour = Trtmt_Day)) +
  facet_grid(. ~ Heat) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-0.009, label=groups)) +
  scale_x_continuous(breaks=c(-0.006, -0.003, 0), limits=c(-0.01, 0.003)) + 
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Innoculated Community Richness",
       title = "Averaged across protegens (with extinct reps)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# finally, do the t-tests
# estimate the sample sizes
temp <- productivity_effects # copy the effects to temp
productivity_effects <- rbind(temp %>% filter(Heat == 6, CommRich == 1) %>% mutate(n = estimate_n(absDen_6h, CommRich=1)),
                              temp %>% filter(Heat == 6, CommRich == 2) %>% mutate(n = estimate_n(absDen_6h, CommRich=2)),
                              temp %>% filter(Heat == 6, CommRich == 3) %>% mutate(n = estimate_n(absDen_6h, CommRich=3)),
                              temp %>% filter(Heat == 12, CommRich == 1) %>% mutate(n = estimate_n(absDen_12h, CommRich=1)),
                              temp %>% filter(Heat == 12, CommRich == 2) %>% mutate(n = estimate_n(absDen_12h, CommRich=2)),
                              temp %>% filter(Heat == 12, CommRich == 3) %>% mutate(n = estimate_n(absDen_12h, CommRich=3)),
                              temp %>% filter(Heat == 24, CommRich == 1) %>% mutate(n = estimate_n(absDen_24h, CommRich=1)),
                              temp %>% filter(Heat == 24, CommRich == 2) %>% mutate(n = estimate_n(absDen_24h, CommRich=2)),
                              temp %>% filter(Heat == 24, CommRich == 3) %>% mutate(n = estimate_n(absDen_24h, CommRich=3)),
                              temp %>% filter(Heat == 48, CommRich == 1) %>% mutate(n = estimate_n(absDen_48h, CommRich=1)),
                              temp %>% filter(Heat == 48, CommRich == 2) %>% mutate(n = estimate_n(absDen_48h, CommRich=2)),
                              temp %>% filter(Heat == 48, CommRich == 3) %>% mutate(n = estimate_n(absDen_48h, CommRich=3)))
rm(temp)
# estimate the SD from the SE
productivity_effects <- productivity_effects %>% mutate(SD = SE * sqrt(n)) %>%
    # re-order by Heat, Trtmt_Day, and CommRich
                          arrange(Heat, Trtmt_Day, CommRich)

# all pairwise combinations of comparisons between the same treatment day for different durations
temp <- t(combn(c(1,10,19,28), 2))
combos <- rbind(temp, temp+1, temp+2, temp+3, temp+4, temp+5, temp+6, temp+7, temp+8)
rm(temp)

# loop through all the combinations and do the t-tests
prodEffects_ttests <- data.frame()
for(i in 1:nrow(combos)){
  prodEffects_ttests <- rbind(prodEffects_ttests,
                              run_ttest(row_x = combos[i,1],
                                        row_y = combos[i,2],
                                        summary_stats_df=productivity_effects))
}
prodEffects_ttests$adjusted_p <- p.adjust(prodEffects_ttests$pvalue, method = "bonferroni")
prodEffects_ttests$Trtmt_Day <- productivity_effects$Trtmt_Day[combos[,1]]
prodEffects_ttests$Heat_1 <- productivity_effects$Heat[combos[,1]]
prodEffects_ttests$Heat_2 <- productivity_effects$Heat[combos[,2]]
prodEffects_ttests$CommRich_1 <- productivity_effects$CommRich[combos[,1]]
prodEffects_ttests$CommRich_2 <- productivity_effects$CommRich[combos[,2]]

print(prodEffects_ttests)
# plot again without the group labels
ggplot(productivity_effects,
       aes(x = est, y = CommRich, colour = Trtmt_Day)) +
  facet_grid(. ~ Heat) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_x_continuous(breaks=c(-0.006, -0.003, 0), limits=c(-0.01, 0.003)) + 
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Innoculated Community Richness",
       title = "Averaged across protegens (with extinct reps)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

rm(productivity6h_H0, productivity6h_H1, productivity6h_H2,
   productivity12h_H0, productivity12h_H1, productivity12h_H2,
   productivity24h_H0, productivity24h_H1, productivity24h_H2,
   productivity48h_H0, productivity48h_H1, productivity48h_H2,
   effect_6h_protegens, effect_12h_protegens, effect_24h_protegens, effect_48h_protegens,
   posthoc_6h, posthoc_12h, posthoc_24h, posthoc_48h, posthoc_48h_protegens,
   productivity_protegens, prodEffects_ttests)
```

A main problem of our analysis is that I can't do post-hoc to get statistical significance
between different heat durations (i.e., because the effect sizes come from different data
sub-sets). Here I have followed Maddy's suggestion that I do all pairwise combinations of
two-tailed t-tests to compare between models. To do it more elegantly, maybe I can re-do
the models in the bayesian way using the package `brms`???

When we plot the effect sizes contingent on protegens, we see that there's little
interaction between protegens and heat, until the very longest heat duration. On the other
hand, the presense of protegens doesn't change the presence or extent of decoupling. This
is why a posthoc analysis was used to average over the effect of protegens.

The main thing we see is that total densities are always lowest during resistance as
compared to recovery days (which makes sense because there is heat). When we compare
within resistance or within recovery between different heat durations, we find that
shorter heat durations (6h VS 12h) tend to not be different from each other and long heat
durations (24h VS 48h) tend to not be different from each other. But 6h and 48h are always
different from each other.

I'm not really convinced that anything in particular is happening in terms of the
biodiversity effect...

Let's look at decoupling:

Given the thermal performance curves of the 4 species, we would expect to see a
correlation between resistance and recovery for both the slow growing species (i.e., low
resistance during heat and slow recovery post heat) and for the fast growing species
(i.e., high resistance during heat and fast recovery post heat). I'm not sure what
intuition I would have *a priori* about the effect of diversity on decoupling... But this
is what Maddy is interested in.

```{r, decoupling_productivity}
# a function that takes the multcomp::cld letters from 2 groups and returns TRUE when no letters are shared (or FALSE when any letter is shared)
are_groups_different <- function(group1, group2) {
  # convert the groups columns into TRUE/FALSE columns indicating significant difference between resistance and recovery effect sizes
  first_group <- group1 %>%
                  # remove any white space
                  str_trim() %>%
                    # split the string up into single characters
                    strsplit(x=., split = character(0))
  second_group <- group2 %>%
                  # remove any white space
                  str_trim() %>%
                    # split the string up into single letters
                    strsplit(x=., split = character(0))
  # test if any letters are common. If there are, then they are NOT different so return FALSE (and vice versa).
  return( !any(first_group[[1]] %in% second_group[[1]]) )
}

# a function to calculate distance from the point (x, y) to the line y = x: positive values are ABOVE the line and negative values are BELOW the line.
# this is used to calculate decoupling
dist_to_xyline <- function(x, y) {
  (y - x) / sqrt(2)  # distance formula derived from y = x line
}

# a function to estimate mean decoupling and its confidence intervals given mean and SYMMETRIC confidence intervals for resistance and recovery.
# Note that I can use the univariate confidence intervals only by assuming there's no correlation between resistance and recovery (which is exactly the opposite of the whole point of coupling)
# ...also, beware the the CI's come from a posthoc so they are more conservative that the real CI's...
estimate_decoupling <- function(resist_est, resist_hiCI,
                                recov_est, recov_hiCI) {
  # check the input values
  if(resist_hiCI < resist_est)
    stop("`resist_hiCI` must be the *UPPER* confidence interval on resistance.")
  if(recov_hiCI < recov_est)
    stop("`recov_hiCI` must be the *UPPER* confidence interval on recovery.")
  
  # get the co-ordinates that define the ellipse
  x0 <- resist_est # x-coordinate of the center of the ellipse
  y0 <- recov_est # y-coordinate of the center of the ellipse
  a <- resist_hiCI - resist_est # semi-major axis: horizontal radius
  b <- recov_hiCI - recov_est # semi-major axis: vertical radius
  
  # generate points on the perimeter of the ellipse
  theta <- seq(0, 2 * pi, length.out = 360)  # angles
  x_ellipse <- x0 + a * cos(theta)  # x-coordinates on the ellipse
  y_ellipse <- y0 + b * sin(theta)   # y-coordinates on the ellipse
  
  # decoupling measures the distance between the point and the y=x line
  mean <- dist_to_xyline(x0, y0)
  
  # do the same for all points on the ellipse defining the CI
  distances <- dist_to_xyline(x_ellipse, y_ellipse)
  
  # maximum and minimum distances define the hiCI and loCI, respectively
  hiCI <- max(distances)
  loCI <- min(distances)
  
  return(c(est_decoupling = mean, loCI_decoupling = loCI, hiCI_decoupling = hiCI))
}
# positive values are ABOVE the y=x line and negative values are BELOW the y=x line



# rename the levels of Trtmt_Day
decoupling_productivity <- productivity_effects
levels(decoupling_productivity$Trtmt_Day) <- c("late_recov", "early_recov", "resist")

# create data.frame for plotting
decoupling_productivity <- decoupling_productivity %>%
                            select(-n, -SD) %>%
                              pivot_wider(names_from = Trtmt_Day,
                                          values_from = c(est, loCI, hiCI, SE, groups))
# columns that indicate if resistance is significantly different from recovery
decoupling_productivity$early_recov_VS_resist <- mapply(are_groups_different,
                                                        decoupling_productivity$groups_early_recov,
                                                        decoupling_productivity$groups_resist)
decoupling_productivity$late_recov_VS_resist <- mapply(are_groups_different,
                                                       decoupling_productivity$groups_late_recov,
                                                       decoupling_productivity$groups_resist)
# clean up extra columns
decoupling_productivity <- decoupling_productivity %>% select(-groups_resist, -groups_early_recov, -groups_late_recov)


# first plot the decoupling on early recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
  facet_grid(~CommRich) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_early_recov, ymax = hiCI_early_recov), width=0) +
# in case we want to center the plot on 0,0
  #scale_x_continuous(limits = c(-0.01, 0.01)) +
  #scale_y_continuous(limits = c(-0.004, 0.004))
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (with extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Early Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# here's another way to plot it where the confidence intervals are shown as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
    facet_grid(~CommRich) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_early_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_early_recov - est_early_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (with extinct reps)",
         x = "Resistance +/- 95% CI",
         y = "Early Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# next plot the decoupling on later recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
  facet_grid(~CommRich) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_late_recov, ymax = hiCI_late_recov), width=0) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (with extinct reps)",
       x = "Resistance +/- 95% CI",
       y = "Late Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# finally estimate decoupling by getting the distance to the y=x line
# calculate decoupling between resistance and early recovery
early_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_early_recov,
                                  recov_hiCI = hiCI_early_recov)))
# add annotation
early_decoupling <- cbind(decoupling_productivity[,1:2],
                          early_decoupling)

ggplot(early_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling, colour = as.factor(CommRich))) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  labs(title = "Early recovery (WITH extinct reps)",
       colour = "Innoculated\nCommunity\nRichness",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")

# calculate decoupling between resistance and late recovery
late_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_late_recov,
                                  recov_hiCI = hiCI_late_recov)))
# add annotation
late_decoupling <- cbind(decoupling_productivity[,1:2],
                          late_decoupling)

ggplot(late_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling, colour = as.factor(CommRich))) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  labs(title = "Late recovery (WITH extinct reps)",
       colour = "Innoculated\nCommunity\nRichness",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")

rm(productivity_effects, decoupling_productivity, early_decoupling, late_decoupling)
```

### Excluding extinct communities & replicates

Let's try the analysis again but this time altogether dropping from the analysis any
communities where all replicates went extinct at 48h (i.e., this means that we drop the
slowing growing monocultures,0_0_0_1 & 0_0_1_0, and the pair of slow growers, 0_0_1_1,
from all heat treatments). We will also drop all time points for any replicates that went
extinct.

```{r, productivity_emmeans_RemoveExtinct}
# for 24h: summarize the total number of reps and the fraction missing during resistance & recovery
absDensity %>% filter(Day %in% 1:3, Heat == 24, !is.na(Total_density), !is.na(CommRich), Total_density > 0) %>% group_by(community, CommRich, Day) %>% summarise(reps = n()) %>% pivot_wider(names_from = Day, values_from = reps, names_prefix = "reps_") %>% mutate(resist_num_n = ifelse(is.na(reps_2),0,reps_2), recov_num_n = ifelse(is.na(reps_3),0,reps_3),.keep="unused")%>% mutate(resist_24h_frac_reps = resist_num_n/reps_1, recov_24h_frac_reps=recov_num_n/reps_1) %>% select(-resist_num_n, -recov_num_n) %>% rename(num_reps = reps_1) %>% arrange(CommRich)

# for 48h: summarize the total number of reps and the fraction missing during resistance & recovery
absDensity %>% filter(Day %in% c(1,3:4), Heat == 48, !is.na(Total_density), !is.na(CommRich), Total_density > 0) %>% group_by(community, CommRich, Day) %>% summarise(reps = n()) %>% pivot_wider(names_from = Day, values_from = reps, names_prefix = "reps_") %>% mutate(resist_num_n = ifelse(is.na(reps_3),0,reps_3), recov_num_n = ifelse(is.na(reps_4),0,reps_4),.keep="unused")%>% mutate(resist_48h_frac_reps = resist_num_n/reps_1, recov_48h_frac_reps=recov_num_n/reps_1) %>% select(-resist_num_n, -recov_num_n) %>% rename(num_reps = reps_1) %>% arrange(CommRich)

# remove the data with extinction
survived_uniqIDs <- extinct.df$uniqID[!is.na(extinct.df$survived) & extinct.df$survived == 1]

####################
# 6h heat duration EXCLUDING EXTINCT
####################
# exclude any replicates that experienced extinction
absDen_6h <- absDen_6h %>% filter(uniqID %in% survived_uniqIDs)

productivity6h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_6h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity6h_H0, plot = TRUE)

productivity6h_H1 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + putida*CommRich + putida*Heat,
                             data = absDen_6h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity6h_H1, plot = TRUE)

productivity6h_H2 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + protegens*CommRich + protegens*Heat,
                             data = absDen_6h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity6h_H2, plot = TRUE)

# check preferred models
anova(productivity6h_H0, productivity6h_H1)
anova(productivity6h_H0, productivity6h_H2)

AIC(productivity6h_H0, productivity6h_H1, productivity6h_H2) %>% arrange(AIC)
BIC(productivity6h_H0, productivity6h_H1, productivity6h_H2) %>% arrange(BIC)
# H2 is the preferred model
summary(productivity6h_H2)

# create data.frame for plotting
prod_predict <- cbind(productivity6h_H2$frame,
                      predict(productivity6h_H2, type="response"))
colnames(prod_predict)[c(1,6)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,6)] <- prod_predict[,c(1,6)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

####################
# 12h heat duration EXCLUDING EXTINCT
####################
# exclude any replicates that experienced extinction
absDen_12h <- absDen_12h %>% filter(uniqID %in% survived_uniqIDs)

productivity12h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_12h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity12h_H0, plot = TRUE)

productivity12h_H1 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + putida*CommRich + putida*Heat,
                             data = absDen_12h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity12h_H1, plot = TRUE)

productivity12h_H2 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + protegens*CommRich + protegens*Heat,
                             data = absDen_12h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity12h_H2, plot = TRUE)

# check preferred models
anova(productivity12h_H0, productivity12h_H1)
anova(productivity12h_H0, productivity12h_H2)

AIC(productivity12h_H0, productivity12h_H1, productivity12h_H2) %>% arrange(AIC)
BIC(productivity12h_H0, productivity12h_H1, productivity12h_H2) %>% arrange(BIC)
# H2 is the preferred model
summary(productivity12h_H2)

# create data.frame for plotting
prod_predict <- cbind(productivity12h_H2$frame,
                      predict(productivity12h_H2, type="response"))
colnames(prod_predict)[c(1,6)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,6)] <- prod_predict[,c(1,6)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

####################
# 24h heat duration EXCLUDING EXTINCT
####################
# exclude any replicates that experienced extinction
absDen_24h <- absDen_24h %>% filter(uniqID %in% survived_uniqIDs)

productivity24h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_24h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity24h_H0, plot = TRUE)

productivity24h_H1 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + putida*CommRich + putida*Heat,
                             data = absDen_24h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity24h_H1, plot = TRUE)

productivity24h_H2 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + protegens*CommRich + protegens*Heat,
                             data = absDen_24h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity24h_H2, plot = TRUE)

# check preferred models
anova(productivity24h_H0, productivity24h_H1)
anova(productivity24h_H0, productivity24h_H2)

AIC(productivity24h_H0, productivity24h_H1, productivity24h_H2) %>% arrange(AIC)
BIC(productivity24h_H0, productivity24h_H1, productivity24h_H2) %>% arrange(BIC)
# H2 is the preferred model
summary(productivity24h_H2)

# create data.frame for plotting
prod_predict <- cbind(productivity24h_H2$frame,
                      predict(productivity24h_H2, type="response"))
colnames(prod_predict)[c(1,6)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,6)] <- prod_predict[,c(1,6)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

####################
# 48h heat duration EXCLUDING EXTINCT
####################
# exclude any replicates that experienced extinction
absDen_48h <- absDen_48h %>% filter(uniqID %in% survived_uniqIDs)

productivity48h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_48h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity48h_H0, plot = TRUE)

productivity48h_H1 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + putida*CommRich + putida*Heat,
                             data = absDen_48h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity48h_H1, plot = TRUE)

productivity48h_H2 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day + protegens*CommRich + protegens*Heat,
                             data = absDen_48h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))
simulateResiduals(fittedModel = productivity48h_H2, plot = TRUE)

# check preferred models
anova(productivity48h_H0, productivity48h_H1)
anova(productivity48h_H0, productivity48h_H2)

AIC(productivity48h_H0, productivity48h_H1, productivity48h_H2) %>% arrange(AIC)
BIC(productivity48h_H0, productivity48h_H1, productivity48h_H2) %>% arrange(BIC)
# H2 is the preferred model
summary(productivity48h_H2)

# create data.frame for plotting
prod_predict <- cbind(productivity48h_H2$frame,
                      predict(productivity48h_H2, type="response"))
colnames(prod_predict)[c(1,6)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,6)] <- prod_predict[,c(1,6)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(protegens~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

# plot the effect size contingent on protegens
effect_6h_protegens <- eff_size(emmeans(productivity6h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = absDen_6h),
                                sigma(productivity6h_H2),
                                edf = df.residual(productivity6h_H2))
effect_12h_protegens <- eff_size(emmeans(productivity12h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = absDen_12h),
                                sigma(productivity12h_H2),
                                edf = df.residual(productivity12h_H2))
effect_24h_protegens <- eff_size(emmeans(productivity24h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = absDen_24h),
                                sigma(productivity24h_H2),
                                edf = df.residual(productivity24h_H2))
effect_48h_protegens <- eff_size(emmeans(productivity48h_H2, ~ Heat | CommRich*Trtmt_Day + protegens, data = absDen_48h),
                                sigma(productivity48h_H2),
                                edf = df.residual(productivity48h_H2))

# a function that extracts the confidence intervals from eff_size contingent on protegens
get_effsize_CIs <- function(eff_size_object, heat_trtmt) {
  data.frame(Heat = heat_trtmt,
             CommRich = confint(eff_size_object)[[2]],
             Trtmt_Day = confint(eff_size_object)[[3]],
             protegens = confint(eff_size_object)[[4]],
             effect_est = confint(eff_size_object)[[5]],
             effect_loCI = confint(eff_size_object)[[8]],
             effect_hiCI = confint(eff_size_object)[[9]])
}
# create a data.frame for plotting marginal effect sizes using a forest plot
productivity_protegens <- data.frame()
productivity_protegens <- rbind(productivity_protegens,
                              get_effsize_CIs(effect_6h_protegens, heat_trtmt = 6),
                              get_effsize_CIs(effect_12h_protegens, heat_trtmt = 12),
                              get_effsize_CIs(effect_24h_protegens, heat_trtmt = 24),
                              get_effsize_CIs(effect_48h_protegens, heat_trtmt = 48))
# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_protegens$Trtmt_Day <- factor(productivity_protegens$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(productivity_protegens$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

ggplot(productivity_protegens,
       aes(x = effect_est, y = CommRich, colour = Trtmt_Day, shape = as.logical(protegens))) +
  facet_grid(. ~ Heat) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = effect_loCI, xmax = effect_hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Innoculated Community Richness",
       shape = "protegens\npresent?",
       title = "(WITHOUT extinct reps)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
# only 48h has any noticable effect with protegens
# let's look at the post-hoc just for 48h with protegens
posthoc_48h_protegens <- emmeans(effect_48h_protegens,
                                 pairwise ~ CommRich + Trtmt_Day + protegens,
                                 data = absDen_48h)
print("Post-hoc for productivity at 48h (WITHOUT extinct replicates) conditional on protegens:")
multcomp::cld(posthoc_48h_protegens, alpha=0.05/4, Letters = letters)
# okay, this makes sense. Only 48h of heat is long enough to depress total density even after 2 days of recovery
# But, in the presence of protegens, total density can bounce back. Meanwhile, in the absence of protegens, total density does not bounce back even after 2 days of recovery.


# But we are not interested in the details of protegens. Let's do the post-hoc again now averaging across the effects of protegens.

posthoc_6h <- emmeans(effect_6h_protegens,
                      pairwise ~ CommRich + Trtmt_Day,
                      data = absDen_6h)
posthoc_12h <- emmeans(effect_12h_protegens,
                       pairwise ~ CommRich + Trtmt_Day,
                       data = absDen_12h)
posthoc_24h <- emmeans(effect_24h_protegens,
                       pairwise ~ CommRich + Trtmt_Day,
                       data = absDen_24h)
posthoc_48h <- emmeans(effect_48h_protegens,
                       pairwise ~ CommRich + Trtmt_Day,
                       data = absDen_48h)

# create a data.frame for plotting marginal effect sizes using a forest plot with the group labels
productivity_effects <- data.frame()
productivity_effects <- rbind(productivity_effects,
                              get_posthoc(posthoc_6h, heat_trtmt = 6),
                              get_posthoc(posthoc_12h, heat_trtmt = 12),
                              get_posthoc(posthoc_24h, heat_trtmt = 24),
                              get_posthoc(posthoc_48h, heat_trtmt = 48))

# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_effects$Trtmt_Day <- factor(productivity_effects$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(productivity_effects$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")

# plot
ggplot(productivity_effects,
       aes(x = est, y = CommRich, colour = Trtmt_Day)) +
  facet_grid(. ~ Heat) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  geom_text(position = position_dodge(width = 0.5),
            aes(x=-0.015, label=groups)) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Innoculated Community Richness",
       title = "Averaged across protegens (WITHOUT extinct reps)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# finally, do the t-tests
# estimate the sample sizes
temp <- productivity_effects # copy the effects to temp
productivity_effects <- rbind(temp %>% filter(Heat == 6, CommRich == 1) %>% mutate(n = estimate_n(absDen_6h, CommRich=1)),
                              temp %>% filter(Heat == 6, CommRich == 2) %>% mutate(n = estimate_n(absDen_6h, CommRich=2)),
                              temp %>% filter(Heat == 6, CommRich == 3) %>% mutate(n = estimate_n(absDen_6h, CommRich=3)),
                              temp %>% filter(Heat == 12, CommRich == 1) %>% mutate(n = estimate_n(absDen_12h, CommRich=1)),
                              temp %>% filter(Heat == 12, CommRich == 2) %>% mutate(n = estimate_n(absDen_12h, CommRich=2)),
                              temp %>% filter(Heat == 12, CommRich == 3) %>% mutate(n = estimate_n(absDen_12h, CommRich=3)),
                              temp %>% filter(Heat == 24, CommRich == 1) %>% mutate(n = estimate_n(absDen_24h, CommRich=1)),
                              temp %>% filter(Heat == 24, CommRich == 2) %>% mutate(n = estimate_n(absDen_24h, CommRich=2)),
                              temp %>% filter(Heat == 24, CommRich == 3) %>% mutate(n = estimate_n(absDen_24h, CommRich=3)),
                              temp %>% filter(Heat == 48, CommRich == 1) %>% mutate(n = estimate_n(absDen_48h, CommRich=1)),
                              temp %>% filter(Heat == 48, CommRich == 2) %>% mutate(n = estimate_n(absDen_48h, CommRich=2)),
                              temp %>% filter(Heat == 48, CommRich == 3) %>% mutate(n = estimate_n(absDen_48h, CommRich=3)))
rm(temp)
# estimate the SD from the SE
productivity_effects <- productivity_effects %>% mutate(SD = SE * sqrt(n)) %>%
    # re-order by Heat, Trtmt_Day, and CommRich
                          arrange(Heat, Trtmt_Day, CommRich)

# all pairwise combinations of comparisons between the same treatment day for different durations
temp <- t(combn(c(1,10,19,28), 2))
combos <- rbind(temp, temp+1, temp+2, temp+3, temp+4, temp+5, temp+6, temp+7, temp+8)
rm(temp)

# loop through all the combinations and do the t-tests
prodEffects_ttests <- data.frame()
for(i in 1:nrow(combos)){
  prodEffects_ttests <- rbind(prodEffects_ttests,
                              run_ttest(row_x = combos[i,1],
                                        row_y = combos[i,2],
                                        summary_stats_df=productivity_effects))
}
prodEffects_ttests$adjusted_p <- p.adjust(prodEffects_ttests$pvalue, method = "bonferroni")
prodEffects_ttests$Trtmt_Day <- productivity_effects$Trtmt_Day[combos[,1]]
prodEffects_ttests$Heat_1 <- productivity_effects$Heat[combos[,1]]
prodEffects_ttests$Heat_2 <- productivity_effects$Heat[combos[,2]]
prodEffects_ttests$CommRich_1 <- productivity_effects$CommRich[combos[,1]]
prodEffects_ttests$CommRich_2 <- productivity_effects$CommRich[combos[,2]]

print(prodEffects_ttests)
# plot again without the group labels
ggplot(productivity_effects,
       aes(x = est, y = CommRich, colour = Trtmt_Day)) +
  facet_grid(. ~ Heat) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Innoculated Community Richness",
       title = "Averaged across protegens (WITHOUT extinct reps)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

rm(productivity6h_H0, productivity6h_H1, productivity6h_H2,
   productivity12h_H0, productivity12h_H1, productivity12h_H2,
   productivity24h_H0, productivity24h_H1, productivity24h_H2,
   productivity48h_H0, productivity48h_H1, productivity48h_H2,
   effect_6h_protegens, effect_12h_protegens, effect_24h_protegens, effect_48h_protegens,
   posthoc_6h, posthoc_12h, posthoc_24h, posthoc_48h, posthoc_48h_protegens,
   productivity_protegens, prodEffects_ttests,
   absDen_6h, absDen_12h, absDen_24h, absDen_48h, combos)
```

Let's look at decoupling:

```{r, decoupling_productivity_noExtinctions}
# rename the levels of Trtmt_Day
decoupling_productivity <- productivity_effects
levels(decoupling_productivity$Trtmt_Day) <- c("late_recov", "early_recov", "resist")

# create data.frame for plotting
decoupling_productivity <- decoupling_productivity %>%
                            select(-n, -SD) %>%
                              pivot_wider(names_from = Trtmt_Day,
                                          values_from = c(est, loCI, hiCI, SE, groups))
# columns that indicate if resistance is significantly different from recovery
decoupling_productivity$early_recov_VS_resist <- mapply(are_groups_different,
                                                        decoupling_productivity$groups_early_recov,
                                                        decoupling_productivity$groups_resist)
decoupling_productivity$late_recov_VS_resist <- mapply(are_groups_different,
                                                       decoupling_productivity$groups_late_recov,
                                                       decoupling_productivity$groups_resist)
# clean up extra columns
decoupling_productivity <- decoupling_productivity %>% select(-groups_resist, -groups_early_recov, -groups_late_recov)


# first plot the decoupling on early recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
  facet_grid(~CommRich) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_early_recov, ymax = hiCI_early_recov), width=0) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (WITHOUT extinct)",
       x = "Resistance +/- 95% CI",
       y = "Early Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# here's another way to plot it where the confidence intervals are shown as ellipses:
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
    facet_grid(~CommRich) +
    geom_hline(yintercept = 0, colour="grey") +
    geom_vline(xintercept = 0, colour="grey") +
    geom_abline(slope = 1) +
    geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
    scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
    scale_fill_manual(values=c("white", "black")) +
    geom_ellipse(aes(x0 = est_resist,
                     y0 = est_early_recov,
                     # radius on x direction:
                     a = hiCI_resist - est_resist,
                     # radius on y direction:
                     b = hiCI_early_recov - est_early_recov,
                     angle = 0)) +
    labs(title = "Decoupling of productivity (WITHOUT extinct)",
         x = "Resistance +/- 95% CI",
         y = "Early Recovery +/- 95% CI",
         colour = "Heat\nDuration",
         fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# next plot the decoupling on later recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
  facet_grid(~CommRich) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  geom_errorbar(aes(ymin = loCI_late_recov, ymax = hiCI_late_recov), width=0) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "Decoupling of productivity (WITHOUT extinct)",
       x = "Resistance +/- 95% CI",
       y = "Late Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# calculate decoupling between resistance and early recovery
NoExtinct_early_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_early_recov,
                                  recov_hiCI = hiCI_early_recov)))
# add annotation
NoExtinct_early_decoupling <- cbind(decoupling_productivity[,1:2],
                                    NoExtinct_early_decoupling)

ggplot(NoExtinct_early_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling, colour = as.factor(CommRich))) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  labs(title = "Early recovery (WITHOUT extinct reps)",
       colour = "Innoculated\nCommunity\nRichness",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")

# calculate decoupling between resistance and late recovery
NoExtinct_late_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_late_recov,
                                  recov_hiCI = hiCI_late_recov)))
# add annotation
NoExtinct_late_decoupling <- cbind(decoupling_productivity[,1:2],
                                   NoExtinct_late_decoupling)

ggplot(NoExtinct_late_decoupling,
       aes(x = as.factor(Heat), y = est_decoupling, colour = as.factor(CommRich))) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  labs(title = "Late recovery (WITHOUT extinct reps)",
       colour = "Innoculated\nCommunity\nRichness",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)")

rm(productivity_effects, decoupling_productivity)
```

### Only extinct reps

This is just for didactic reasons to show that extinct replicates have perfect coupling:

```{r, perfect_coupling}
# recall the data just for the extinction prone communities
absDen_48h <- productivitySubsettedData$h48 %>% filter(community %in% extict_prone_comms)

productivity48h_H0 <- glmmTMB(as.integer(TotDensity_scale * 1000) ~ CommRich*Heat*Trtmt_Day,
                             data = absDen_48h,
                             family = genpois,
                             control = glmmTMBControl(optCtrl = list(iter.max = 500000,eval.max = 500000)))

# create data.frame for plotting prediction
prod_predict <- cbind(productivity48h_H0$frame,
                      predict(productivity48h_H0, type="response"))
colnames(prod_predict)[c(1,5)] <- c("observed", "predicted")
# and remember to divide by 1000 as we did for transforming the data
prod_predict[,c(1,5)] <- prod_predict[,c(1,5)]/1000
# plot the model predictions against the data
ggplot(prod_predict, 
       aes(x=Trtmt_Day, y=observed, colour=CommRich)) +
  facet_grid(~Heat) +
  geom_jitter(alpha=0.4) +
  geom_line(aes(y=predicted, group=CommRich)) +
  scale_colour_viridis_d(option = "viridis", begin=0.1, end=0.85) +
  labs(y="Total density (rescaled)",
       colour="CommRich")
# cleanup
rm(prod_predict)

# get the effect sizes
effect_48h <- eff_size(emmeans(productivity48h_H0, ~ Heat | CommRich*Trtmt_Day, data = absDen_48h),
                                sigma(productivity48h_H0),
                                edf = df.residual(productivity48h_H0))
# do the post hoc
posthoc_48h <- emmeans(effect_48h,
                       pairwise ~ CommRich + Trtmt_Day,
                       data = absDen_48h)

# create a data.frame of effect sizes using a forest plot with the group labels
productivity_effects <- data.frame()
productivity_effects <- rbind(productivity_effects,
                              get_posthoc(posthoc_48h, heat_trtmt = 48))

# re-order the levels of Trtmt_Day to go from resistance to recovery then rename them for nice plotting
productivity_effects$Trtmt_Day <- factor(productivity_effects$Trtmt_Day,
                                         levels = c("recov_2", "recov_1", "resist"))
levels(productivity_effects$Trtmt_Day) <- c("Recovery (Day 2)", "Recovery (Day 1)", "Resistance")


ggplot(productivity_effects,
       aes(x = est, y = CommRich, colour = Trtmt_Day)) +
  facet_grid(. ~ Heat) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  scale_x_continuous(limits = c(-0.015, 0.005)) +
  labs(x = "Effect Size on Total Density",
       y = "Innoculated Community Richness",
       title = "(only extinction-prone communities)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggplot(productivity_effects,
       aes(x = est, y = CommRich, colour = Trtmt_Day)) +
  facet_grid(. ~ Heat) +
  geom_vline(xintercept = 0, colour="darkgrey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbarh(position = position_dodge(width = 0.5),
                 aes(xmin = loCI, xmax = hiCI), height = 0.1) +
  scale_colour_manual(values=trtmt_pal) +
  labs(x = "Effect Size on Total Density",
       y = "Innoculated Community Richness",
       title = "(only extinction-prone communities)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

# rename the levels of Trtmt_Day
decoupling_productivity <- productivity_effects
levels(decoupling_productivity$Trtmt_Day) <- c("late_recov", "early_recov", "resist")

# create data.frame for plotting
decoupling_productivity <- decoupling_productivity %>%
                              pivot_wider(names_from = Trtmt_Day,
                                          values_from = c(est, loCI, hiCI, SE, groups))
# columns that indicate if resistance is significantly different from recovery
decoupling_productivity$early_recov_VS_resist <- mapply(are_groups_different,
                                                        decoupling_productivity$groups_early_recov,
                                                        decoupling_productivity$groups_resist)
decoupling_productivity$late_recov_VS_resist <- mapply(are_groups_different,
                                                       decoupling_productivity$groups_late_recov,
                                                       decoupling_productivity$groups_resist)
# clean up extra columns
decoupling_productivity <- decoupling_productivity %>% select(-groups_resist, -groups_early_recov, -groups_late_recov)


# first plot the decoupling on early recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_early_recov, colour = as.factor(Heat))) +
  facet_grid(~CommRich) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(early_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  scale_x_continuous(limits = c(-0.015, 0.005)) +
  geom_errorbar(aes(ymin = loCI_early_recov, ymax = hiCI_early_recov), width=0) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "(only extinction-prone communities)",
       x = "Resistance +/- 95% CI",
       y = "Early Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# next plot the decoupling on later recovery
ggplot(decoupling_productivity,
       aes(x = est_resist, y = est_late_recov, colour = as.factor(Heat))) +
  facet_grid(~CommRich) +
  geom_hline(yintercept = 0, colour="grey") +
  geom_vline(xintercept = 0, colour="grey") +
  geom_abline(slope = 1) +
  geom_point(shape=21, size=3, aes(fill=as.factor(late_recov_VS_resist))) +
  geom_errorbarh(aes(xmin = loCI_resist, xmax = hiCI_resist), height=0) +
  scale_x_continuous(limits = c(-0.015, 0.005)) +
  geom_errorbar(aes(ymin = loCI_late_recov, ymax = hiCI_late_recov), width=0) +
  scale_colour_viridis_d(option = "plasma", begin=0.2, end = 0.9) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "(only extinction-prone communities)",
       x = "Resistance +/- 95% CI",
       y = "Late Recovery +/- 95% CI",
       colour = "Heat\nDuration",
       fill="Resistance\nvs. Recovery\nSignificantly\nDifferent?")

# calculate decoupling between resistance and early recovery
early_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_early_recov,
                                  recov_hiCI = hiCI_early_recov)))
# add annotation
early_decoupling <- cbind(decoupling_productivity[,1:2],
                          early_decoupling)

ggplot(rbind(NoExtinct_early_decoupling %>% filter(Heat == 48) %>%
               mutate(data = "no extinctions"),
             early_decoupling %>% mutate(data = "extinction-prone")),
       aes(x = as.factor(Heat),
           y = est_decoupling,
           colour = as.factor(CommRich),
           shape = data)) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  scale_y_continuous(limits = c(-0.005, 0.015))+
  labs(title = "effect of extinction on early recovery",
       colour = "Innoculated\nCommunity\nRichness",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)",
       shape = "communities")

ggplot(rbind(NoExtinct_early_decoupling %>% filter(Heat == 48) %>%
               mutate(data = "no extinctions"),
             early_decoupling %>% mutate(data = "extinction-prone")),
       aes(x = as.factor(Heat),
           y = est_decoupling,
           colour = as.factor(CommRich),
           shape = data)) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  scale_y_continuous(limits = c(-0.005, 0.015))+
  labs(title = "effect of extinction on early recovery",
       colour = "Innoculated\nCommunity\nRichness",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)",
       shape = "communities")

# calculate decoupling between resistance and late recovery
late_decoupling <- t(with(decoupling_productivity,
                           mapply(estimate_decoupling,
                                  resist_est = est_resist,
                                  resist_hiCI = hiCI_resist,
                                  recov_est = est_late_recov,
                                  recov_hiCI = hiCI_late_recov)))
# add annotation
late_decoupling <- cbind(decoupling_productivity[,1:2],
                          late_decoupling)

ggplot(rbind(NoExtinct_late_decoupling %>% filter(Heat == 48) %>%
               mutate(data = "no extinctions"),
             late_decoupling %>% mutate(data = "extinction-prone")),
       aes(x = as.factor(Heat),
           y = est_decoupling,
           colour = as.factor(CommRich),
           shape = data)) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  scale_y_continuous(limits = c(-0.005, 0.015))+
  labs(title = "effect of extinction on late recovery",
       colour = "Innoculated\nCommunity\nRichness",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)",
       shape = "communities")

ggplot(rbind(NoExtinct_late_decoupling %>% filter(Heat == 48) %>%
               mutate(data = "no extinctions"),
             late_decoupling %>% mutate(data = "extinction-prone")),
       aes(x = as.factor(Heat),
           y = est_decoupling,
           colour = as.factor(CommRich),
           shape = data)) +
  geom_hline(yintercept = 0, colour = "grey") +
  geom_point(position = position_dodge(width = 0.5)) +
  geom_errorbar(position = position_dodge(width = 0.5),
                aes(ymin = loCI_decoupling, ymax = hiCI_decoupling),
                alpha=0.4, width=0.1) +
  scale_colour_viridis_d(option = "viridis", end=0.85) +
  labs(title = "effect of extinction on late recovery",
       colour = "Innoculated\nCommunity\nRichness",
       y = "Decoupling +/- 95% CI",
       x = "Heat Duration (hrs)",
       shape = "communities")
```

